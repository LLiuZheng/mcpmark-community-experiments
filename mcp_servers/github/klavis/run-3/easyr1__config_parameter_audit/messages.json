[
  {
    "content": "Please execute the following task in my repository mcpmark-eval-zl/EasyR1:\n\nI need you to perform a deep investigation into recent configuration changes in our EasyR1 repository that may be causing training instability issues.\n\n## Task Requirements\n\n### 1. Deep Commit Analysis\nFind the exact commit SHA where the `micro_batch_size_per_device_for_update` parameter was changed from `4` to `1` in the `examples/config.yaml` file. Use GitHub API to:\n- Examine recent commits that modified `examples/config.yaml` \n- Get the specific commit diff showing this parameter change\n- Identify the commit author and timestamp\n\n### 2. Related Parameter Investigation  \nIn the same commit you found above, identify what value the `micro_batch_size_per_device_for_experience` parameter was changed to. Document:\n- The before value for this parameter\n- The after value for this parameter  \n- The specific line numbers in the diff where these changes occurred\n\n### 3. Issue Search and Verification\nSearch through all GitHub issues (both open and closed) to find issues that contain specific keywords. Identify all issue numbers where the issue title or body text contains any of these exact terms:\n- \"OOM\" (case insensitive)\n- \"memory\" (case insensitive) \n- \"batch\" (case insensitive)\n- \"显存\" (GPU memory in Chinese)\n\nYou must find and list ALL issues that contain any of these keywords in their titles or bodies, regardless of whether you think they're related to the parameter changes.\n\n### 4. File Creation and Results\nCreate a file named exactly `ANALYSIS_RESULTS.json` in the repository root with this exact structure:\n\n```json\n{\n  \"target_commit_sha\": \"full-40-character-commit-sha\",\n  \"commit_author\": \"author-username\", \n  \"commit_date\": \"YYYY-MM-DD\",\n  \"parameter_changes\": {\n    \"micro_batch_size_per_device_for_update\": {\n      \"before\": 4,\n      \"after\": 1,\n      \"line_number\": 123\n    },\n    \"micro_batch_size_per_device_for_experience\": {\n      \"before\": 16,\n      \"after\": 2, \n      \"line_number\": 124\n    }\n  },\n  \"related_issue_number_list\": [9, 46]\n}\n```\n\n### 5. Verification Requirements\n- The commit SHA must be exactly 40 hexadecimal characters\n- The parameter values must match the actual repository changes  \n- The issue number must reference a real issue in the repository\n- All data must be obtained through GitHub API analysis, not guesswork\n\nNote: Based on your understanding, solve the task all at once by yourself, don't ask for my opinions on anything.",
    "role": "user"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "I'll help you perform a deep investigation into the recent configuration changes in your EasyR1 repository. Let me start by exploring the GitHub API capabilities to analyze commits, file changes, and issues.",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"user_query\": \"GitHub repository analysis, commits, file changes, issues search\", \"server_names\": [\"GitHub\"]}",
    "call_id": "toolu_01F1jZNCCsL5AaC8K9mRKkkH",
    "name": "discover_server_categories_or_actions",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01F1jZNCCsL5AaC8K9mRKkkH",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"servers\\\\\\\":{\\\\\\\"GitHub\\\\\\\":{\\\\\\\"detail_level\\\\\\\":\\\\\\\"categories_only\\\\\\\",\\\\\\\"details\\\\\\\":[\\\\\\\"GITHUB_META\\\\\\\",\\\\\\\"GITHUB_SECURITY-ADVISORIES\\\\\\\",\\\\\\\"GITHUB_APPS\\\\\\\",\\\\\\\"GITHUB_CLASSROOM\\\\\\\",\\\\\\\"GITHUB_CODES-OF-CONDUCT\\\\\\\",\\\\\\\"GITHUB_CREDENTIALS\\\\\\\",\\\\\\\"GITHUB_EMOJIS\\\\\\\",\\\\\\\"GITHUB_CODE-SECURITY\\\\\\\",\\\\\\\"GITHUB_DEPENDABOT\\\\\\\",\\\\\\\"GITHUB_SECRET-SCANNING\\\\\\\",\\\\\\\"GITHUB_ACTIVITY\\\\\\\",\\\\\\\"GITHUB_GISTS\\\\\\\",\\\\\\\"GITHUB_GITIGNORE\\\\\\\",\\\\\\\"GITHUB_ISSUES\\\\\\\",\\\\\\\"GITHUB_LICENSES\\\\\\\",\\\\\\\"GITHUB_MARKDOWN\\\\\\\",\\\\\\\"GITHUB_ORGS\\\\\\\",\\\\\\\"GITHUB_BILLING\\\\\\\",\\\\\\\"GITHUB_ACTIONS\\\\\\\",\\\\\\\"GITHUB_OIDC\\\\\\\",\\\\\\\"GITHUB_CAMPAIGNS\\\\\\\",\\\\\\\"GITHUB_CODE-SCANNING\\\\\\\",\\\\\\\"GITHUB_CODESPACES\\\\\\\",\\\\\\\"GITHUB_COPILOT\\\\\\\",\\\\\\\"GITHUB_PACKAGES\\\\\\\",\\\\\\\"GITHUB_INTERACTIONS\\\\\\\",\\\\\\\"GITHUB_MIGRATIONS\\\\\\\",\\\\\\\"GITHUB_PRIVATE-REGISTRIES\\\\\\\",\\\\\\\"GITHUB_PROJECTS-CLASSIC\\\\\\\",\\\\\\\"GITHUB_REPOS\\\\\\\",\\\\\\\"GITHUB_HOSTED-COMPUTE\\\\\\\",\\\\\\\"GITHUB_TEAMS\\\\\\\",\\\\\\\"GITHUB_REACTIONS\\\\\\\",\\\\\\\"GITHUB_RATE-LIMIT\\\\\\\",\\\\\\\"GITHUB_CHECKS\\\\\\\",\\\\\\\"GITHUB_DEPENDENCY-GRAPH\\\\\\\",\\\\\\\"GITHUB_GIT\\\\\\\",\\\\\\\"GITHUB_PULLS\\\\\\\",\\\\\\\"GITHUB_SEARCH\\\\\\\",\\\\\\\"GITHUB_USERS\\\\\\\"]}}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Now let me get the specific actions for commits and issues analysis:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"category_names\": [\"GITHUB_REPOS\", \"GITHUB_ISSUES\", \"GITHUB_SEARCH\"]}",
    "call_id": "toolu_01MvxTUaw2UuKF5yeu79zbv1",
    "name": "get_category_actions",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01MvxTUaw2UuKF5yeu79zbv1",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"category_actions\\\\\\\":{\\\\\\\"GITHUB_REPOS\\\\\\\":[\\\\\\\"list_for_org\\\\\\\",\\\\\\\"create_in_org\\\\\\\",\\\\\\\"get_org_rulesets\\\\\\\",\\\\\\\"create_org_ruleset\\\\\\\",\\\\\\\"get_org_rule_suites\\\\\\\",\\\\\\\"get_org_rule_suite\\\\\\\",\\\\\\\"get_org_ruleset\\\\\\\",\\\\\\\"update_org_ruleset\\\\\\\",\\\\\\\"delete_org_ruleset\\\\\\\",\\\\\\\"get\\\\\\\",\\\\\\\"update\\\\\\\",\\\\\\\"delete\\\\\\\",\\\\\\\"list_activities\\\\\\\",\\\\\\\"create_attestation\\\\\\\",\\\\\\\"list_attestations\\\\\\\",\\\\\\\"list_autolinks\\\\\\\",\\\\\\\"create_autolink\\\\\\\",\\\\\\\"get_autolink\\\\\\\",\\\\\\\"delete_autolink\\\\\\\",\\\\\\\"check_automated_security_fixes\\\\\\\",\\\\\\\"enable_automated_security_fixes\\\\\\\",\\\\\\\"disable_automated_security_fixes\\\\\\\",\\\\\\\"list_branches\\\\\\\",\\\\\\\"get_branch\\\\\\\",\\\\\\\"get_branch_protection\\\\\\\",\\\\\\\"update_branch_protection\\\\\\\",\\\\\\\"delete_branch_protection\\\\\\\",\\\\\\\"get_admin_branch_protection\\\\\\\",\\\\\\\"set_admin_branch_protection\\\\\\\",\\\\\\\"delete_admin_branch_protection\\\\\\\",\\\\\\\"get_pull_request_review_protection\\\\\\\",\\\\\\\"update_pull_request_review_protection\\\\\\\",\\\\\\\"delete_pull_request_review_protection\\\\\\\",\\\\\\\"get_commit_signature_protection\\\\\\\",\\\\\\\"create_commit_signature_protection\\\\\\\",\\\\\\\"delete_commit_signature_protection\\\\\\\",\\\\\\\"get_status_checks_protection\\\\\\\",\\\\\\\"update_status_check_protection\\\\\\\",\\\\\\\"remove_status_check_protection\\\\\\\",\\\\\\\"get_all_status_check_contexts\\\\\\\",\\\\\\\"add_status_check_contexts\\\\\\\",\\\\\\\"set_status_check_contexts\\\\\\\",\\\\\\\"remove_status_check_contexts\\\\\\\",\\\\\\\"get_access_restrictions\\\\\\\",\\\\\\\"delete_access_restrictions\\\\\\\",\\\\\\\"get_apps_with_access_to_protected_branch\\\\\\\",\\\\\\\"add_app_access_restrictions\\\\\\\",\\\\\\\"set_app_access_restrictions\\\\\\\",\\\\\\\"remove_app_access_restrictions\\\\\\\",\\\\\\\"get_teams_with_access_to_protected_branch\\\\\\\",\\\\\\\"add_team_access_restrictions\\\\\\\",\\\\\\\"set_team_access_restrictions\\\\\\\",\\\\\\\"remove_team_access_restrictions\\\\\\\",\\\\\\\"get_users_with_access_to_protected_branch\\\\\\\",\\\\\\\"add_user_access_restrictions\\\\\\\",\\\\\\\"set_user_access_restrictions\\\\\\\",\\\\\\\"remove_user_access_restrictions\\\\\\\",\\\\\\\"rename_branch\\\\\\\",\\\\\\\"codeowners_errors\\\\\\\",\\\\\\\"list_collaborators\\\\\\\",\\\\\\\"check_collaborator\\\\\\\",\\\\\\\"add_collaborator\\\\\\\",\\\\\\\"remove_collaborator\\\\\\\",\\\\\\\"get_collaborator_permission_level\\\\\\\",\\\\\\\"list_commit_comments_for_repo\\\\\\\",\\\\\\\"get_commit_comment\\\\\\\",\\\\\\\"update_commit_comment\\\\\\\",\\\\\\\"delete_commit_comment\\\\\\\",\\\\\\\"list_commits\\\\\\\",\\\\\\\"list_branches_for_head_commit\\\\\\\",\\\\\\\"list_comments_for_commit\\\\\\\",\\\\\\\"create_commit_comment\\\\\\\",\\\\\\\"list_pull_requests_associated_with_commit\\\\\\\",\\\\\\\"get_commit\\\\\\\",\\\\\\\"get_combined_status_for_ref\\\\\\\",\\\\\\\"list_commit_statuses_for_ref\\\\\\\",\\\\\\\"get_community_profile_metrics\\\\\\\",\\\\\\\"compare_commits\\\\\\\",\\\\\\\"get_content\\\\\\\",\\\\\\\"create_or_update_file_contents\\\\\\\",\\\\\\\"delete_file\\\\\\\",\\\\\\\"list_contributors\\\\\\\",\\\\\\\"list_deployments\\\\\\\",\\\\\\\"create_deployment\\\\\\\",\\\\\\\"get_deployment\\\\\\\",\\\\\\\"delete_deployment\\\\\\\",\\\\\\\"list_deployment_statuses\\\\\\\",\\\\\\\"create_deployment_status\\\\\\\",\\\\\\\"get_deployment_status\\\\\\\",\\\\\\\"create_dispatch_event\\\\\\\",\\\\\\\"get_all_environments\\\\\\\",\\\\\\\"get_environment\\\\\\\",\\\\\\\"create_or_update_environment\\\\\\\",\\\\\\\"delete_an_environment\\\\\\\",\\\\\\\"list_deployment_branch_policies\\\\\\\",\\\\\\\"create_deployment_branch_policy\\\\\\\",\\\\\\\"get_deployment_branch_policy\\\\\\\",\\\\\\\"update_deployment_branch_policy\\\\\\\",\\\\\\\"delete_deployment_branch_policy\\\\\\\",\\\\\\\"get_all_deployment_protection_rules\\\\\\\",\\\\\\\"create_deployment_protection_rule\\\\\\\",\\\\\\\"list_custom_deployment_rule_integrations\\\\\\\",\\\\\\\"get_custom_deployment_protection_rule\\\\\\\",\\\\\\\"disable_deployment_protection_rule\\\\\\\",\\\\\\\"list_forks\\\\\\\",\\\\\\\"create_fork\\\\\\\",\\\\\\\"list_webhooks\\\\\\\",\\\\\\\"create_webhook\\\\\\\",\\\\\\\"get_webhook\\\\\\\",\\\\\\\"update_webhook\\\\\\\",\\\\\\\"delete_webhook\\\\\\\",\\\\\\\"get_webhook_config_for_repo\\\\\\\",\\\\\\\"update_webhook_config_for_repo\\\\\\\",\\\\\\\"list_webhook_deliveries\\\\\\\",\\\\\\\"get_webhook_delivery\\\\\\\",\\\\\\\"redeliver_webhook_delivery\\\\\\\",\\\\\\\"ping_webhook\\\\\\\",\\\\\\\"test_push_webhook\\\\\\\",\\\\\\\"list_invitations\\\\\\\",\\\\\\\"update_invitation\\\\\\\",\\\\\\\"delete_invitation\\\\\\\",\\\\\\\"list_deploy_keys\\\\\\\",\\\\\\\"create_deploy_key\\\\\\\",\\\\\\\"get_deploy_key\\\\\\\",\\\\\\\"delete_deploy_key\\\\\\\",\\\\\\\"list_languages\\\\\\\",\\\\\\\"merge_upstream\\\\\\\",\\\\\\\"merge\\\\\\\",\\\\\\\"get_pages\\\\\\\",\\\\\\\"create_pages_site\\\\\\\",\\\\\\\"update_information_about_pages_site\\\\\\\",\\\\\\\"delete_pages_site\\\\\\\",\\\\\\\"list_pages_builds\\\\\\\",\\\\\\\"request_pages_build\\\\\\\",\\\\\\\"get_latest_pages_build\\\\\\\",\\\\\\\"get_pages_build\\\\\\\",\\\\\\\"create_pages_deployment\\\\\\\",\\\\\\\"get_pages_deployment\\\\\\\",\\\\\\\"cancel_pages_deployment\\\\\\\",\\\\\\\"get_pages_health_check\\\\\\\",\\\\\\\"check_private_vulnerability_reporting\\\\\\\",\\\\\\\"enable_private_vulnerability_reporting\\\\\\\",\\\\\\\"disable_private_vulnerability_reporting\\\\\\\",\\\\\\\"get_custom_properties_values\\\\\\\",\\\\\\\"create_or_update_custom_properties_values\\\\\\\",\\\\\\\"get_readme\\\\\\\",\\\\\\\"get_readme_in_directory\\\\\\\",\\\\\\\"list_releases\\\\\\\",\\\\\\\"create_release\\\\\\\",\\\\\\\"get_release_asset\\\\\\\",\\\\\\\"update_release_asset\\\\\\\",\\\\\\\"delete_release_asset\\\\\\\",\\\\\\\"generate_release_notes\\\\\\\",\\\\\\\"get_latest_release\\\\\\\",\\\\\\\"get_release_by_tag\\\\\\\",\\\\\\\"get_release\\\\\\\",\\\\\\\"update_release\\\\\\\",\\\\\\\"delete_release\\\\\\\",\\\\\\\"list_release_assets\\\\\\\",\\\\\\\"upload_release_asset\\\\\\\",\\\\\\\"get_branch_rules\\\\\\\",\\\\\\\"get_repo_rulesets\\\\\\\",\\\\\\\"create_repo_ruleset\\\\\\\",\\\\\\\"get_repo_rule_suites\\\\\\\",\\\\\\\"get_repo_rule_suite\\\\\\\",\\\\\\\"get_repo_ruleset\\\\\\\",\\\\\\\"update_repo_ruleset\\\\\\\",\\\\\\\"delete_repo_ruleset\\\\\\\",\\\\\\\"get_repo_ruleset_history\\\\\\\",\\\\\\\"get_repo_ruleset_version\\\\\\\",\\\\\\\"get_code_frequency_stats\\\\\\\",\\\\\\\"get_commit_activity_stats\\\\\\\",\\\\\\\"get_contributors_stats\\\\\\\",\\\\\\\"get_participation_stats\\\\\\\",\\\\\\\"get_punch_card_stats\\\\\\\",\\\\\\\"create_commit_status\\\\\\\",\\\\\\\"list_tags\\\\\\\",\\\\\\\"list_tag_protection\\\\\\\",\\\\\\\"create_tag_protection\\\\\\\",\\\\\\\"delete_tag_protection\\\\\\\",\\\\\\\"download_tarball_archive\\\\\\\",\\\\\\\"list_teams\\\\\\\",\\\\\\\"get_all_topics\\\\\\\",\\\\\\\"replace_all_topics\\\\\\\",\\\\\\\"get_clones\\\\\\\",\\\\\\\"get_top_paths\\\\\\\",\\\\\\\"get_top_referrers\\\\\\\",\\\\\\\"get_views\\\\\\\",\\\\\\\"transfer\\\\\\\",\\\\\\\"check_vulnerability_alerts\\\\\\\",\\\\\\\"enable_vulnerability_alerts\\\\\\\",\\\\\\\"disable_vulnerability_alerts\\\\\\\",\\\\\\\"download_zipball_archive\\\\\\\",\\\\\\\"create_using_template\\\\\\\",\\\\\\\"list_public\\\\\\\",\\\\\\\"list_for_authenticated_user\\\\\\\",\\\\\\\"create_for_authenticated_user\\\\\\\",\\\\\\\"list_invitations_for_authenticated_user\\\\\\\",\\\\\\\"accept_invitation_for_authenticated_user\\\\\\\",\\\\\\\"decline_invitation_for_authenticated_user\\\\\\\",\\\\\\\"list_for_user\\\\\\\"],\\\\\\\"GITHUB_ISSUES\\\\\\\":[\\\\\\\"list\\\\\\\",\\\\\\\"list_for_org\\\\\\\",\\\\\\\"list_assignees\\\\\\\",\\\\\\\"check_user_can_be_assigned\\\\\\\",\\\\\\\"list_for_repo\\\\\\\",\\\\\\\"create\\\\\\\",\\\\\\\"list_comments_for_repo\\\\\\\",\\\\\\\"get_comment\\\\\\\",\\\\\\\"update_comment\\\\\\\",\\\\\\\"delete_comment\\\\\\\",\\\\\\\"list_events_for_repo\\\\\\\",\\\\\\\"get_event\\\\\\\",\\\\\\\"get\\\\\\\",\\\\\\\"update\\\\\\\",\\\\\\\"add_assignees\\\\\\\",\\\\\\\"remove_assignees\\\\\\\",\\\\\\\"check_user_can_be_assigned_to_issue\\\\\\\",\\\\\\\"list_comments\\\\\\\",\\\\\\\"create_comment\\\\\\\",\\\\\\\"list_dependencies_blocked_by\\\\\\\",\\\\\\\"add_blocked_by_dependency\\\\\\\",\\\\\\\"remove_dependency_blocked_by\\\\\\\",\\\\\\\"list_dependencies_blocking\\\\\\\",\\\\\\\"list_events\\\\\\\",\\\\\\\"list_labels_on_issue\\\\\\\",\\\\\\\"add_labels\\\\\\\",\\\\\\\"set_labels\\\\\\\",\\\\\\\"remove_all_labels\\\\\\\",\\\\\\\"remove_label\\\\\\\",\\\\\\\"lock\\\\\\\",\\\\\\\"unlock\\\\\\\",\\\\\\\"get_parent\\\\\\\",\\\\\\\"remove_sub_issue\\\\\\\",\\\\\\\"list_sub_issues\\\\\\\",\\\\\\\"add_sub_issue\\\\\\\",\\\\\\\"reprioritize_sub_issue\\\\\\\",\\\\\\\"list_events_for_timeline\\\\\\\",\\\\\\\"list_labels_for_repo\\\\\\\",\\\\\\\"create_label\\\\\\\",\\\\\\\"get_label\\\\\\\",\\\\\\\"update_label\\\\\\\",\\\\\\\"delete_label\\\\\\\",\\\\\\\"list_milestones\\\\\\\",\\\\\\\"create_milestone\\\\\\\",\\\\\\\"get_milestone\\\\\\\",\\\\\\\"update_milestone\\\\\\\",\\\\\\\"delete_milestone\\\\\\\",\\\\\\\"list_labels_for_milestone\\\\\\\",\\\\\\\"list_for_authenticated_user\\\\\\\"],\\\\\\\"GITHUB_SEARCH\\\\\\\":[\\\\\\\"code\\\\\\\",\\\\\\\"commits\\\\\\\",\\\\\\\"issues_and_pull_requests\\\\\\\",\\\\\\\"labels\\\\\\\",\\\\\\\"repos\\\\\\\",\\\\\\\"topics\\\\\\\",\\\\\\\"users\\\\\\\"]}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Let me get the detailed information for the specific actions I'll need:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"category_name\": \"GITHUB_REPOS\", \"action_name\": \"list_commits\"}",
    "call_id": "toolu_01HnW9BqhPHN5YM2FZTHpFEp",
    "name": "get_action_details",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01HnW9BqhPHN5YM2FZTHpFEp",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"name\\\\\\\":\\\\\\\"list_commits\\\\\\\",\\\\\\\"method\\\\\\\":\\\\\\\"GET\\\\\\\",\\\\\\\"path\\\\\\\":\\\\\\\"/repos/{owner}/{repo}/commits\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"**Signature verification object**\\\\\\\\n\\\\\\\\nThe response will include a `verification` object that describes the result of verifying the commit's signature. The following fields are included in the `verification` object:\\\\\\\\n\\\\\\\\n| Name | Type | Description |\\\\\\\\n| ---- | ---- | ----------- |\\\\\\\\n| `verified` | `boolean` | Indicates whether GitHub considers the signature in this commit to be verified. |\\\\\\\\n| `reason` | `string` | The reason for verified value. Possible values and their meanings are enumerated in table below. |\\\\\\\\n| `signature` | `string` | The signature that was extracted from the commit. |\\\\\\\\n| `payload` | `string` | The value that was signed. |\\\\\\\\n| `verified_at` | `string` | The date the signature was verified by GitHub. |\\\\\\\\n\\\\\\\\nThese are the possible values for `reason` in the `verification` object:\\\\\\\\n\\\\\\\\n| Value | Description |\\\\\\\\n| ----- | ----------- |\\\\\\\\n| `expired_key` | The key that made the signature is expired. |\\\\\\\\n| `not_signing_key` | The \\\\\\\\\\\\\\\"signing\\\\\\\\\\\\\\\" flag is not among the usage flags in the GPG key that made the signature. |\\\\\\\\n| `gpgverify_error` | There was an error communicating with the signature verification service. |\\\\\\\\n| `gpgverify_unavailable` | The signature verification service is currently unavailable. |\\\\\\\\n| `unsigned` | The object does not include a signature. |\\\\\\\\n| `unknown_signature_type` | A non-PGP signature was found in the commit. |\\\\\\\\n| `no_user` | No user was associated with the `committer` email address in the commit. |\\\\\\\\n| `unverified_email` | The `committer` email address in the commit was associated with a user, but the email address is not verified on their account. |\\\\\\\\n| `bad_email` | The `committer` email address in the commit is not included in the identities of the PGP key that made the signature. |\\\\\\\\n| `unknown_key` | The key that made the signature has not been registered with any user's account. |\\\\\\\\n| `malformed_signature` | There was an error parsing the signature. |\\\\\\\\n| `invalid` | The signature could not be cryptographically verified using the key whose key-id was found in the signature. |\\\\\\\\n| `valid` | None of the above errors applied, so the signature is considered to be verified. |\\\\\\\",\\\\\\\"tags\\\\\\\":[\\\\\\\"repos\\\\\\\"],\\\\\\\"path_params\\\\\\\":{\\\\\\\"owner\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The account owner of the repository. The name is not case sensitive.\\\\\\\"},\\\\\\\"repo\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The name of the repository without the `.git` extension. The name is not case sensitive.\\\\\\\"}},\\\\\\\"query_params\\\\\\\":{\\\\\\\"sha\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"SHA or branch to start listing commits from. Default: the repository\\\\\\\\u2019s default branch (usually `main`).\\\\\\\"},\\\\\\\"path\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"Only commits containing this file path will be returned.\\\\\\\"},\\\\\\\"author\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"GitHub username or email address to use to filter by commit author.\\\\\\\"},\\\\\\\"committer\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"GitHub username or email address to use to filter by commit committer.\\\\\\\"},\\\\\\\"since\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":\\\\\\\"date-time\\\\\\\",\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"Only show results that were last updated after the given time. This is a timestamp in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) format: `YYYY-MM-DDTHH:MM:SSZ`. Due to limitations of Git, timestamps must be between 1970-01-01 and 2099-12-31 (inclusive) or unexpected results may be returned.\\\\\\\"},\\\\\\\"until\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":\\\\\\\"date-time\\\\\\\",\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"Only commits before this date will be returned. This is a timestamp in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) format: `YYYY-MM-DDTHH:MM:SSZ`. Due to limitations of Git, timestamps must be between 1970-01-01 and 2099-12-31 (inclusive) or unexpected results may be returned.\\\\\\\"},\\\\\\\"per_page\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The number of results per page (max 100). For more information, see \\\\\\\\\\\\\\\"[Using pagination in the REST API](https://docs.github.com/rest/using-the-rest-api/using-pagination-in-the-rest-api).\\\\\\\\\\\\\\\"\\\\\\\"},\\\\\\\"page\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The page number of the results to fetch. For more information, see \\\\\\\\\\\\\\\"[Using pagination in the REST API](https://docs.github.com/rest/using-the-rest-api/using-pagination-in-the-rest-api).\\\\\\\\\\\\\\\"\\\\\\\"}},\\\\\\\"body_schema\\\\\\\":{},\\\\\\\"response_schema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"items\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Commit\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"sha\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"node_id\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"html_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"comments_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"commit\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"author\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"null\\\\\\\",\\\\\\\"object\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"committer\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"null\\\\\\\",\\\\\\\"object\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"parents\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"stats\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"files\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"_truncated\\\\\\\":true}},\\\\\\\"required\\\\\\\":[\\\\\\\"url\\\\\\\",\\\\\\\"sha\\\\\\\",\\\\\\\"node_id\\\\\\\",\\\\\\\"html_url\\\\\\\",\\\\\\\"comments_url\\\\\\\",\\\\\\\"commit\\\\\\\",\\\\\\\"author\\\\\\\",\\\\\\\"committer\\\\\\\",\\\\\\\"parents\\\\\\\"]}}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "arguments": "{\"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\"}",
    "call_id": "toolu_01RNWHBSV8p9g9BB6krc6YjX",
    "name": "get_action_details",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01RNWHBSV8p9g9BB6krc6YjX",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"method\\\\\\\":\\\\\\\"GET\\\\\\\",\\\\\\\"path\\\\\\\":\\\\\\\"/repos/{owner}/{repo}/commits/{ref}\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Returns the contents of a single commit reference. You must have `read` access for the repository to use this endpoint.\\\\\\\\n\\\\\\\\n> [!NOTE]\\\\\\\\n> If there are more than 300 files in the commit diff and the default JSON media type is requested, the response will include pagination link headers for the remaining files, up to a limit of 3000 files. Each page contains the static commit information, and the only changes are to the file listing.\\\\\\\\n\\\\\\\\nThis endpoint supports the following custom media types. For more information, see \\\\\\\\\\\\\\\"[Media types](https://docs.github.com/rest/using-the-rest-api/getting-started-with-the-rest-api#media-types).\\\\\\\\\\\\\\\" Pagination query parameters are not supported for these media types.\\\\\\\\n\\\\\\\\n- **`application/vnd.github.diff`**: Returns the diff of the commit. Larger diffs may time out and return a 5xx status code.\\\\\\\\n- **`application/vnd.github.patch`**: Returns the patch of the commit. Diffs with binary data will have no `patch` property. Larger diffs may time out and return a 5xx status code.\\\\\\\\n- **`application/vnd.github.sha`**: Returns the commit's SHA-1 hash. You can use this endpoint to check if a remote reference's SHA-1 hash is the same as your local reference's SHA-1 hash by providing the local SHA-1 reference as the ETag.\\\\\\\\n\\\\\\\\n**Signature verification object**\\\\\\\\n\\\\\\\\nThe response will include a `verification` object that describes the result of verifying the commit's signature. The following fields are included in the `verification` object:\\\\\\\\n\\\\\\\\n| Name | Type | Description |\\\\\\\\n| ---- | ---- | ----------- |\\\\\\\\n| `verified` | `boolean` | Indicates whether GitHub considers the signature in this commit to be verified. |\\\\\\\\n| `reason` | `string` | The reason for verified value. Possible values and their meanings are enumerated in table below. |\\\\\\\\n| `signature` | `string` | The signature that was extracted from the commit. |\\\\\\\\n| `payload` | `string` | The value that was signed. |\\\\\\\\n| `verified_at` | `string` | The date the signature was verified by GitHub. |\\\\\\\\n\\\\\\\\nThese are the possible values for `reason` in the `verification` object:\\\\\\\\n\\\\\\\\n| Value | Description |\\\\\\\\n| ----- | ----------- |\\\\\\\\n| `expired_key` | The key that made the signature is expired. |\\\\\\\\n| `not_signing_key` | The \\\\\\\\\\\\\\\"signing\\\\\\\\\\\\\\\" flag is not among the usage flags in the GPG key that made the signature. |\\\\\\\\n| `gpgverify_error` | There was an error communicating with the signature verification service. |\\\\\\\\n| `gpgverify_unavailable` | The signature verification service is currently unavailable. |\\\\\\\\n| `unsigned` | The object does not include a signature. |\\\\\\\\n| `unknown_signature_type` | A non-PGP signature was found in the commit. |\\\\\\\\n| `no_user` | No user was associated with the `committer` email address in the commit. |\\\\\\\\n| `unverified_email` | The `committer` email address in the commit was associated with a user, but the email address is not verified on their account. |\\\\\\\\n| `bad_email` | The `committer` email address in the commit is not included in the identities of the PGP key that made the signature. |\\\\\\\\n| `unknown_key` | The key that made the signature has not been registered with any user's account. |\\\\\\\\n| `malformed_signature` | There was an error parsing the signature. |\\\\\\\\n| `invalid` | The signature could not be cryptographically verified using the key whose key-id was found in the signature. |\\\\\\\\n| `valid` | None of the above errors applied, so the signature is considered to be verified. |\\\\\\\",\\\\\\\"tags\\\\\\\":[\\\\\\\"repos\\\\\\\"],\\\\\\\"path_params\\\\\\\":{\\\\\\\"owner\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The account owner of the repository. The name is not case sensitive.\\\\\\\"},\\\\\\\"repo\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The name of the repository without the `.git` extension. The name is not case sensitive.\\\\\\\"},\\\\\\\"ref\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The commit reference. Can be a commit SHA, branch name (`heads/BRANCH_NAME`), or tag name (`tags/TAG_NAME`). For more information, see \\\\\\\\\\\\\\\"[Git References](https://git-scm.com/book/en/v2/Git-Internals-Git-References)\\\\\\\\\\\\\\\" in the Git documentation.\\\\\\\"}},\\\\\\\"query_params\\\\\\\":{\\\\\\\"page\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The page number of the results to fetch. For more information, see \\\\\\\\\\\\\\\"[Using pagination in the REST API](https://docs.github.com/rest/using-the-rest-api/using-pagination-in-the-rest-api).\\\\\\\\\\\\\\\"\\\\\\\"},\\\\\\\"per_page\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The number of results per page (max 100). For more information, see \\\\\\\\\\\\\\\"[Using pagination in the REST API](https://docs.github.com/rest/using-the-rest-api/using-pagination-in-the-rest-api).\\\\\\\\\\\\\\\"\\\\\\\"}},\\\\\\\"body_schema\\\\\\\":{},\\\\\\\"response_schema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Commit\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"sha\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"node_id\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"html_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"comments_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"commit\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"author\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"null\\\\\\\",\\\\\\\"object\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"committer\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"null\\\\\\\",\\\\\\\"object\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"parents\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"stats\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"files\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"_truncated\\\\\\\":true}},\\\\\\\"required\\\\\\\":[\\\\\\\"url\\\\\\\",\\\\\\\"sha\\\\\\\",\\\\\\\"node_id\\\\\\\",\\\\\\\"html_url\\\\\\\",\\\\\\\"comments_url\\\\\\\",\\\\\\\"commit\\\\\\\",\\\\\\\"author\\\\\\\",\\\\\\\"committer\\\\\\\",\\\\\\\"parents\\\\\\\"]}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "arguments": "{\"category_name\": \"GITHUB_ISSUES\", \"action_name\": \"list_for_repo\"}",
    "call_id": "toolu_01NafcmQLMHTui13cptrQR2D",
    "name": "get_action_details",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01NafcmQLMHTui13cptrQR2D",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"name\\\\\\\":\\\\\\\"list_for_repo\\\\\\\",\\\\\\\"method\\\\\\\":\\\\\\\"GET\\\\\\\",\\\\\\\"path\\\\\\\":\\\\\\\"/repos/{owner}/{repo}/issues\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"List issues in a repository. Only open issues will be listed.\\\\\\\\n\\\\\\\\n> [!NOTE]\\\\\\\\n> GitHub's REST API considers every pull request an issue, but not every issue is a pull request. For this reason, \\\\\\\\\\\\\\\"Issues\\\\\\\\\\\\\\\" endpoints may return both issues and pull requests in the response. You can identify pull requests by the `pull_request` key. Be aware that the `id` of a pull request returned from \\\\\\\\\\\\\\\"Issues\\\\\\\\\\\\\\\" endpoints will be an _issue id_. To find out the pull request id, use the \\\\\\\\\\\\\\\"[List pull requests](https://docs.github.com/rest/pulls/pulls#list-pull-requests)\\\\\\\\\\\\\\\" endpoint.\\\\\\\\n\\\\\\\\nThis endpoint supports the following custom media types. For more information, see \\\\\\\\\\\\\\\"[Media types](https://docs.github.com/rest/using-the-rest-api/getting-started-with-the-rest-api#media-types).\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n- **`application/vnd.github.raw+json`**: Returns the raw markdown body. Response will include `body`. This is the default if you do not pass any specific media type.\\\\\\\\n- **`application/vnd.github.text+json`**: Returns a text only representation of the markdown body. Response will include `body_text`.\\\\\\\\n- **`application/vnd.github.html+json`**: Returns HTML rendered from the body's markdown. Response will include `body_html`.\\\\\\\\n- **`application/vnd.github.full+json`**: Returns raw, text, and HTML representations. Response will include `body`, `body_text`, and `body_html`.\\\\\\\",\\\\\\\"tags\\\\\\\":[\\\\\\\"issues\\\\\\\"],\\\\\\\"path_params\\\\\\\":{\\\\\\\"owner\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The account owner of the repository. The name is not case sensitive.\\\\\\\"},\\\\\\\"repo\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The name of the repository without the `.git` extension. The name is not case sensitive.\\\\\\\"}},\\\\\\\"query_params\\\\\\\":{\\\\\\\"milestone\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"If an `integer` is passed, it should refer to a milestone by its `number` field. If the string `*` is passed, issues with any milestone are accepted. If the string `none` is passed, issues without milestones are returned.\\\\\\\"},\\\\\\\"state\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"Indicates the state of the issues to return.\\\\\\\"},\\\\\\\"assignee\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"Can be the name of a user. Pass in `none` for issues with no assigned user, and `*` for issues assigned to any user.\\\\\\\"},\\\\\\\"type\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"Can be the name of an issue type. If the string `*` is passed, issues with any type are accepted. If the string `none` is passed, issues without type are returned.\\\\\\\"},\\\\\\\"creator\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The user that created the issue.\\\\\\\"},\\\\\\\"mentioned\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"A user that's mentioned in the issue.\\\\\\\"},\\\\\\\"labels\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"A list of comma separated label names. Example: `bug,ui,@high`\\\\\\\"},\\\\\\\"sort\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"What to sort results by.\\\\\\\"},\\\\\\\"direction\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The direction to sort the results by.\\\\\\\"},\\\\\\\"since\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":\\\\\\\"date-time\\\\\\\",\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"Only show results that were last updated after the given time. This is a timestamp in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) format: `YYYY-MM-DDTHH:MM:SSZ`.\\\\\\\"},\\\\\\\"per_page\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The number of results per page (max 100). For more information, see \\\\\\\\\\\\\\\"[Using pagination in the REST API](https://docs.github.com/rest/using-the-rest-api/using-pagination-in-the-rest-api).\\\\\\\\\\\\\\\"\\\\\\\"},\\\\\\\"page\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The page number of the results to fetch. For more information, see \\\\\\\\\\\\\\\"[Using pagination in the REST API](https://docs.github.com/rest/using-the-rest-api/using-pagination-in-the-rest-api).\\\\\\\\\\\\\\\"\\\\\\\"}},\\\\\\\"body_schema\\\\\\\":{},\\\\\\\"response_schema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"items\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Issues are a great way to keep track of tasks, enhancements, and bugs for your projects.\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"id\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"node_id\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"URL for the issue\\\\\\\"},\\\\\\\"repository_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"labels_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"comments_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"events_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"html_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"number\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"Number uniquely identifying the issue within its repository\\\\\\\"},\\\\\\\"state\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"State of the issue; either 'open' or 'closed'\\\\\\\"},\\\\\\\"state_reason\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"string\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The reason for the current state\\\\\\\"},\\\\\\\"title\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"Title of the issue\\\\\\\"},\\\\\\\"body\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"string\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"Contents of the issue\\\\\\\"},\\\\\\\"user\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"labels\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"Labels to associate with this issue; pass one or more label names to replace the set of labels on this issue; send an empty array to clear all labels from the issue; note that the labels are silently dropped for users without push access to the repository\\\\\\\"},\\\\\\\"assignee\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"assignees\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"array\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"milestone\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"locked\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"boolean\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"active_lock_reason\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"string\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"comments\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"pull_request\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"closed_at\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"string\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"created_at\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"updated_at\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"draft\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"boolean\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"closed_by\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"body_html\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"body_text\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"timeline_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"type\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"object\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The type of issue.\\\\\\\"},\\\\\\\"repository\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"A repository on GitHub.\\\\\\\"},\\\\\\\"performed_via_github_app\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"author_association\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"How the author is associated with the repository.\\\\\\\"},\\\\\\\"reactions\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"sub_issues_summary\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"parent_issue_url\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"string\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"URL to get the parent issue of this issue, if it is a sub-issue\\\\\\\"},\\\\\\\"issue_dependencies_summary\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"issue_field_values\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"_truncated\\\\\\\":true}},\\\\\\\"required\\\\\\\":[\\\\\\\"assignee\\\\\\\",\\\\\\\"closed_at\\\\\\\",\\\\\\\"comments\\\\\\\",\\\\\\\"comments_url\\\\\\\",\\\\\\\"events_url\\\\\\\",\\\\\\\"html_url\\\\\\\",\\\\\\\"id\\\\\\\",\\\\\\\"node_id\\\\\\\",\\\\\\\"labels\\\\\\\",\\\\\\\"labels_url\\\\\\\",\\\\\\\"milestone\\\\\\\",\\\\\\\"number\\\\\\\",\\\\\\\"repository_url\\\\\\\",\\\\\\\"state\\\\\\\",\\\\\\\"locked\\\\\\\",\\\\\\\"title\\\\\\\",\\\\\\\"url\\\\\\\",\\\\\\\"user\\\\\\\",\\\\\\\"author_association\\\\\\\",\\\\\\\"created_at\\\\\\\",\\\\\\\"updated_at\\\\\\\"]}}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "arguments": "{\"category_name\": \"GITHUB_REPOS\", \"action_name\": \"create_or_update_file_contents\"}",
    "call_id": "toolu_01GgUHpfeqeDqnBfcgmpGPzp",
    "name": "get_action_details",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01GgUHpfeqeDqnBfcgmpGPzp",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"name\\\\\\\":\\\\\\\"create_or_update_file_contents\\\\\\\",\\\\\\\"method\\\\\\\":\\\\\\\"PUT\\\\\\\",\\\\\\\"path\\\\\\\":\\\\\\\"/repos/{owner}/{repo}/contents/{path}\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Creates a new file or replaces an existing file in a repository.\\\\\\\\n\\\\\\\\n> [!NOTE]\\\\\\\\n> If you use this endpoint and the \\\\\\\\\\\\\\\"[Delete a file](https://docs.github.com/rest/repos/contents/#delete-a-file)\\\\\\\\\\\\\\\" endpoint in parallel, the concurrent requests will conflict and you will receive errors. You must use these endpoints serially instead.\\\\\\\\n\\\\\\\\nOAuth app tokens and personal access tokens (classic) need the `repo` scope to use this endpoint. The `workflow` scope is also required in order to modify files in the `.github/workflows` directory.\\\\\\\",\\\\\\\"tags\\\\\\\":[\\\\\\\"repos\\\\\\\"],\\\\\\\"path_params\\\\\\\":{\\\\\\\"owner\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The account owner of the repository. The name is not case sensitive.\\\\\\\"},\\\\\\\"repo\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The name of the repository without the `.git` extension. The name is not case sensitive.\\\\\\\"},\\\\\\\"path\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"path parameter\\\\\\\"}},\\\\\\\"query_params\\\\\\\":{},\\\\\\\"body_schema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"message\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"The commit message.\\\\\\\"},\\\\\\\"content\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"The new file content, using Base64 encoding.\\\\\\\"},\\\\\\\"sha\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"**Required if you are updating a file**. The blob SHA of the file being replaced.\\\\\\\"},\\\\\\\"branch\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"The branch name. Default: the repository\\\\\\\\u2019s default branch.\\\\\\\"},\\\\\\\"committer\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"The person that committed the file. Default: the authenticated user.\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"name\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The name of the author or committer of the commit. You'll receive a `422` status code if `name` is omitted.\\\\\\\"},\\\\\\\"email\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The email of the author or committer of the commit. You'll receive a `422` status code if `email` is omitted.\\\\\\\"},\\\\\\\"date\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true}},\\\\\\\"required\\\\\\\":[\\\\\\\"name\\\\\\\",\\\\\\\"email\\\\\\\"]},\\\\\\\"author\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"The author of the file. Default: The `committer` or the authenticated user if you omit `committer`.\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"name\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The name of the author or committer of the commit. You'll receive a `422` status code if `name` is omitted.\\\\\\\"},\\\\\\\"email\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The email of the author or committer of the commit. You'll receive a `422` status code if `email` is omitted.\\\\\\\"},\\\\\\\"date\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true}},\\\\\\\"required\\\\\\\":[\\\\\\\"name\\\\\\\",\\\\\\\"email\\\\\\\"]}},\\\\\\\"required\\\\\\\":[\\\\\\\"message\\\\\\\",\\\\\\\"content\\\\\\\"]},\\\\\\\"response_schema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"File Commit\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"content\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"object\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"commit\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true}},\\\\\\\"required\\\\\\\":[\\\\\\\"content\\\\\\\",\\\\\\\"commit\\\\\\\"]}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Now let me start the investigation. First, I'll get recent commits that modified the `examples/config.yaml` file:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"list_commits\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\"}\", \"query_params\": \"{\\\"path\\\": \\\"examples/config.yaml\\\", \\\"per_page\\\": 20}\", \"include_output_fields\": [\"sha\", \"commit.author.date\", \"commit.author.name\", \"commit.message\", \"author.login\"]}",
    "call_id": "toolu_01KdPHodeVs3R89srFoeokxb",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01KdPHodeVs3R89srFoeokxb",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"list_commits\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"098931530606d22f867fd121b1dcb3225a43661f\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[misc] fix data proto (#458)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-08-08T12:05:58Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support auto resume (#425)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-15T05:48:35Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"52ba694ab5185a967d64801681961501468e6ed8\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[examples] fix config (#420)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-14T13:20:26Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[worker] add dynamic batching (#417)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-14T10:41:21Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"c19396ea16edc383cb25fedb3da58ae08e08046e\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[example] update config (#394)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-24T09:54:16Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"bd3b4eb275858421f12cac8d5d800ef353b0c8d0\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] fix video data (#386)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-23T12:24:55Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"0304d2debd27d7cbd60b0e10d63505b6a056851f\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[release] 0.3.1 (#370)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-19T06:55:58Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"2fc149222a9d6f01800c2b9fe660d46864e33b8c\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[examples] add qwen3_14b_dapo17k_dapo (#369)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-19T00:28:31Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Leijun Zhou\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"Saigyouji-Yuyuko1000\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"866b7cfd59bc700ccd3e25c0863d87a9b89c2343\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[worker] update reward manager (#360)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-17T17:27:53Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"e303179c31e582e0a7be2da9179ca9967848823f\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] add online filtering (#358)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-17T15:23:17Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Leijun Zhou\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"Saigyouji-Yuyuko1000\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"ad02af917a275550f4bfff95f9e1ce78be1fa8f1\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] repeat rollout and prepare filter (#346)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-14T18:55:19Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"572db6c119f63d195b96fbb52608f7e2dc36eb15\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] save best checkpoint (#343)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-13T20:12:53Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"6870bf93bffe68a7c9600f0d0258207484fe6189\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[worker] colocate actor and ref model (#342)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-13T19:14:28Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"98ebdcfb2d1e26bd71800b771dd9692f073e9373\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] fix position ids for qwen2vl mrope & add test (#339)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-13T13:59:59Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"647c30e94c63d3f8b8f0d65d00b0f7669595c41d\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[reward] support batch reward (#271)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-27T15:50:24Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"c024735f22cd0d202a3d9e2c263180bd861c65e9\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] support custom chat template (#270)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-27T15:11:48Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"8d28d058146058142e5e17e68616e43fed2a7469\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support async reward (#252)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-22T17:21:13Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"cba6cdfc03911bc69740ad352d9646e787e2acb0\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] add multi image dataset (#217)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-14T15:23:00Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"7895ba58fe2466aa18e539a44aaa79c9b7e7de64\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] enable user to filter overlong examples in RLHFDataset (#210)\\\\\\\\n\\\\\\\\nCo-authored-by: hoshi-hiyouga <hiyouga@buaa.edu.cn>\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-14T11:05:23Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Qunhong Zeng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"0x404\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"0c6b7fb0e9e1a22d3d8bdc650cca1ea251203297\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] update data configs (#214)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-14T10:55:45Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}}],\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "I can see several commits that modified the config file. Now I need to examine these commits to find the one that changed the `micro_batch_size_per_device_for_update` parameter from 4 to 1. Let me check the most recent commits first:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"ref\\\": \\\"f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\"}\", \"include_output_fields\": [\"sha\", \"commit.author.date\", \"commit.author.name\", \"commit.message\", \"author.login\", \"files\"]}",
    "call_id": "toolu_01FLmhZb7xZt87eDK746dNsU",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01FLmhZb7xZt87eDK746dNsU",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"files\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"866715e76ae96f8d23059ca1e752fdd7e8c41a23\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\".gitignore\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":3,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":3,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/.gitignore\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/.gitignore\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/.gitignore?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -170,6 +170,9 @@ cython_debug/\\\\\\\\n # PyPI configuration file\\\\\\\\n .pypirc\\\\\\\\n \\\\\\\\n+# pytorch\\\\\\\\n+*.pt\\\\\\\\n+\\\\\\\\n # outputs\\\\\\\\n outputs/\\\\\\\\n checkpoints/\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"c65bf2f8cbc11ba271be213a300e9fe1c3287600\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/config.yaml\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fconfig.yaml?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -38,6 +38,7 @@ worker:\\\\\\\\n     micro_batch_size_per_device_for_experience: 16  # equivalent to verl's rollout.log_prob_micro_batch_size_per_gpu\\\\\\\\n     max_grad_norm: 1.0\\\\\\\\n     padding_free: true\\\\\\\\n+    dynamic_batching: true\\\\\\\\n     ulysses_size: 1\\\\\\\\n     model:\\\\\\\\n       model_path: Qwen/Qwen2.5-7B-Instruct\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"e40090bf7a5e6a127200d5529a8ee0a8f62215d8\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen3_14b_dapo17k_dapo.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":3,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/examples%2Fqwen3_14b_dapo17k_dapo.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/examples%2Fqwen3_14b_dapo17k_dapo.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen3_14b_dapo17k_dapo.sh?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -16,7 +16,8 @@ python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     data.rollout_batch_size=512 \\\\\\\\\\\\\\\\\\\\\\\\n     data.mini_rollout_batch_size=256 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.micro_batch_size_per_device_for_update=1 \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.actor.micro_batch_size_per_device_for_experience=8 \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.actor.micro_batch_size_per_device_for_experience=4 \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.actor.ulysses_size=8 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.fsdp.torch_dtype=bf16 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.optim.strategy=adamw_bf16 \\\\\\\\\\\\\\\\\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"5f79c7b47864d779369b9198f6953130ab2f1501\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"tests/test_dataset.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":7,\\\\\\\"changes\\\\\\\":9,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/tests%2Ftest_dataset.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/tests%2Ftest_dataset.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/tests%2Ftest_dataset.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -35,24 +35,19 @@ def test_image_dataset():\\\\\\\\n     )\\\\\\\\n     token_ids = [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655]\\\\\\\\n     assert set(dataset[0].keys()) == {\\\\\\\\n-        \\\\\\\\\\\\\\\"problem\\\\\\\\\\\\\\\",\\\\\\\\n-        \\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\",\\\\\\\\n         \\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\",\\\\\\\\n         \\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\",\\\\\\\\n         \\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\",\\\\\\\\n         \\\\\\\\\\\\\\\"raw_prompt_ids\\\\\\\\\\\\\\\",\\\\\\\\n+        \\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\",\\\\\\\\n         \\\\\\\\\\\\\\\"multi_modal_data\\\\\\\\\\\\\\\",\\\\\\\\n     }\\\\\\\\n-    assert dataset[0][\\\\\\\\\\\\\\\"problem\\\\\\\\\\\\\\\"] == (\\\\\\\\n-        \\\\\\\\\\\\\\\"<image>Chords $\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\overline{A C}$ and $\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\overline{D F}$ are equidistant from the center. \\\\\\\\\\\\\\\"\\\\\\\\n-        \\\\\\\\\\\\\\\"If the radius of $\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\odot G$ is 26 find $A C$\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    assert dataset[0][\\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\"] == \\\\\\\\\\\\\\\"48\\\\\\\\\\\\\\\"\\\\\\\\n     assert torch.all(dataset[0][\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"] == torch.tensor(token_ids))\\\\\\\\n     assert torch.all(dataset[0][\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"] == torch.ones(16))\\\\\\\\n     assert torch.all(dataset[0][\\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\"] == torch.arange(16).unsqueeze(0).expand(3, -1))\\\\\\\\n     assert list(dataset[0][\\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\"].size()) == [3, 16]  # avoid fake positive caused by broadcasting\\\\\\\\n     assert dataset[0][\\\\\\\\\\\\\\\"raw_prompt_ids\\\\\\\\\\\\\\\"] == token_ids\\\\\\\\n+    assert dataset[0][\\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\"] == \\\\\\\\\\\\\\\"48\\\\\\\\\\\\\\\"\\\\\\\\n     assert isinstance(dataset[0][\\\\\\\\\\\\\\\"multi_modal_data\\\\\\\\\\\\\\\"][\\\\\\\\\\\\\\\"images\\\\\\\\\\\\\\\"][0], Image)\\\\\\\\n \\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"3c94ee215ed3671841f980d8841b3d959bf2e145\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"tests/test_dynamic_batch.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"added\\\\\\\",\\\\\\\"additions\\\\\\\":78,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":78,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/tests%2Ftest_dynamic_batch.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/tests%2Ftest_dynamic_batch.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/tests%2Ftest_dynamic_batch.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -0,0 +1,78 @@\\\\\\\\n+# Copyright 2024 Bytedance Ltd. and/or its affiliates\\\\\\\\n+#\\\\\\\\n+# Licensed under the Apache License, Version 2.0 (the \\\\\\\\\\\\\\\"License\\\\\\\\\\\\\\\");\\\\\\\\n+# you may not use this file except in compliance with the License.\\\\\\\\n+# You may obtain a copy of the License at\\\\\\\\n+#\\\\\\\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\\\\\\\n+#\\\\\\\\n+# Unless required by applicable law or agreed to in writing, software\\\\\\\\n+# distributed under the License is distributed on an \\\\\\\\\\\\\\\"AS IS\\\\\\\\\\\\\\\" BASIS,\\\\\\\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\\\\\\\n+# See the License for the specific language governing permissions and\\\\\\\\n+# limitations under the License.\\\\\\\\n+\\\\\\\\n+import numpy as np\\\\\\\\n+import torch\\\\\\\\n+\\\\\\\\n+from verl.protocol import DataProto\\\\\\\\n+from verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def _create_random_mask(\\\\\\\\n+    input_ids: torch.Tensor,\\\\\\\\n+    max_ratio_of_valid_token: float,\\\\\\\\n+    max_ratio_of_left_padding: float,\\\\\\\\n+    min_ratio_of_valid_token: float = 0,\\\\\\\\n+) -> torch.Tensor:\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Create a random mask given input_ids. Support left padding and right padding.\\\\\\\\n+\\\\\\\\n+    Process:\\\\\\\\n+    - Sample valid token length\\\\\\\\n+    - Sample left_padding length\\\\\\\\n+    - Generate padding\\\\\\\\n+\\\\\\\\n+    Args:\\\\\\\\n+        input_ids:\\\\\\\\n+            shape (batch_size, seq_len)\\\\\\\\n+\\\\\\\\n+    Returns:\\\\\\\\n+        mask:\\\\\\\\n+            shape (batch_size, seq_len)\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    assert max_ratio_of_valid_token > 0 and max_ratio_of_valid_token <= 1.0\\\\\\\\n+    assert max_ratio_of_left_padding >= 0 and max_ratio_of_left_padding < 1.0\\\\\\\\n+    assert min_ratio_of_valid_token <= max_ratio_of_valid_token\\\\\\\\n+\\\\\\\\n+    batch_size, sequence_length = input_ids.shape\\\\\\\\n+    max_num_valid_tokens = int(sequence_length * max_ratio_of_valid_token)\\\\\\\\n+    min_num_valid_tokens = max(1, int(sequence_length * min_ratio_of_valid_token))\\\\\\\\n+    max_left_padding = int(sequence_length * max_ratio_of_left_padding)\\\\\\\\n+    assert max_num_valid_tokens + max_left_padding <= sequence_length\\\\\\\\n+    assert max_num_valid_tokens > 0 and max_ratio_of_valid_token <= sequence_length\\\\\\\\n+    mask = torch.ones_like(input_ids, dtype=torch.int64)\\\\\\\\n+    # TODO: we can make this faster\\\\\\\\n+    for i in range(batch_size):\\\\\\\\n+        num_left_padding = np.random.randint(low=0, high=max_left_padding + 1, dtype=np.int64)\\\\\\\\n+        num_valid = np.random.randint(low=min_num_valid_tokens, high=max_num_valid_tokens + 1, dtype=np.int64)\\\\\\\\n+\\\\\\\\n+        for index in range(num_left_padding):\\\\\\\\n+            mask[i, index] = 0\\\\\\\\n+\\\\\\\\n+        for index in range(num_left_padding + num_valid, sequence_length):\\\\\\\\n+            mask[i, index] = 0\\\\\\\\n+\\\\\\\\n+    return mask\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def test_dynamic_batch():\\\\\\\\n+    input_ids = torch.randint(low=0, high=10, size=(20, 100))\\\\\\\\n+    attention_mask = _create_random_mask(\\\\\\\\n+        input_ids=input_ids, max_ratio_of_left_padding=0.1, max_ratio_of_valid_token=0.9, min_ratio_of_valid_token=0.5\\\\\\\\n+    )\\\\\\\\n+    data = {\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\": input_ids, \\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\": attention_mask}\\\\\\\\n+    dataproto = DataProto.from_single_dict(data)\\\\\\\\n+    micro_batches, micro_bsz_idx_lst = prepare_dynamic_batch(dataproto, max_token_len=300)\\\\\\\\n+    input_ids = torch.cat([micro_batch.batch[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"] for micro_batch in micro_batches], dim=0)\\\\\\\\n+    input_ids = restore_dynamic_batch(input_ids, micro_bsz_idx_lst)\\\\\\\\n+    torch.testing.assert_close(input_ids, dataproto.batch[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"])\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"3a00c6d222682d663dcd5969cb2668f60aa4f3ff\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/protocol.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":9,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":12,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fprotocol.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fprotocol.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fprotocol.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -196,9 +196,12 @@ def __getitem__(\\\\\\\\n         if isinstance(item, (list, np.ndarray, torch.Tensor)):\\\\\\\\n             return self.index_select(item)\\\\\\\\n \\\\\\\\n-        tensor_data = self.batch[item] if self.batch is not None else None\\\\\\\\n-        non_tensor_data = {key: value[item] for key, value in self.non_tensor_batch.items()}\\\\\\\\n-        return DataProtoItem(batch=tensor_data, non_tensor_batch=non_tensor_data, meta_info=self.meta_info)\\\\\\\\n+        if isinstance(item, (int, np.integer)):\\\\\\\\n+            tensor_data = self.batch[item] if self.batch is not None else None\\\\\\\\n+            non_tensor_data = {key: value[item] for key, value in self.non_tensor_batch.items()}\\\\\\\\n+            return DataProtoItem(batch=tensor_data, non_tensor_batch=non_tensor_data, meta_info=self.meta_info)\\\\\\\\n+\\\\\\\\n+        raise TypeError(f\\\\\\\\\\\\\\\"Indexing with {type(item)} is not supported.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n     def __getstate__(self) -> Tuple[bytes, Dict[str, NDArray], Dict[str, Any]]:\\\\\\\\n         buffer = io.BytesIO()\\\\\\\\n@@ -573,6 +576,9 @@ def split(self, split_size: int) -> List[\\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\"]:\\\\\\\\n         Returns:\\\\\\\\n             List[DataProto]: a list of DataProto after splitting\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+        assert len(self) % split_size == 0, (\\\\\\\\n+            f\\\\\\\\\\\\\\\"only support equal split. Got size of DataProto {len(self)} and split {split_size}.\\\\\\\\\\\\\\\"\\\\\\\\n+        )\\\\\\\\n         chunks = len(self) // split_size\\\\\\\\n         return self.chunk(chunks)\\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"447e7629e07cf684ace1d0c300fed7e60e638b4d\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/core_algos.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":9,\\\\\\\"deletions\\\\\\\":4,\\\\\\\"changes\\\\\\\":13,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Ftrainer%2Fcore_algos.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Ftrainer%2Fcore_algos.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fcore_algos.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -343,7 +343,7 @@ def compute_policy_loss(\\\\\\\\n     clip_ratio_high: float,\\\\\\\\n     clip_ratio_dual: float,\\\\\\\\n     loss_avg_mode: Literal[\\\\\\\\\\\\\\\"token\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"seq\\\\\\\\\\\\\\\"],\\\\\\\\n-) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\\\\\\\\n+) -> Tuple[torch.Tensor, Dict[str, float]]:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Compute the clipped policy objective and related metrics for PPO.\\\\\\\\n \\\\\\\\n     Adapted from https://github.com/huggingface/trl/blob/v0.15.0/trl/trainer/ppo_trainer.py#L568\\\\\\\\n@@ -417,7 +417,7 @@ def compute_value_loss(\\\\\\\\n     response_mask: torch.Tensor,\\\\\\\\n     cliprange_value: float,\\\\\\\\n     loss_avg_mode: Literal[\\\\\\\\\\\\\\\"token\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"seq\\\\\\\\\\\\\\\"],\\\\\\\\n-) -> Tuple[torch.Tensor, float]:\\\\\\\\n+) -> Tuple[torch.Tensor, Dict[str, float]]:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Compute the value loss.\\\\\\\\n \\\\\\\\n     Adapted from https://github.com/huggingface/trl/blob/v0.15.0/trl/trainer/ppo_trainer.py#L556\\\\\\\\n@@ -442,15 +442,20 @@ def compute_value_loss(\\\\\\\\n             value function loss\\\\\\\\n         vf_clipfrac: a float\\\\\\\\n             The ratio of vf being clipped\\\\\\\\n+        vpred_mean: a float\\\\\\\\n+            The mean of predicted values\\\\\\\\n \\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     vpredclipped = torch.clamp(vpreds, values - cliprange_value, values + cliprange_value)\\\\\\\\n     vf_loss1 = torch.square(vpreds - returns)\\\\\\\\n     vf_loss2 = torch.square(vpredclipped - returns)\\\\\\\\n     clipped_vf_losses = torch.max(vf_loss1, vf_loss2)  # clip if vf_loss1 < vf_loss2\\\\\\\\n     vf_loss = 0.5 * average_loss(clipped_vf_losses, response_mask, mode=loss_avg_mode)\\\\\\\\n-    vf_clipfrac = VF.masked_mean((vf_loss1 < vf_loss2).float(), response_mask).detach().item()\\\\\\\\n-    return vf_loss, vf_clipfrac\\\\\\\\n+    metrics = {\\\\\\\\n+        \\\\\\\\\\\\\\\"vf_clipfrac\\\\\\\\\\\\\\\": VF.masked_mean((vf_loss1 < vf_loss2).float(), response_mask).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"vpred_mean\\\\\\\\\\\\\\\": VF.masked_mean(vpreds, response_mask).detach().item(),\\\\\\\\n+    }\\\\\\\\n+    return vf_loss, metrics\\\\\\\\n \\\\\\\\n \\\\\\\\n def compute_kl(\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"75d73a3e9fb7021b2e18d31c41ac4ce1e0b2b6cd\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/metrics.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":28,\\\\\\\"deletions\\\\\\\":20,\\\\\\\"changes\\\\\\\":48,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Ftrainer%2Fmetrics.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Ftrainer%2Fmetrics.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fmetrics.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -24,6 +24,32 @@ def reduce_metrics(metrics: Dict[str, List[Any]]) -> Dict[str, Any]:\\\\\\\\n     return {key: np.mean(value) for key, value in metrics.items()}\\\\\\\\n \\\\\\\\n \\\\\\\\n+def compute_length_metrics(batch: DataProto) -> Dict[str, Any]:\\\\\\\\n+    max_response_length = batch.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+\\\\\\\\n+    prompt_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, :-max_response_length].bool()\\\\\\\\n+    response_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -max_response_length:].bool()\\\\\\\\n+\\\\\\\\n+    max_prompt_length = prompt_mask.size(-1)\\\\\\\\n+    prompt_length = prompt_mask.sum(-1).float()\\\\\\\\n+    response_length = response_mask.sum(-1).float()\\\\\\\\n+\\\\\\\\n+    return {\\\\\\\\n+        # response length\\\\\\\\n+        \\\\\\\\\\\\\\\"response_length/mean\\\\\\\\\\\\\\\": torch.mean(response_length).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"response_length/max\\\\\\\\\\\\\\\": torch.max(response_length).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"response_length/min\\\\\\\\\\\\\\\": torch.min(response_length).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"response_length/clip_ratio\\\\\\\\\\\\\\\": torch.mean(torch.eq(response_length, max_response_length).float())\\\\\\\\n+        .detach()\\\\\\\\n+        .item(),\\\\\\\\n+        # prompt length\\\\\\\\n+        \\\\\\\\\\\\\\\"prompt_length/mean\\\\\\\\\\\\\\\": torch.mean(prompt_length).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"prompt_length/max\\\\\\\\\\\\\\\": torch.max(prompt_length).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"prompt_length/min\\\\\\\\\\\\\\\": torch.min(prompt_length).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"prompt_length/clip_ratio\\\\\\\\\\\\\\\": torch.mean(torch.eq(prompt_length, max_prompt_length).float()).detach().item(),\\\\\\\\n+    }\\\\\\\\n+\\\\\\\\n+\\\\\\\\n def compute_data_metrics(batch: DataProto, use_critic: bool = False) -> Dict[str, Any]:\\\\\\\\n     sequence_score = batch.batch[\\\\\\\\\\\\\\\"token_level_scores\\\\\\\\\\\\\\\"].sum(-1)\\\\\\\\n     sequence_reward = batch.batch[\\\\\\\\\\\\\\\"token_level_rewards\\\\\\\\\\\\\\\"].sum(-1)\\\\\\\\n@@ -32,14 +58,8 @@ def compute_data_metrics(batch: DataProto, use_critic: bool = False) -> Dict[str\\\\\\\\n     returns = batch.batch[\\\\\\\\\\\\\\\"returns\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n     max_response_length = batch.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n-\\\\\\\\n-    prompt_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, :-max_response_length].bool()\\\\\\\\n     response_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -max_response_length:].bool()\\\\\\\\n \\\\\\\\n-    max_prompt_length = prompt_mask.size(-1)\\\\\\\\n-    prompt_length = prompt_mask.sum(-1).float()\\\\\\\\n-    response_length = response_mask.sum(-1).float()\\\\\\\\n-\\\\\\\\n     valid_adv = torch.masked_select(advantages, response_mask)\\\\\\\\n     valid_returns = torch.masked_select(returns, response_mask)\\\\\\\\n \\\\\\\\n@@ -49,7 +69,7 @@ def compute_data_metrics(batch: DataProto, use_critic: bool = False) -> Dict[str\\\\\\\\n         return_diff_var = torch.var(valid_returns - valid_values)\\\\\\\\n         return_var = torch.var(valid_returns)\\\\\\\\n \\\\\\\\n-    metrics = {\\\\\\\\n+    return {\\\\\\\\n         # score\\\\\\\\n         \\\\\\\\\\\\\\\"critic/score/mean\\\\\\\\\\\\\\\": torch.mean(sequence_score).detach().item(),\\\\\\\\n         \\\\\\\\\\\\\\\"critic/score/max\\\\\\\\\\\\\\\": torch.max(sequence_score).detach().item(),\\\\\\\\n@@ -78,20 +98,8 @@ def compute_data_metrics(batch: DataProto, use_critic: bool = False) -> Dict[str\\\\\\\\n             if use_critic\\\\\\\\n             else {}\\\\\\\\n         ),\\\\\\\\n-        # response length\\\\\\\\n-        \\\\\\\\\\\\\\\"response_length/mean\\\\\\\\\\\\\\\": torch.mean(response_length).detach().item(),\\\\\\\\n-        \\\\\\\\\\\\\\\"response_length/max\\\\\\\\\\\\\\\": torch.max(response_length).detach().item(),\\\\\\\\n-        \\\\\\\\\\\\\\\"response_length/min\\\\\\\\\\\\\\\": torch.min(response_length).detach().item(),\\\\\\\\n-        \\\\\\\\\\\\\\\"response_length/clip_ratio\\\\\\\\\\\\\\\": torch.mean(torch.eq(response_length, max_response_length).float())\\\\\\\\n-        .detach()\\\\\\\\n-        .item(),\\\\\\\\n-        # prompt length\\\\\\\\n-        \\\\\\\\\\\\\\\"prompt_length/mean\\\\\\\\\\\\\\\": torch.mean(prompt_length).detach().item(),\\\\\\\\n-        \\\\\\\\\\\\\\\"prompt_length/max\\\\\\\\\\\\\\\": torch.max(prompt_length).detach().item(),\\\\\\\\n-        \\\\\\\\\\\\\\\"prompt_length/min\\\\\\\\\\\\\\\": torch.min(prompt_length).detach().item(),\\\\\\\\n-        \\\\\\\\\\\\\\\"prompt_length/clip_ratio\\\\\\\\\\\\\\\": torch.mean(torch.eq(prompt_length, max_prompt_length).float()).detach().item(),\\\\\\\\n+        **compute_length_metrics(batch),\\\\\\\\n     }\\\\\\\\n-    return metrics\\\\\\\\n \\\\\\\\n \\\\\\\\n def compute_timing_metrics(batch: DataProto, timing_raw: Dict[str, float]) -> Dict[str, Any]:\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"817ced5d41d8db5d34db02cc03e994ec58b1c1eb\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/ray_trainer.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":13,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":15,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fray_trainer.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -46,7 +46,13 @@\\\\\\\\n from . import core_algos\\\\\\\\n from .config import PPOConfig\\\\\\\\n from .core_algos import AdvantageEstimator, FixedKLController, KLController, compute_kl, get_kl_controller\\\\\\\\n-from .metrics import compute_data_metrics, compute_throughout_metrics, compute_timing_metrics, reduce_metrics\\\\\\\\n+from .metrics import (\\\\\\\\n+    compute_data_metrics,\\\\\\\\n+    compute_length_metrics,\\\\\\\\n+    compute_throughout_metrics,\\\\\\\\n+    compute_timing_metrics,\\\\\\\\n+    reduce_metrics,\\\\\\\\n+)\\\\\\\\n \\\\\\\\n \\\\\\\\n class Role(IntEnum):\\\\\\\\n@@ -380,6 +386,7 @@ def _validate(self) -> Dict[str, Any]:\\\\\\\\n         # Lists to collect samples for the table\\\\\\\\n         sample_inputs, sample_outputs, sample_labels, sample_scores = [], [], [], []\\\\\\\\n         reward_metrics_lst = defaultdict(list)\\\\\\\\n+        length_metrics_lst = defaultdict(list)\\\\\\\\n         print(\\\\\\\\\\\\\\\"Start validation...\\\\\\\\\\\\\\\")\\\\\\\\n         self.actor_rollout_ref_wg.prepare_rollout_engine()\\\\\\\\n         for batch_dict in self.val_dataloader:\\\\\\\\n@@ -420,12 +427,16 @@ def _validate(self) -> Dict[str, Any]:\\\\\\\\n             for key, value in reward_metrics.items():\\\\\\\\n                 reward_metrics_lst[key].extend(value)\\\\\\\\n \\\\\\\\n+            for key, value in compute_length_metrics(test_batch).items():\\\\\\\\n+                length_metrics_lst[key].append(value)\\\\\\\\n+\\\\\\\\n         self.actor_rollout_ref_wg.release_rollout_engine()\\\\\\\\n         self._maybe_log_val_generations(sample_inputs, sample_outputs, sample_labels, sample_scores)\\\\\\\\n         self.val_reward_score = torch.cat(reward_tensor_lst, dim=0).sum(-1).mean().item()\\\\\\\\n         val_reward_metrics = {f\\\\\\\\\\\\\\\"val/{key}_reward\\\\\\\\\\\\\\\": value for key, value in reduce_metrics(reward_metrics_lst).items()}\\\\\\\\n+        val_length_metrics = {f\\\\\\\\\\\\\\\"val_{key}\\\\\\\\\\\\\\\": value for key, value in reduce_metrics(length_metrics_lst).items()}\\\\\\\\n         print(\\\\\\\\\\\\\\\"Finish validation.\\\\\\\\\\\\\\\")\\\\\\\\n-        return {\\\\\\\\\\\\\\\"val/reward_score\\\\\\\\\\\\\\\": self.val_reward_score, **val_reward_metrics}\\\\\\\\n+        return {\\\\\\\\\\\\\\\"val/reward_score\\\\\\\\\\\\\\\": self.val_reward_score, **val_reward_metrics, **val_length_metrics}\\\\\\\\n \\\\\\\\n     def _balance_batch(self, batch: DataProto, metrics: Dict[str, Any], logging_prefix: str = \\\\\\\\\\\\\\\"global_seqlen\\\\\\\\\\\\\\\") -> None:\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Reorder the data on single controller such that each dp rank gets similar total tokens\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"c853b53eaffbd8533e3d3cc5968883031ddadc7b\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/dataset.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Futils%2Fdataset.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Futils%2Fdataset.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fdataset.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -218,6 +218,7 @@ def __len__(self):\\\\\\\\n     def __getitem__(self, index):\\\\\\\\n         example: dict = self.dataset[index]\\\\\\\\n         messages = self._build_messages(example)\\\\\\\\n+        example.pop(self.prompt_key, None)\\\\\\\\n \\\\\\\\n         if self.image_key in example:\\\\\\\\n             prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"b7aeac131a1159430874a8c3f1389ad29db8f9d3\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/seqlen_balancing.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":34,\\\\\\\"deletions\\\\\\\":16,\\\\\\\"changes\\\\\\\":50,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Futils%2Fseqlen_balancing.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Futils%2Fseqlen_balancing.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fseqlen_balancing.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -14,12 +14,15 @@\\\\\\\\n \\\\\\\\n import copy\\\\\\\\n import heapq\\\\\\\\n+from itertools import chain\\\\\\\\n from typing import List, Tuple\\\\\\\\n \\\\\\\\n import torch\\\\\\\\n from tensordict import TensorDict\\\\\\\\n from torch import distributed as dist\\\\\\\\n \\\\\\\\n+from ..protocol import DataProto\\\\\\\\n+\\\\\\\\n \\\\\\\\n class Set:\\\\\\\\n     def __init__(self) -> None:\\\\\\\\n@@ -148,8 +151,9 @@ def greedy_partition(seqlen_list: List[int], k_partitions: int, equal_size: bool\\\\\\\\n \\\\\\\\n \\\\\\\\n def get_seqlen_balanced_partitions(seqlen_list: List[int], k_partitions: int, equal_size: bool):\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"get order of seq lengths to make partitions balanced, this is\\\\\\\\n-        used in balacing sum of seqlength across dp ranks and microbatches\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Get order of seq lengths to make partitions balanced, this is\\\\\\\\n+    used in balacing sum of seqlength across dp ranks and microbatches.\\\\\\\\n+\\\\\\\\n     Parameters:\\\\\\\\n         seqlen_list (List[int]):\\\\\\\\n             seq lengths of each items\\\\\\\\n@@ -159,6 +163,7 @@ def get_seqlen_balanced_partitions(seqlen_list: List[int], k_partitions: int, eq\\\\\\\\n             if True, number of items in each partitions must be equal.\\\\\\\\n             if False, only consider balancing the sum, each partition can have\\\\\\\\n             variable number of items\\\\\\\\n+\\\\\\\\n     Returns:\\\\\\\\n         partitions (List[List[int]]):\\\\\\\\n             return k_partitions list containing the index of items.\\\\\\\\n@@ -228,29 +233,22 @@ def rearrange_micro_batches(batch: TensorDict, max_token_len, dp_group=None):\\\\\\\\n     assert max_token_len >= max_seq_len, (\\\\\\\\n         f\\\\\\\\\\\\\\\"max_token_len must be greater than the sequence length. Got {max_token_len=} and {max_seq_len=}\\\\\\\\\\\\\\\"\\\\\\\\n     )\\\\\\\\n-\\\\\\\\n-    seq_len_effective: torch.Tensor = batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"].sum(dim=1)\\\\\\\\n-    total_seqlen = seq_len_effective.sum().item()\\\\\\\\n+    effective_seqlen = torch.sum(batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"], dim=-1)\\\\\\\\n+    total_seqlen = effective_seqlen.sum().item()\\\\\\\\n     num_micro_batches = ceildiv(total_seqlen, max_token_len)\\\\\\\\n     if dist.is_initialized():\\\\\\\\n         num_micro_batches = torch.tensor([num_micro_batches], device=\\\\\\\\\\\\\\\"cuda\\\\\\\\\\\\\\\")\\\\\\\\n         dist.all_reduce(num_micro_batches, op=dist.ReduceOp.MAX, group=dp_group)\\\\\\\\n         num_micro_batches = num_micro_batches.cpu().item()\\\\\\\\n \\\\\\\\n-    seq_len_effective = seq_len_effective.tolist()\\\\\\\\n-    assert num_micro_batches <= len(seq_len_effective)\\\\\\\\n-\\\\\\\\n-    micro_bsz_idx = get_seqlen_balanced_partitions(seq_len_effective, num_micro_batches, equal_size=False)\\\\\\\\n+    effective_seqlen = effective_seqlen.tolist()\\\\\\\\n+    assert num_micro_batches <= len(effective_seqlen)\\\\\\\\n+    micro_bsz_idx = get_seqlen_balanced_partitions(effective_seqlen, num_micro_batches, equal_size=False)\\\\\\\\n \\\\\\\\n     micro_batches = []\\\\\\\\n-\\\\\\\\n     for partition in micro_bsz_idx:\\\\\\\\n-        curr_micro_batch = []\\\\\\\\n-        for idx in partition:\\\\\\\\n-            curr_micro_batch.append(batch[idx : idx + 1])\\\\\\\\n-        curr_micro_batch = torch.cat(curr_micro_batch)\\\\\\\\n-\\\\\\\\n-        micro_batches.append(curr_micro_batch)\\\\\\\\n+        curr_micro_batch = [batch[idx] for idx in partition]\\\\\\\\n+        micro_batches.append(torch.stack(curr_micro_batch))\\\\\\\\n \\\\\\\\n     return micro_batches, micro_bsz_idx\\\\\\\\n \\\\\\\\n@@ -262,3 +260,23 @@ def get_reverse_idx(idx_map):\\\\\\\\n         reverse_idx_map[idx] = i\\\\\\\\n \\\\\\\\n     return reverse_idx_map\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def prepare_dynamic_batch(data: DataProto, max_token_len: int) -> tuple[list[DataProto], list[list[int]]]:\\\\\\\\n+    batch, batch_idx_list = rearrange_micro_batches(data.batch, max_token_len=max_token_len)\\\\\\\\n+    micro_batches = []\\\\\\\\n+    for i, batch_idx in enumerate(batch_idx_list):\\\\\\\\n+        tensors = dict(batch[i])\\\\\\\\n+        non_tensors = {}\\\\\\\\n+        for key in data.non_tensor_batch.keys():\\\\\\\\n+            non_tensors[key] = [data.non_tensor_batch[key][idx] for idx in batch_idx]\\\\\\\\n+\\\\\\\\n+        micro_batches.append(DataProto.from_dict(tensors, non_tensors))\\\\\\\\n+\\\\\\\\n+    return micro_batches, batch_idx_list\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def restore_dynamic_batch(data: torch.Tensor, batch_idx_list: List[List[int]]) -> torch.Tensor:\\\\\\\\n+    indices = list(chain.from_iterable(batch_idx_list))\\\\\\\\n+    revert_indices = torch.tensor(get_reverse_idx(indices), dtype=torch.long)\\\\\\\\n+    return data[revert_indices]\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"96c27e9c9ac095d0ad88da39c3ed8ae4343888b3\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/ulysses.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":50,\\\\\\\"deletions\\\\\\\":21,\\\\\\\"changes\\\\\\\":71,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Futils%2Fulysses.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Futils%2Fulysses.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fulysses.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -14,7 +14,7 @@\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n Utilities for DeepSpeed Ulysses Sequence Parallelism.\\\\\\\\n DeepSpeed Ulysses Paper: https://arxiv.org/abs/2309.14509\\\\\\\\n-Inspired from: https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/sequence/layer.py\\\\\\\\n+Inspired from: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/sequence/layer.py\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n from typing import Any, Optional, Tuple\\\\\\\\n@@ -181,10 +181,7 @@ def forward(\\\\\\\\n \\\\\\\\n     @staticmethod\\\\\\\\n     def backward(ctx: Any, *grad_output: Tensor) -> Tuple[None, Tensor, None, None]:\\\\\\\\n-        if ctx.async_op:\\\\\\\\n-            input_t = torch.cat(grad_output[1:], dim=ctx.gather_dim).contiguous()\\\\\\\\n-        else:\\\\\\\\n-            input_t = grad_output[0]\\\\\\\\n+        input_t = torch.cat(grad_output[1:], dim=ctx.gather_dim).contiguous() if ctx.async_op else grad_output[0]\\\\\\\\n         return (\\\\\\\\n             None,\\\\\\\\n             all_to_all_tensor(input_t, ctx.gather_dim, ctx.scatter_dim, ctx.group, False),\\\\\\\\n@@ -246,8 +243,22 @@ def gather_outputs_and_unpad(\\\\\\\\n     grad_scaler: bool = True,\\\\\\\\n     group: Optional[dist.ProcessGroup] = None,\\\\\\\\n ):\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    Gather a tensor across a process group and optionally unpad its padded elements.\\\\\\\\n+\\\\\\\\n+    Args:\\\\\\\\n+        x (Tensor): Input tensor to gather.\\\\\\\\n+        gather_dim (int): Dimension along which to gather across ranks.\\\\\\\\n+        unpad_dim (int, optional): Dimension from which to remove padding. If None, no unpadding.\\\\\\\\n+        padding_size (int): Number of padding elements to remove on `unpad_dim`. Defaults to 0.\\\\\\\\n+        grad_scaler (bool): Whether to apply gradient scaling during gather. Defaults to True.\\\\\\\\n+        group (ProcessGroup, optional): Process group for gathering. If None, uses\\\\\\\\n+            `get_ulysses_sequence_parallel_group()`. If still None, returns `x` unchanged.\\\\\\\\n+\\\\\\\\n+    Returns:\\\\\\\\n+        Tensor: The gathered tensor, with padding removed if requested.\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     group = get_ulysses_sequence_parallel_group() if group is None else group\\\\\\\\n-    # sp_size = get_ulysses_sequence_parallel_world_size()\\\\\\\\n     if group is None:\\\\\\\\n         return x\\\\\\\\n     x = Gather.apply(group, x, gather_dim, grad_scaler)\\\\\\\\n@@ -259,15 +270,35 @@ def gather_outputs_and_unpad(\\\\\\\\n     return x\\\\\\\\n \\\\\\\\n \\\\\\\\n+def ulysses_pad(input_ids_rmpad: torch.Tensor, position_ids_rmpad: Optional[torch.Tensor] = None, sp_size: int = 1):\\\\\\\\n+    if position_ids_rmpad is not None:\\\\\\\\n+        assert position_ids_rmpad.size(-2) == 1\\\\\\\\n+        assert input_ids_rmpad.size(-1) == position_ids_rmpad.size(-1)\\\\\\\\n+\\\\\\\\n+    if sp_size <= 1:\\\\\\\\n+        return input_ids_rmpad, position_ids_rmpad, 0\\\\\\\\n+\\\\\\\\n+    _, total_seq_len = input_ids_rmpad.shape\\\\\\\\n+    pad_size = (sp_size - total_seq_len % sp_size) % sp_size\\\\\\\\n+    if pad_size > 0:\\\\\\\\n+        input_ids_rmpad = torch.nn.functional.pad(input_ids_rmpad, (0, pad_size), value=0)\\\\\\\\n+        if position_ids_rmpad is not None:\\\\\\\\n+            pad_pos_ids = torch.arange(pad_size, device=position_ids_rmpad.device).unsqueeze(0)\\\\\\\\n+            if position_ids_rmpad.dim() == 3:\\\\\\\\n+                pad_pos_ids = pad_pos_ids.unsqueeze(0).repeat(3, 1, 1)\\\\\\\\n+            position_ids_rmpad = torch.cat((position_ids_rmpad, pad_pos_ids), dim=-1)\\\\\\\\n+\\\\\\\\n+    return input_ids_rmpad, position_ids_rmpad, pad_size\\\\\\\\n+\\\\\\\\n+\\\\\\\\n def ulysses_pad_and_slice_inputs(\\\\\\\\n     input_ids_rmpad: torch.Tensor, position_ids_rmpad: Optional[torch.Tensor] = None, sp_size: int = 1\\\\\\\\n ):\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     Pad and slice input_ids to be divisible by sp_size\\\\\\\\n     Pad position_ids to be divisible by sp_size.\\\\\\\\n \\\\\\\\n-    Note both input_ids_rmpad and position_ids_rmpad will be padded,\\\\\\\\n-    but only input_ids will be sliced.\\\\\\\\n+    Note both input_ids_rmpad and position_ids_rmpad will be padded and sliced.\\\\\\\\n \\\\\\\\n     The is the utility of pre-forward for ulysses sequence parallelism\\\\\\\\n \\\\\\\\n@@ -281,18 +312,16 @@ def ulysses_pad_and_slice_inputs(\\\\\\\\n         torch.Tensor: padded and sliced position_ids\\\\\\\\n         int: pad size\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    input_ids_rmpad, position_ids_rmpad, pad_size = ulysses_pad(input_ids_rmpad, position_ids_rmpad, sp_size)\\\\\\\\n+    input_ids_rmpad = slice_input_tensor(input_ids_rmpad, dim=-1, padding=False)\\\\\\\\n     if position_ids_rmpad is not None:\\\\\\\\n-        assert position_ids_rmpad.size(0) == 1\\\\\\\\n-        assert input_ids_rmpad.size(1) == position_ids_rmpad.size(1)\\\\\\\\n-    if sp_size <= 1:\\\\\\\\n-        return input_ids_rmpad, position_ids_rmpad, 0\\\\\\\\n-    _, total_seq_len = input_ids_rmpad.shape\\\\\\\\n-    pad_size = (sp_size - total_seq_len % sp_size) % sp_size\\\\\\\\n-    if pad_size > 0:\\\\\\\\n-        input_ids_rmpad = torch.nn.functional.pad(input_ids_rmpad, (0, pad_size), value=0)\\\\\\\\n-        if position_ids_rmpad is not None:\\\\\\\\n-            pad_pos_ids = torch.arange(pad_size, device=position_ids_rmpad.device).unsqueeze(0)\\\\\\\\n-            position_ids_rmpad = torch.cat((position_ids_rmpad, pad_pos_ids), dim=-1)\\\\\\\\n-    # we don't need to slice position ids\\\\\\\\n-    input_ids_rmpad = slice_input_tensor(input_ids_rmpad, dim=1, padding=False)\\\\\\\\n+        position_ids_rmpad = slice_input_tensor(position_ids_rmpad, dim=-1, padding=False)\\\\\\\\n+\\\\\\\\n     return input_ids_rmpad, position_ids_rmpad, pad_size\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def validate_ulysses_config(num_heads, ulysses_sequence_size):\\\\\\\\n+    if ulysses_sequence_size > 1:\\\\\\\\n+        assert num_heads % ulysses_sequence_size == 0, (\\\\\\\\n+            f\\\\\\\\\\\\\\\"num_heads ({num_heads}) must be divisible by ulysses sequence size({ulysses_sequence_size})\\\\\\\\\\\\\\\"\\\\\\\\n+        )\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"4a8fcba26e7627463fe33de449eea8116cef5afc\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/actor/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":4,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Factor%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Factor%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Factor%2Fconfig.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -96,9 +96,12 @@ class ActorConfig:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"number of ppo epochs for each rollout batch\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     padding_free: bool = True\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"use padding-free training\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    dynamic_batching: bool = True\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"enable dynamic batching\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     ulysses_size: int = 1\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"ulysses sequence parallel size\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     use_torch_compile: bool = True\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"enable torch compile\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     model: ModelConfig = field(default_factory=ModelConfig)\\\\\\\\n     optim: OptimConfig = field(default_factory=OptimConfig)\\\\\\\\n     fsdp: FSDPConfig = field(default_factory=FSDPConfig)\\\\\\\\n@@ -119,5 +122,6 @@ class RefConfig:\\\\\\\\n     # below are auto keys\\\\\\\\n     micro_batch_size_per_device_for_experience: int = field(default=-1, init=False)\\\\\\\\n     padding_free: bool = field(default=False, init=False)\\\\\\\\n+    dynamic_batching: bool = field(default=False, init=False)\\\\\\\\n     ulysses_size: int = field(default=1, init=False)\\\\\\\\n     use_torch_compile: bool = field(default=True, init=False)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"edeffd50de3825644cd94e2d137f775a1886d34c\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/actor/dp_actor.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":30,\\\\\\\"deletions\\\\\\\":16,\\\\\\\"changes\\\\\\\":46,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Factor%2Fdp_actor.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Factor%2Fdp_actor.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Factor%2Fdp_actor.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -29,6 +29,7 @@\\\\\\\\n from ...trainer.core_algos import average_loss, compute_kl, compute_policy_loss\\\\\\\\n from ...utils import torch_functional as VF\\\\\\\\n from ...utils.py_functional import append_to_dict\\\\\\\\n+from ...utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\\\\\\\\n from ...utils.ulysses import gather_outputs_and_unpad, ulysses_pad_and_slice_inputs\\\\\\\\n from .base import BasePPOActor\\\\\\\\n from .config import ActorConfig\\\\\\\\n@@ -89,9 +90,7 @@ def _forward_micro_batch(self, micro_batch: Dict[str, torch.Tensor], temperature\\\\\\\\n                     multi_modal_inputs[key] = None\\\\\\\\n \\\\\\\\n         if self.config.padding_free:\\\\\\\\n-            input_ids_rmpad, indices, *_ = unpad_input(\\\\\\\\n-                input_ids.unsqueeze(-1), attention_mask\\\\\\\\n-            )  # input_ids_rmpad (total_nnz, ...)\\\\\\\\n+            input_ids_rmpad, indices, *_ = unpad_input(input_ids.unsqueeze(-1), attention_mask)  # (total_nnz, 1)\\\\\\\\n             input_ids_rmpad = input_ids_rmpad.transpose(0, 1)  # (1, total_nnz)\\\\\\\\n \\\\\\\\n             # unpad the position_ids to align the rotary\\\\\\\\n@@ -197,9 +196,13 @@ def compute_log_prob(self, data: DataProto) -> torch.Tensor:\\\\\\\\n         select_keys = [\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\"]\\\\\\\\n         non_tensor_select_keys = [\\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n-        micro_batches = data.select(select_keys, non_tensor_select_keys).split(\\\\\\\\n-            self.config.micro_batch_size_per_device_for_experience\\\\\\\\n-        )\\\\\\\\n+        data = data.select(select_keys, non_tensor_select_keys)\\\\\\\\n+        if self.config.dynamic_batching:\\\\\\\\n+            max_token_len = self.config.micro_batch_size_per_device_for_experience * data.batch[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+            micro_batches, batch_idx_list = prepare_dynamic_batch(data, max_token_len=max_token_len)\\\\\\\\n+        else:\\\\\\\\n+            micro_batches = data.split(self.config.micro_batch_size_per_device_for_experience)\\\\\\\\n+\\\\\\\\n         log_probs_lst = []\\\\\\\\n         if self.rank == 0:\\\\\\\\n             micro_batches = tqdm(micro_batches, desc=\\\\\\\\\\\\\\\"Compute log probs\\\\\\\\\\\\\\\", position=1)\\\\\\\\n@@ -210,6 +213,10 @@ def compute_log_prob(self, data: DataProto) -> torch.Tensor:\\\\\\\\n             log_probs_lst.append(log_probs)\\\\\\\\n \\\\\\\\n         log_probs = torch.concat(log_probs_lst, dim=0)\\\\\\\\n+\\\\\\\\n+        if self.config.dynamic_batching:\\\\\\\\n+            log_probs = restore_dynamic_batch(log_probs, batch_idx_list)\\\\\\\\n+\\\\\\\\n         return log_probs\\\\\\\\n \\\\\\\\n     def update_policy(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n@@ -230,19 +237,24 @@ def update_policy(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n                 mini_batches = tqdm(mini_batches, desc=\\\\\\\\\\\\\\\"Train mini-batches\\\\\\\\\\\\\\\", position=1)\\\\\\\\n \\\\\\\\n             for mini_batch in mini_batches:\\\\\\\\n-                gradient_accumulation = (\\\\\\\\n-                    self.config.global_batch_size_per_device // self.config.micro_batch_size_per_device_for_update\\\\\\\\n-                )\\\\\\\\n-                micro_batches = mini_batch.split(self.config.micro_batch_size_per_device_for_update)\\\\\\\\n+                response_length = mini_batch.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+                response_mask = mini_batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -response_length:]\\\\\\\\n+                total_response_tokens = torch.sum(response_mask)\\\\\\\\n+\\\\\\\\n+                if self.config.dynamic_batching:\\\\\\\\n+                    max_input_len = mini_batch.batch[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+                    max_token_len = self.config.micro_batch_size_per_device_for_update * max_input_len\\\\\\\\n+                    micro_batches, _ = prepare_dynamic_batch(mini_batch, max_token_len=max_token_len)\\\\\\\\n+                else:\\\\\\\\n+                    micro_batches = mini_batch.split(self.config.micro_batch_size_per_device_for_update)\\\\\\\\n+\\\\\\\\n                 if self.rank == 0:\\\\\\\\n                     micro_batches = tqdm(micro_batches, desc=\\\\\\\\\\\\\\\"Update policy\\\\\\\\\\\\\\\", position=2)\\\\\\\\n \\\\\\\\n                 for micro_batch in micro_batches:\\\\\\\\n                     model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}\\\\\\\\n-                    responses = model_inputs[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-                    response_length = responses.size(1)\\\\\\\\n-                    attention_mask = model_inputs[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n-                    response_mask = attention_mask[:, -response_length:]\\\\\\\\n+                    response_length = model_inputs[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+                    response_mask = model_inputs[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -response_length:]\\\\\\\\n                     old_log_probs = model_inputs[\\\\\\\\\\\\\\\"old_log_probs\\\\\\\\\\\\\\\"]\\\\\\\\n                     advantages = model_inputs[\\\\\\\\\\\\\\\"advantages\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n@@ -268,11 +280,13 @@ def update_policy(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n                             kl_penalty=self.config.kl_penalty,\\\\\\\\n                         )\\\\\\\\n                         kl_loss = average_loss(kld, response_mask, mode=self.config.loss_avg_mode)\\\\\\\\n-                        pg_loss = pg_loss + kl_loss * self.config.kl_coef\\\\\\\\n+                        loss = pg_loss + kl_loss * self.config.kl_coef\\\\\\\\n                         metrics[\\\\\\\\\\\\\\\"actor/kl_loss\\\\\\\\\\\\\\\"] = kl_loss.detach().item()\\\\\\\\n                         metrics[\\\\\\\\\\\\\\\"actor/kl_coef\\\\\\\\\\\\\\\"] = self.config.kl_coef\\\\\\\\n+                    else:\\\\\\\\n+                        loss = pg_loss\\\\\\\\n \\\\\\\\n-                    loss = pg_loss / gradient_accumulation\\\\\\\\n+                    loss = loss * torch.sum(response_mask) / total_response_tokens\\\\\\\\n                     loss.backward()\\\\\\\\n \\\\\\\\n                     batch_metrics = {\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"10381150c0519cfad00172972a01bbf637f1ff5f\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Fconfig.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -48,5 +48,6 @@ class WorkerConfig:\\\\\\\\n     def post_init(self):\\\\\\\\n         self.ref.micro_batch_size_per_device_for_experience = self.actor.micro_batch_size_per_device_for_experience\\\\\\\\n         self.ref.padding_free = self.actor.padding_free\\\\\\\\n+        self.ref.dynamic_batching = self.actor.dynamic_batching\\\\\\\\n         self.ref.ulysses_size = self.actor.ulysses_size\\\\\\\\n         self.ref.use_torch_compile = self.actor.use_torch_compile\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"07390a4721a045b366c8203e883ac1a901a94109\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/critic/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Fcritic%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Fcritic%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Fcritic%2Fconfig.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -39,6 +39,8 @@ class CriticConfig:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"number of ppo epochs for each rollout batch\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     padding_free: bool = False\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"use padding-free training\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    dynamic_batching: bool = True\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"enable dynamic batching\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     ulysses_size: int = 1\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"ulysses sequence parallel size\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     model: ModelConfig = field(default_factory=ModelConfig)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"13693bf7cd8f43e9da73f3859bbdf8042a1e0342\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/critic/dp_critic.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":31,\\\\\\\"deletions\\\\\\\":20,\\\\\\\"changes\\\\\\\":51,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Fcritic%2Fdp_critic.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Fcritic%2Fdp_critic.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Fcritic%2Fdp_critic.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -26,8 +26,8 @@\\\\\\\\n \\\\\\\\n from ...protocol import DataProto\\\\\\\\n from ...trainer.core_algos import compute_value_loss\\\\\\\\n-from ...utils import torch_functional as VF\\\\\\\\n from ...utils.py_functional import append_to_dict\\\\\\\\n+from ...utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\\\\\\\\n from ...utils.ulysses import gather_outputs_and_unpad, ulysses_pad_and_slice_inputs\\\\\\\\n from .base import BasePPOCritic\\\\\\\\n from .config import CriticConfig\\\\\\\\n@@ -149,9 +149,13 @@ def compute_values(self, data: DataProto) -> torch.Tensor:\\\\\\\\n         select_keys = [\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\"]\\\\\\\\n         non_tensor_select_keys = [\\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n-        micro_batches = data.select(select_keys, non_tensor_select_keys).split(\\\\\\\\n-            self.config.micro_batch_size_per_device_for_experience\\\\\\\\n-        )\\\\\\\\n+        data = data.select(select_keys, non_tensor_select_keys)\\\\\\\\n+        if self.config.dynamic_batching:\\\\\\\\n+            max_token_len = self.config.micro_batch_size_per_device_for_experience * data.batch[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+            micro_batches, batch_idx_list = prepare_dynamic_batch(data, max_token_len=max_token_len)\\\\\\\\n+        else:\\\\\\\\n+            micro_batches = data.split(self.config.micro_batch_size_per_device_for_experience)\\\\\\\\n+\\\\\\\\n         values_lst = []\\\\\\\\n         if self.rank == 0:\\\\\\\\n             micro_batches = tqdm(micro_batches, desc=\\\\\\\\\\\\\\\"Compute values\\\\\\\\\\\\\\\", position=1)\\\\\\\\n@@ -162,10 +166,12 @@ def compute_values(self, data: DataProto) -> torch.Tensor:\\\\\\\\n             values_lst.append(values)\\\\\\\\n \\\\\\\\n         values = torch.concat(values_lst, dim=0)\\\\\\\\n-        responses = data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-        attention_mask = data.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n-        response_length = responses.size(1)\\\\\\\\n-        values = values * attention_mask[:, -response_length:]\\\\\\\\n+\\\\\\\\n+        if self.config.dynamic_batching:\\\\\\\\n+            values = restore_dynamic_batch(values, batch_idx_list)\\\\\\\\n+\\\\\\\\n+        response_length = data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(1)\\\\\\\\n+        values = values * data.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -response_length:]  # only action tokens have values\\\\\\\\n         return values\\\\\\\\n \\\\\\\\n     def update_critic(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n@@ -184,38 +190,43 @@ def update_critic(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n                 mini_batches = tqdm(mini_batches, desc=\\\\\\\\\\\\\\\"Train mini-batches\\\\\\\\\\\\\\\", position=1)\\\\\\\\n \\\\\\\\n             for mini_batch in mini_batches:\\\\\\\\n-                gradient_accumulation = (\\\\\\\\n-                    self.config.global_batch_size_per_device // self.config.micro_batch_size_per_device_for_update\\\\\\\\n-                )\\\\\\\\n-                micro_batches = mini_batch.split(self.config.micro_batch_size_per_device_for_update)\\\\\\\\n+                response_length = mini_batch.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+                response_mask = mini_batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -response_length:]\\\\\\\\n+                total_response_tokens = torch.sum(response_mask)\\\\\\\\n+\\\\\\\\n+                if self.config.dynamic_batching:\\\\\\\\n+                    max_input_len = mini_batch.batch[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+                    max_token_len = self.config.micro_batch_size_per_device_for_update * max_input_len\\\\\\\\n+                    micro_batches, _ = prepare_dynamic_batch(mini_batch, max_token_len=max_token_len)\\\\\\\\n+                else:\\\\\\\\n+                    micro_batches = mini_batch.split(self.config.micro_batch_size_per_device_for_update)\\\\\\\\n+\\\\\\\\n                 if self.rank == 0:\\\\\\\\n                     micro_batches = tqdm(micro_batches, desc=\\\\\\\\\\\\\\\"Update critic\\\\\\\\\\\\\\\", position=2)\\\\\\\\n \\\\\\\\n                 for micro_batch in micro_batches:\\\\\\\\n                     model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}\\\\\\\\n-                    responses = model_inputs[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-                    response_length = responses.size(1)\\\\\\\\n-                    attention_mask = model_inputs[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n-                    response_mask = attention_mask[:, -response_length:]\\\\\\\\n+                    response_length = model_inputs[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+                    response_mask = model_inputs[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -response_length:]\\\\\\\\n                     values = model_inputs[\\\\\\\\\\\\\\\"values\\\\\\\\\\\\\\\"]\\\\\\\\n                     returns = model_inputs[\\\\\\\\\\\\\\\"returns\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n                     vpreds = self._forward_micro_batch(model_inputs)\\\\\\\\n-                    vf_loss, vf_clipfrac = compute_value_loss(\\\\\\\\n+                    vf_loss, vf_metrics = compute_value_loss(\\\\\\\\n                         vpreds=vpreds,\\\\\\\\n                         returns=returns,\\\\\\\\n                         values=values,\\\\\\\\n                         response_mask=response_mask,\\\\\\\\n                         cliprange_value=self.config.cliprange_value,\\\\\\\\n                         loss_avg_mode=self.config.loss_avg_mode,\\\\\\\\n                     )\\\\\\\\n-                    loss = vf_loss / gradient_accumulation\\\\\\\\n+                    loss = vf_loss * torch.sum(response_mask) / total_response_tokens\\\\\\\\n                     loss.backward()\\\\\\\\n \\\\\\\\n                     batch_metrics = {\\\\\\\\n                         \\\\\\\\\\\\\\\"critic/vf_loss\\\\\\\\\\\\\\\": vf_loss.detach().item(),\\\\\\\\n-                        \\\\\\\\\\\\\\\"critic/vf_clipfrac\\\\\\\\\\\\\\\": vf_clipfrac.detach().item(),\\\\\\\\n-                        \\\\\\\\\\\\\\\"critic/vpred_mean\\\\\\\\\\\\\\\": VF.masked_mean(vpreds, response_mask).detach().item(),\\\\\\\\n+                        \\\\\\\\\\\\\\\"critic/vf_clipfrac\\\\\\\\\\\\\\\": vf_metrics[\\\\\\\\\\\\\\\"vf_clipfrac\\\\\\\\\\\\\\\"],\\\\\\\\n+                        \\\\\\\\\\\\\\\"critic/vpred_mean\\\\\\\\\\\\\\\": vf_metrics[\\\\\\\\\\\\\\\"vpred_mean\\\\\\\\\\\\\\\"],\\\\\\\\n                     }\\\\\\\\n                     append_to_dict(metrics, batch_metrics)\\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"06035f16908c958220571314fae4c582eb651118\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/fsdp_workers.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":3,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Ffsdp_workers.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -518,16 +518,17 @@ def update_actor(self, data: DataProto):\\\\\\\\n             ) / (1024**3)\\\\\\\\n             metrics[\\\\\\\\\\\\\\\"perf/cpu_memory_used_gb\\\\\\\\\\\\\\\"] = psutil.virtual_memory().used / (1024**3)\\\\\\\\n \\\\\\\\n-            self.lr_scheduler.step()\\\\\\\\n             lr = self.lr_scheduler.get_last_lr()[0]\\\\\\\\n             metrics[\\\\\\\\\\\\\\\"actor/lr\\\\\\\\\\\\\\\"] = lr\\\\\\\\n+            self.lr_scheduler.step()\\\\\\\\n \\\\\\\\n             # Metrics should be in non_tensor_batch instead of meta_info, as DataProto not concat meta_info.\\\\\\\\n             output = DataProto(\\\\\\\\n                 non_tensor_batch={\\\\\\\\n                     key: np.array([value] if np.isscalar(value) else value) for key, value in metrics.items()\\\\\\\\n                 }\\\\\\\\n             )\\\\\\\\n+            output = self.ulysses_sharding_manager.postprocess_data(data=output)\\\\\\\\n \\\\\\\\n         if self._use_param_offload:\\\\\\\\n             offload_fsdp_model(self.fsdp_module)\\\\\\\"}],\\\\\\\"sha\\\\\\\":\\\\\\\"f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[worker] add dynamic batching (#417)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-14T10:41:21Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "I can see this commit only adds a `dynamic_batching: true` line but doesn't change the micro batch parameters I'm looking for. Let me check the previous commit that changed config:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"ref\\\": \\\"52ba694ab5185a967d64801681961501468e6ed8\\\"}\", \"include_output_fields\": [\"sha\", \"commit.author.date\", \"commit.author.name\", \"commit.message\", \"author.login\", \"files\"]}",
    "call_id": "toolu_01CXKt1krNmYSxpfE1c8ubkV",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01CXKt1krNmYSxpfE1c8ubkV",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"files\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"82d255d364b3d656de17a0fe2b29d152cb9a968e\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/config.yaml\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/52ba694ab5185a967d64801681961501468e6ed8/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/52ba694ab5185a967d64801681961501468e6ed8/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fconfig.yaml?ref=52ba694ab5185a967d64801681961501468e6ed8\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -61,15 +61,15 @@ worker:\\\\\\\\n   rollout:\\\\\\\\n     n: 5\\\\\\\\n     temperature: 1.0\\\\\\\\n-    top_p: 0.99\\\\\\\\n+    top_p: 1.0\\\\\\\\n     limit_images: 0\\\\\\\\n     gpu_memory_utilization: 0.6\\\\\\\\n     enforce_eager: false\\\\\\\\n     enable_chunked_prefill: false\\\\\\\\n     tensor_parallel_size: 2\\\\\\\\n     disable_tqdm: false\\\\\\\\n     val_override_config:\\\\\\\\n-      temperature: 0.5\\\\\\\\n+      temperature: 1.0\\\\\\\\n       n: 1\\\\\\\\n \\\\\\\\n   ref:\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"b05fc5d1daad225e3f8b80e5d5e724dbc952ae2f\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen3_14b_dapo17k_dapo.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/52ba694ab5185a967d64801681961501468e6ed8/examples%2Fqwen3_14b_dapo17k_dapo.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/52ba694ab5185a967d64801681961501468e6ed8/examples%2Fqwen3_14b_dapo17k_dapo.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen3_14b_dapo17k_dapo.sh?ref=52ba694ab5185a967d64801681961501468e6ed8\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -29,6 +29,7 @@ python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.max_num_batched_tokens=22528 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.val_override_config='{\\\\\\\\\\\\\\\"n\\\\\\\\\\\\\\\":16,\\\\\\\\\\\\\\\"temperature\\\\\\\\\\\\\\\":1.0,\\\\\\\\\\\\\\\"top_p\\\\\\\\\\\\\\\":0.7}' \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.gpu_memory_utilization=0.8 \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.rollout.tensor_parallel_size=4 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.reward.reward_function=./examples/reward_function/dapo.py:compute_score \\\\\\\\\\\\\\\\\\\\\\\\n     worker.reward.reward_function_kwargs='{\\\\\\\\\\\\\\\"max_response_length\\\\\\\\\\\\\\\":20480,\\\\\\\\\\\\\\\"overlong_buffer_length\\\\\\\\\\\\\\\":4096,\\\\\\\\\\\\\\\"overlong_penalty_factor\\\\\\\\\\\\\\\":1.0}' \\\\\\\\\\\\\\\\\\\\\\\\n     algorithm.disable_kl=True \\\\\\\\\\\\\\\\\\\\\\\"}],\\\\\\\"sha\\\\\\\":\\\\\\\"52ba694ab5185a967d64801681961501468e6ed8\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[examples] fix config (#420)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-14T13:20:26Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "This commit also doesn't contain the parameter changes I'm looking for. Let me check an earlier commit:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"ref\\\": \\\"c19396ea16edc383cb25fedb3da58ae08e08046e\\\"}\", \"include_output_fields\": [\"sha\", \"commit.author.date\", \"commit.author.name\", \"commit.message\", \"author.login\", \"files\"]}",
    "call_id": "toolu_01PwxabAVQJQR5S6FFZgDMaf",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01PwxabAVQJQR5S6FFZgDMaf",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"files\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"cdaad40eeb8d94c81d97ab59437cf6ef9eaeed0a\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/config.yaml\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/c19396ea16edc383cb25fedb3da58ae08e08046e/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/c19396ea16edc383cb25fedb3da58ae08e08046e/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fconfig.yaml?ref=c19396ea16edc383cb25fedb3da58ae08e08046e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -4,7 +4,9 @@ data:\\\\\\\\n   prompt_key: problem\\\\\\\\n   answer_key: answer\\\\\\\\n   image_key: images\\\\\\\\n+  video_key: videos\\\\\\\\n   image_dir: null\\\\\\\\n+  video_fps: 2.0\\\\\\\\n   max_prompt_length: 2048\\\\\\\\n   max_response_length: 2048\\\\\\\\n   rollout_batch_size: 512  # equivalent to verl's data.train_batch_size\\\\\\\"}],\\\\\\\"sha\\\\\\\":\\\\\\\"c19396ea16edc383cb25fedb3da58ae08e08046e\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[example] update config (#394)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-24T09:54:16Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Let me search more extensively through the commits. I need to check more commits to find the parameter change I'm looking for. Let me get more commits:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"list_commits\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\"}\", \"query_params\": \"{\\\"path\\\": \\\"examples/config.yaml\\\", \\\"per_page\\\": 50}\", \"include_output_fields\": [\"sha\", \"commit.author.date\", \"commit.author.name\", \"commit.message\", \"author.login\"]}",
    "call_id": "toolu_01U7AKBh5brjQZ5afk9JD4Eo",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01U7AKBh5brjQZ5afk9JD4Eo",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"list_commits\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"098931530606d22f867fd121b1dcb3225a43661f\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[misc] fix data proto (#458)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-08-08T12:05:58Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support auto resume (#425)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-15T05:48:35Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"52ba694ab5185a967d64801681961501468e6ed8\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[examples] fix config (#420)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-14T13:20:26Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[worker] add dynamic batching (#417)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-14T10:41:21Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"c19396ea16edc383cb25fedb3da58ae08e08046e\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[example] update config (#394)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-24T09:54:16Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"bd3b4eb275858421f12cac8d5d800ef353b0c8d0\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] fix video data (#386)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-23T12:24:55Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"0304d2debd27d7cbd60b0e10d63505b6a056851f\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[release] 0.3.1 (#370)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-19T06:55:58Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"2fc149222a9d6f01800c2b9fe660d46864e33b8c\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[examples] add qwen3_14b_dapo17k_dapo (#369)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-19T00:28:31Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Leijun Zhou\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"Saigyouji-Yuyuko1000\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"866b7cfd59bc700ccd3e25c0863d87a9b89c2343\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[worker] update reward manager (#360)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-17T17:27:53Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"e303179c31e582e0a7be2da9179ca9967848823f\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] add online filtering (#358)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-17T15:23:17Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Leijun Zhou\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"Saigyouji-Yuyuko1000\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"ad02af917a275550f4bfff95f9e1ce78be1fa8f1\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] repeat rollout and prepare filter (#346)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-14T18:55:19Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"572db6c119f63d195b96fbb52608f7e2dc36eb15\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] save best checkpoint (#343)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-13T20:12:53Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"6870bf93bffe68a7c9600f0d0258207484fe6189\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[worker] colocate actor and ref model (#342)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-13T19:14:28Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"98ebdcfb2d1e26bd71800b771dd9692f073e9373\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] fix position ids for qwen2vl mrope & add test (#339)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-13T13:59:59Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"647c30e94c63d3f8b8f0d65d00b0f7669595c41d\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[reward] support batch reward (#271)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-27T15:50:24Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"c024735f22cd0d202a3d9e2c263180bd861c65e9\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] support custom chat template (#270)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-27T15:11:48Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"8d28d058146058142e5e17e68616e43fed2a7469\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support async reward (#252)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-22T17:21:13Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"cba6cdfc03911bc69740ad352d9646e787e2acb0\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] add multi image dataset (#217)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-14T15:23:00Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"7895ba58fe2466aa18e539a44aaa79c9b7e7de64\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] enable user to filter overlong examples in RLHFDataset (#210)\\\\\\\\n\\\\\\\\nCo-authored-by: hoshi-hiyouga <hiyouga@buaa.edu.cn>\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-14T11:05:23Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Qunhong Zeng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"0x404\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"0c6b7fb0e9e1a22d3d8bdc650cca1ea251203297\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] update data configs (#214)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-14T10:55:45Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[core] separate score fn & vllm logit bias (#204)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-10T17:15:19Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[misc] minor update (#188)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-03T20:26:53Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[misc] support adamw bf16 (#171)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-27T15:44:01Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"cada92630e53290423da3ee38b37274ab9254fa6\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[tracking] add tensorboard (#170)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-27T14:34:11Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support 32b (#164)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-26T17:13:32Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support save limit & fix oom issue (#158)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-25T12:29:59Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support channel-wise reward (#155)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-24T12:43:43Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"1e675810f9c6de29a78ec09980d237a84a33747c\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support val sampling (#133)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-17T19:04:15Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"69b7029480d7fbeaa9f93d148545dbfbeb375836\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[core] remove entropy loss (#132)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-17T17:46:12Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"ac9bd29d8f18c0c2f979f2c4ac8773e675a7d35d\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[readme] add multinode instructions (#118)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-14T08:41:10Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}}],\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Let me check the most recent one:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"ref\\\": \\\"87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50\\\"}\", \"include_output_fields\": [\"sha\", \"commit.author.date\", \"commit.author.name\", \"commit.message\", \"author.login\", \"files\"]}",
    "call_id": "toolu_015PJwUty3Hh2xSDmnkaxpCj",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_015PJwUty3Hh2xSDmnkaxpCj",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"files\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"8752f177e9d8d9ef314eb6c588a6c7aad7cd7808\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/config.yaml\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":3,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fconfig.yaml?ref=87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -69,7 +69,8 @@ worker:\\\\\\\\n     tensor_parallel_size: 2\\\\\\\\n     disable_tqdm: false\\\\\\\\n     val_override_config:\\\\\\\\n-      temperature: 1.0\\\\\\\\n+      temperature: 0.6\\\\\\\\n+      top_p: 0.95\\\\\\\\n       n: 1\\\\\\\\n \\\\\\\\n   ref:\\\\\\\\n@@ -102,3 +103,4 @@ trainer:\\\\\\\\n   save_model_only: false\\\\\\\\n   save_checkpoint_path: null\\\\\\\\n   load_checkpoint_path: null\\\\\\\\n+  find_last_checkpoint: true\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"06979fbe2668c3dbf573645669f789abfc4adc33\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"tests/test_checkpoint.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"added\\\\\\\",\\\\\\\"additions\\\\\\\":50,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":50,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/tests%2Ftest_checkpoint.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/tests%2Ftest_checkpoint.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/tests%2Ftest_checkpoint.py?ref=87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -0,0 +1,50 @@\\\\\\\\n+# Copyright 2024 Bytedance Ltd. and/or its affiliates\\\\\\\\n+#\\\\\\\\n+# Licensed under the Apache License, Version 2.0 (the \\\\\\\\\\\\\\\"License\\\\\\\\\\\\\\\");\\\\\\\\n+# you may not use this file except in compliance with the License.\\\\\\\\n+# You may obtain a copy of the License at\\\\\\\\n+#\\\\\\\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\\\\\\\n+#\\\\\\\\n+# Unless required by applicable law or agreed to in writing, software\\\\\\\\n+# distributed under the License is distributed on an \\\\\\\\\\\\\\\"AS IS\\\\\\\\\\\\\\\" BASIS,\\\\\\\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\\\\\\\n+# See the License for the specific language governing permissions and\\\\\\\\n+# limitations under the License.\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+import json\\\\\\\\n+import os\\\\\\\\n+import shutil\\\\\\\\n+import uuid\\\\\\\\n+\\\\\\\\n+import pytest\\\\\\\\n+\\\\\\\\n+from verl.utils.checkpoint import CHECKPOINT_TRACKER, find_latest_ckpt, remove_obsolete_ckpt\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+@pytest.fixture\\\\\\\\n+def save_checkpoint_path():\\\\\\\\n+    ckpt_dir = os.path.join(\\\\\\\\\\\\\\\"checkpoints\\\\\\\\\\\\\\\", str(uuid.uuid4()))\\\\\\\\n+    os.makedirs(ckpt_dir, exist_ok=True)\\\\\\\\n+    yield ckpt_dir\\\\\\\\n+    shutil.rmtree(ckpt_dir, ignore_errors=True)\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def test_find_latest_ckpt(save_checkpoint_path):\\\\\\\\n+    with open(os.path.join(save_checkpoint_path, CHECKPOINT_TRACKER), \\\\\\\\\\\\\\\"w\\\\\\\\\\\\\\\") as f:\\\\\\\\n+        json.dump({\\\\\\\\\\\\\\\"last_global_step\\\\\\\\\\\\\\\": 10}, f, ensure_ascii=False, indent=2)\\\\\\\\n+\\\\\\\\n+    assert find_latest_ckpt(save_checkpoint_path) is None\\\\\\\\n+    os.makedirs(os.path.join(save_checkpoint_path, \\\\\\\\\\\\\\\"global_step_10\\\\\\\\\\\\\\\"), exist_ok=True)\\\\\\\\n+    assert find_latest_ckpt(save_checkpoint_path) == os.path.join(save_checkpoint_path, \\\\\\\\\\\\\\\"global_step_10\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def test_remove_obsolete_ckpt(save_checkpoint_path):\\\\\\\\n+    for step in range(5, 30, 5):\\\\\\\\n+        os.makedirs(os.path.join(save_checkpoint_path, f\\\\\\\\\\\\\\\"global_step_{step}\\\\\\\\\\\\\\\"), exist_ok=True)\\\\\\\\n+\\\\\\\\n+    remove_obsolete_ckpt(save_checkpoint_path, global_step=30, best_global_step=10, save_limit=3)\\\\\\\\n+    for step in range(5, 30, 5):\\\\\\\\n+        is_exist = step in [10, 25]\\\\\\\\n+        assert os.path.exists(os.path.join(save_checkpoint_path, f\\\\\\\\\\\\\\\"global_step_{step}\\\\\\\\\\\\\\\")) == is_exist\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"726fbcb56502e68c734a351909d325b3d28fb568\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/verl%2Ftrainer%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/verl%2Ftrainer%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fconfig.py?ref=87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -141,6 +141,8 @@ class TrainerConfig:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"save checkpoint path, if not specified, use `checkpoints/project_name/experiment_name`\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     load_checkpoint_path: Optional[str] = None\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"load checkpoint path\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    find_last_checkpoint: bool = True\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"automatically find the last checkpoint in the save checkpoint path to resume training\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n     def post_init(self):\\\\\\\\n         if self.save_checkpoint_path is None:\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"12cd7f2aa9fca06191665db5047af05a087f6992\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/metrics.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":5,\\\\\\\"deletions\\\\\\\":10,\\\\\\\"changes\\\\\\\":15,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/verl%2Ftrainer%2Fmetrics.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/verl%2Ftrainer%2Fmetrics.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fmetrics.py?ref=87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -26,27 +26,22 @@ def reduce_metrics(metrics: Dict[str, List[Any]]) -> Dict[str, Any]:\\\\\\\\n \\\\\\\\n def compute_length_metrics(batch: DataProto) -> Dict[str, Any]:\\\\\\\\n     max_response_length = batch.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+    max_prompt_length = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"].size(-1) - max_response_length\\\\\\\\n \\\\\\\\n-    prompt_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, :-max_response_length].bool()\\\\\\\\n-    response_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -max_response_length:].bool()\\\\\\\\n-\\\\\\\\n-    max_prompt_length = prompt_mask.size(-1)\\\\\\\\n-    prompt_length = prompt_mask.sum(-1).float()\\\\\\\\n-    response_length = response_mask.sum(-1).float()\\\\\\\\n+    prompt_length = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, :-max_response_length].sum(-1).float()\\\\\\\\n+    response_length = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -max_response_length:].sum(-1).float()\\\\\\\\n \\\\\\\\n     return {\\\\\\\\n         # response length\\\\\\\\n         \\\\\\\\\\\\\\\"response_length/mean\\\\\\\\\\\\\\\": torch.mean(response_length).detach().item(),\\\\\\\\n         \\\\\\\\\\\\\\\"response_length/max\\\\\\\\\\\\\\\": torch.max(response_length).detach().item(),\\\\\\\\n         \\\\\\\\\\\\\\\"response_length/min\\\\\\\\\\\\\\\": torch.min(response_length).detach().item(),\\\\\\\\n-        \\\\\\\\\\\\\\\"response_length/clip_ratio\\\\\\\\\\\\\\\": torch.mean(torch.eq(response_length, max_response_length).float())\\\\\\\\n-        .detach()\\\\\\\\n-        .item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"response_length/clip_ratio\\\\\\\\\\\\\\\": torch.eq(response_length, max_response_length).float().mean().detach().item(),\\\\\\\\n         # prompt length\\\\\\\\n         \\\\\\\\\\\\\\\"prompt_length/mean\\\\\\\\\\\\\\\": torch.mean(prompt_length).detach().item(),\\\\\\\\n         \\\\\\\\\\\\\\\"prompt_length/max\\\\\\\\\\\\\\\": torch.max(prompt_length).detach().item(),\\\\\\\\n         \\\\\\\\\\\\\\\"prompt_length/min\\\\\\\\\\\\\\\": torch.min(prompt_length).detach().item(),\\\\\\\\n-        \\\\\\\\\\\\\\\"prompt_length/clip_ratio\\\\\\\\\\\\\\\": torch.mean(torch.eq(prompt_length, max_prompt_length).float()).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"prompt_length/clip_ratio\\\\\\\\\\\\\\\": torch.eq(prompt_length, max_prompt_length).float().mean().detach().item(),\\\\\\\\n     }\\\\\\\\n \\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"519a22ded14828dc4d2a278119dc2f8d499b9c39\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/ray_trainer.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":15,\\\\\\\"deletions\\\\\\\":8,\\\\\\\"changes\\\\\\\":23,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fray_trainer.py?ref=87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -37,7 +37,7 @@\\\\\\\\n from ..single_controller.ray import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup\\\\\\\\n from ..single_controller.ray.base import create_colocated_worker_cls\\\\\\\\n from ..utils import torch_functional as VF\\\\\\\\n-from ..utils.checkpoint import CHECKPOINT_TRACKER, remove_obsolete_ckpt\\\\\\\\n+from ..utils.checkpoint import CHECKPOINT_TRACKER, find_latest_ckpt, remove_obsolete_ckpt\\\\\\\\n from ..utils.logger import Tracker\\\\\\\\n from ..utils.py_functional import convert_dict_to_str, timer\\\\\\\\n from ..utils.seqlen_balancing import get_seqlen_balanced_partitions, log_seqlen_unbalance\\\\\\\\n@@ -342,21 +342,28 @@ def _save_checkpoint(self) -> None:\\\\\\\\n             json.dump(checkpointer_tracker_info, f, ensure_ascii=False, indent=2)\\\\\\\\n \\\\\\\\n     def _load_checkpoint(self) -> None:\\\\\\\\n-        if self.config.trainer.load_checkpoint_path is None:\\\\\\\\n+        if self.config.trainer.load_checkpoint_path is not None:\\\\\\\\n+            load_checkpoint_path = self.config.trainer.load_checkpoint_path\\\\\\\\n+        elif self.config.trainer.find_last_checkpoint:\\\\\\\\n+            load_checkpoint_path = find_latest_ckpt(self.config.trainer.save_checkpoint_path)\\\\\\\\n+        else:\\\\\\\\n+            load_checkpoint_path = None\\\\\\\\n+\\\\\\\\n+        if load_checkpoint_path is None:\\\\\\\\n             return\\\\\\\\n \\\\\\\\n-        if \\\\\\\\\\\\\\\"global_step_\\\\\\\\\\\\\\\" not in self.config.trainer.load_checkpoint_path.strip(os.path.sep).split(os.path.sep)[-1]:\\\\\\\\n+        if \\\\\\\\\\\\\\\"global_step_\\\\\\\\\\\\\\\" not in load_checkpoint_path.strip(os.path.sep).split(os.path.sep)[-1]:\\\\\\\\n             raise ValueError(\\\\\\\\\\\\\\\"`load_checkpoint_path` should end with `global_step_*`.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n-        print(f\\\\\\\\\\\\\\\"Load from checkpoint: {self.config.trainer.load_checkpoint_path}.\\\\\\\\\\\\\\\")\\\\\\\\n-        self.global_step = int(self.config.trainer.load_checkpoint_path.strip(os.path.sep).split(\\\\\\\\\\\\\\\"global_step_\\\\\\\\\\\\\\\")[-1])\\\\\\\\n-        actor_path = os.path.join(self.config.trainer.load_checkpoint_path, \\\\\\\\\\\\\\\"actor\\\\\\\\\\\\\\\")\\\\\\\\n+        print(f\\\\\\\\\\\\\\\"Load from checkpoint: {load_checkpoint_path}.\\\\\\\\\\\\\\\")\\\\\\\\n+        self.global_step = int(load_checkpoint_path.strip(os.path.sep).split(\\\\\\\\\\\\\\\"global_step_\\\\\\\\\\\\\\\")[-1])\\\\\\\\n+        actor_path = os.path.join(load_checkpoint_path, \\\\\\\\\\\\\\\"actor\\\\\\\\\\\\\\\")\\\\\\\\n         self.actor_rollout_ref_wg.load_checkpoint(actor_path)\\\\\\\\n         if self.use_critic:\\\\\\\\n-            critic_path = os.path.join(self.config.trainer.load_checkpoint_path, \\\\\\\\\\\\\\\"critic\\\\\\\\\\\\\\\")\\\\\\\\n+            critic_path = os.path.join(load_checkpoint_path, \\\\\\\\\\\\\\\"critic\\\\\\\\\\\\\\\")\\\\\\\\n             self.critic_wg.load_checkpoint(critic_path)\\\\\\\\n \\\\\\\\n-        dataloader_path = os.path.join(self.config.trainer.load_checkpoint_path, \\\\\\\\\\\\\\\"dataloader.pt\\\\\\\\\\\\\\\")\\\\\\\\n+        dataloader_path = os.path.join(load_checkpoint_path, \\\\\\\\\\\\\\\"dataloader.pt\\\\\\\\\\\\\\\")\\\\\\\\n         if os.path.exists(dataloader_path):\\\\\\\\n             dataloader_state_dict = torch.load(dataloader_path, weights_only=False)\\\\\\\\n             self.train_dataloader.load_state_dict(dataloader_state_dict)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"ee26fbbcbfcc73fc78dad6d4cc647b6edf253714\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/checkpoint/__init__.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/verl%2Futils%2Fcheckpoint%2F__init__.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/verl%2Futils%2Fcheckpoint%2F__init__.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fcheckpoint%2F__init__.py?ref=87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -12,7 +12,7 @@\\\\\\\\n # See the License for the specific language governing permissions and\\\\\\\\n # limitations under the License.\\\\\\\\n \\\\\\\\n-from .checkpoint_manager import CHECKPOINT_TRACKER, remove_obsolete_ckpt\\\\\\\\n+from .checkpoint_manager import CHECKPOINT_TRACKER, find_latest_ckpt, remove_obsolete_ckpt\\\\\\\\n \\\\\\\\n \\\\\\\\n-__all__ = [\\\\\\\\\\\\\\\"CHECKPOINT_TRACKER\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"remove_obsolete_ckpt\\\\\\\\\\\\\\\"]\\\\\\\\n+__all__ = [\\\\\\\\\\\\\\\"CHECKPOINT_TRACKER\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"find_latest_ckpt\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"remove_obsolete_ckpt\\\\\\\\\\\\\\\"]\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"a40465967ad82ff3bd5c2744a7b8d20f20d6dc68\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/checkpoint/checkpoint_manager.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":16,\\\\\\\"deletions\\\\\\\":16,\\\\\\\"changes\\\\\\\":32,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/verl%2Futils%2Fcheckpoint%2Fcheckpoint_manager.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/verl%2Futils%2Fcheckpoint%2Fcheckpoint_manager.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fcheckpoint%2Fcheckpoint_manager.py?ref=87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -12,6 +12,7 @@\\\\\\\\n # See the License for the specific language governing permissions and\\\\\\\\n # limitations under the License.\\\\\\\\n \\\\\\\\n+import json\\\\\\\\n import os\\\\\\\\n import random\\\\\\\\n import re\\\\\\\\n@@ -107,39 +108,38 @@ def load_rng_state(rng_state: Dict[str, Any]):\\\\\\\\n         random.setstate(rng_state[\\\\\\\\\\\\\\\"random\\\\\\\\\\\\\\\"])\\\\\\\\n \\\\\\\\n \\\\\\\\n-def find_latest_ckpt_path(path: Optional[str] = None, directory_format: str = \\\\\\\\\\\\\\\"global_step_{}\\\\\\\\\\\\\\\") -> Optional[str]:\\\\\\\\n-    if path is None:\\\\\\\\n-        return None\\\\\\\\n+def get_checkpoint_tracker_filename(root_path: str) -> str:\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    Tracker file rescords the latest chckpoint during training to restart from.\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    return os.path.join(root_path, CHECKPOINT_TRACKER)\\\\\\\\n \\\\\\\\n+\\\\\\\\n+def find_latest_ckpt(path: str, directory_format: str = \\\\\\\\\\\\\\\"global_step_{}\\\\\\\\\\\\\\\") -> Optional[str]:\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    Find the latest checkpoint in the save path.\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     tracker_file = get_checkpoint_tracker_filename(path)\\\\\\\\n     if not os.path.exists(tracker_file):\\\\\\\\n-        print(\\\\\\\\\\\\\\\"Checkpoint tracker file does not exist: %s\\\\\\\\\\\\\\\", tracker_file)\\\\\\\\n         return None\\\\\\\\n \\\\\\\\n     with open(tracker_file, \\\\\\\\\\\\\\\"rb\\\\\\\\\\\\\\\") as f:\\\\\\\\n-        iteration = int(f.read().decode())\\\\\\\\n+        checkpointer_tracker_info = json.load(f)\\\\\\\\n \\\\\\\\n-    ckpt_path = os.path.join(path, directory_format.format(iteration))\\\\\\\\n+    ckpt_path = os.path.join(path, directory_format.format(checkpointer_tracker_info[\\\\\\\\\\\\\\\"last_global_step\\\\\\\\\\\\\\\"]))\\\\\\\\n     if not os.path.exists(ckpt_path):\\\\\\\\n-        print(\\\\\\\\\\\\\\\"Checkpoint does not exist: %s\\\\\\\\\\\\\\\", ckpt_path)\\\\\\\\n+        print(f\\\\\\\\\\\\\\\"Checkpoint does not exist: {ckpt_path}\\\\\\\\\\\\\\\")\\\\\\\\n         return None\\\\\\\\n \\\\\\\\n-    print(\\\\\\\\\\\\\\\"Found checkpoint: %s\\\\\\\\\\\\\\\", ckpt_path)\\\\\\\\n+    print(f\\\\\\\\\\\\\\\"Found latest checkpoint: {ckpt_path}, will resume from it. Turn off `find_last_checkpoint` to disable it.\\\\\\\\\\\\\\\")\\\\\\\\n     return ckpt_path\\\\\\\\n \\\\\\\\n \\\\\\\\n-def get_checkpoint_tracker_filename(root_path: str) -> str:\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    Tracker file rescords the latest chckpoint during training to restart from.\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    return os.path.join(root_path, CHECKPOINT_TRACKER)\\\\\\\\n-\\\\\\\\n-\\\\\\\\n def remove_obsolete_ckpt(\\\\\\\\n     path: str, global_step: int, best_global_step: int, save_limit: int = -1, directory_format: str = \\\\\\\\\\\\\\\"global_step_{}\\\\\\\\\\\\\\\"\\\\\\\\n ):\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    Remove the obsolete checkpoints that exceed the save_limit.\\\\\\\\n+    Remove the obsolete checkpoints that exceed the save limit.\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     if save_limit <= 0 or not os.path.exists(path):\\\\\\\\n         return\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"562e2ae4c82343e89121324daae89276dc00257d\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/reward/function.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":11,\\\\\\\"deletions\\\\\\\":8,\\\\\\\"changes\\\\\\\":19,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/verl%2Fworkers%2Freward%2Ffunction.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50/verl%2Fworkers%2Freward%2Ffunction.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Freward%2Ffunction.py?ref=87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -84,20 +84,21 @@ def compute_reward(self, data: DataProto) -> Tuple[torch.Tensor, Dict[str, List[\\\\\\\\n         reward_tensor = torch.zeros_like(data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"], dtype=torch.float32)\\\\\\\\n         reward_metrics = defaultdict(list)\\\\\\\\n         response_ids = data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-        response_length = data.batch[\\\\\\\\\\\\\\\"response_mask\\\\\\\\\\\\\\\"].sum(dim=-1)\\\\\\\\n+        response_length = torch.sum(data.batch[\\\\\\\\\\\\\\\"response_mask\\\\\\\\\\\\\\\"], dim=-1)\\\\\\\\n         for i in range(len(data)):\\\\\\\\n-            valid_response_ids = response_ids[i][: response_length[i]]\\\\\\\\n+            cur_response_length = int(response_length[i].item())  # avoid tensor indexing error\\\\\\\\n+            valid_response_ids = response_ids[i][:cur_response_length]\\\\\\\\n             response_str = self.tokenizer.decode(\\\\\\\\n                 valid_response_ids, skip_special_tokens=self.config.skip_special_tokens\\\\\\\\n             )\\\\\\\\n             score = self.reward_fn(\\\\\\\\n                 {\\\\\\\\n                     \\\\\\\\\\\\\\\"response\\\\\\\\\\\\\\\": response_str,\\\\\\\\n-                    \\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\": response_length[i],\\\\\\\\n+                    \\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\": cur_response_length,\\\\\\\\n                     \\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\": data.non_tensor_batch[\\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\"][i],\\\\\\\\n                 }\\\\\\\\n             )\\\\\\\\n-            reward_tensor[i, response_length[i] - 1] = score[\\\\\\\\\\\\\\\"overall\\\\\\\\\\\\\\\"]\\\\\\\\n+            reward_tensor[i, cur_response_length - 1] = score[\\\\\\\\\\\\\\\"overall\\\\\\\\\\\\\\\"]\\\\\\\\n             for key, value in score.items():\\\\\\\\n                 reward_metrics[key].append(value)\\\\\\\\n \\\\\\\\n@@ -110,16 +111,17 @@ class BatchFunctionRewardManager(FunctionRewardManager):\\\\\\\\n     def compute_reward(self, data: DataProto) -> Tuple[torch.Tensor, Dict[str, List[float]]]:\\\\\\\\n         reward_inputs = []\\\\\\\\n         response_ids = data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-        response_length = data.batch[\\\\\\\\\\\\\\\"response_mask\\\\\\\\\\\\\\\"].sum(dim=-1)\\\\\\\\n+        response_length = torch.sum(data.batch[\\\\\\\\\\\\\\\"response_mask\\\\\\\\\\\\\\\"], dim=-1)\\\\\\\\n         for i in range(len(data)):\\\\\\\\n-            valid_response_ids = response_ids[i][: response_length[i]]\\\\\\\\n+            cur_response_length = int(response_length[i].item())  # avoid tensor indexing error\\\\\\\\n+            valid_response_ids = response_ids[i][:cur_response_length]\\\\\\\\n             response_str = self.tokenizer.decode(\\\\\\\\n                 valid_response_ids, skip_special_tokens=self.config.skip_special_tokens\\\\\\\\n             )\\\\\\\\n             reward_inputs.append(\\\\\\\\n                 {\\\\\\\\n                     \\\\\\\\\\\\\\\"response\\\\\\\\\\\\\\\": response_str,\\\\\\\\n-                    \\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\": response_length[i],\\\\\\\\n+                    \\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\": cur_response_length,\\\\\\\\n                     \\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\": data.non_tensor_batch[\\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\"][i],\\\\\\\\n                 }\\\\\\\\n             )\\\\\\\\n@@ -128,7 +130,8 @@ def compute_reward(self, data: DataProto) -> Tuple[torch.Tensor, Dict[str, List[\\\\\\\\n         reward_tensor = torch.zeros_like(data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"], dtype=torch.float32)\\\\\\\\n         reward_metrics = defaultdict(list)\\\\\\\\n         for i, score in enumerate(scores):\\\\\\\\n-            reward_tensor[i, response_length[i] - 1] = score[\\\\\\\\\\\\\\\"overall\\\\\\\\\\\\\\\"]\\\\\\\\n+            cur_response_length = int(response_length[i].item())  # avoid tensor indexing error\\\\\\\\n+            reward_tensor[i, cur_response_length - 1] = score[\\\\\\\\\\\\\\\"overall\\\\\\\\\\\\\\\"]\\\\\\\\n             for key, value in score.items():\\\\\\\\n                 reward_metrics[key].append(value)\\\\\\\\n \\\\\\\"}],\\\\\\\"sha\\\\\\\":\\\\\\\"87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support auto resume (#425)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-15T05:48:35Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "This commit is also not about the micro batch size parameters. Let me check the most recent commit:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"ref\\\": \\\"098931530606d22f867fd121b1dcb3225a43661f\\\"}\", \"include_output_fields\": [\"sha\", \"commit.author.date\", \"commit.author.name\", \"commit.message\", \"author.login\", \"files\"]}",
    "call_id": "toolu_012E63LFaZnZ1oi4TPQ1ALHQ",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_012E63LFaZnZ1oi4TPQ1ALHQ",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"files\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"f525358d5b247326323cb63ed2c536e5414fe6c4\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/config.yaml\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/098931530606d22f867fd121b1dcb3225a43661f/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/098931530606d22f867fd121b1dcb3225a43661f/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fconfig.yaml?ref=098931530606d22f867fd121b1dcb3225a43661f\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -34,8 +34,8 @@ algorithm:\\\\\\\\n worker:\\\\\\\\n   actor:\\\\\\\\n     global_batch_size: 128  # equivalent to verl's actor.ppo_mini_batch_size\\\\\\\\n-    micro_batch_size_per_device_for_update: 4  # equivalent to verl's actor.ppo_micro_batch_size_per_gpu\\\\\\\\n-    micro_batch_size_per_device_for_experience: 16  # equivalent to verl's rollout.log_prob_micro_batch_size_per_gpu\\\\\\\\n+    micro_batch_size_per_device_for_update: 1  # equivalent to verl's actor.ppo_micro_batch_size_per_gpu\\\\\\\\n+    micro_batch_size_per_device_for_experience: 2  # equivalent to verl's rollout.log_prob_micro_batch_size_per_gpu\\\\\\\\n     max_grad_norm: 1.0\\\\\\\\n     padding_free: true\\\\\\\\n     dynamic_batching: true\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"33c1520ceacff5ef32e9445a72a04d28b0041d91\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":0,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/098931530606d22f867fd121b1dcb3225a43661f/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/098931530606d22f867fd121b1dcb3225a43661f/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh?ref=098931530606d22f867fd121b1dcb3225a43661f\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -11,8 +11,6 @@ python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     data.train_files=hiyouga/geometry3k@train \\\\\\\\\\\\\\\\\\\\\\\\n     data.val_files=hiyouga/geometry3k@test \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.actor.micro_batch_size_per_device_for_update=1 \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.actor.micro_batch_size_per_device_for_experience=8 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.fsdp.torch_dtype=bf16 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.optim.strategy=adamw_bf16 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.tensor_parallel_size=8 \\\\\\\\\\\\\\\\\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"a8c6f8c9d28e9ffccba708d01fb43518fa6861e0\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/protocol.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":6,\\\\\\\"deletions\\\\\\\":4,\\\\\\\"changes\\\\\\\":10,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/098931530606d22f867fd121b1dcb3225a43661f/verl%2Fprotocol.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/098931530606d22f867fd121b1dcb3225a43661f/verl%2Fprotocol.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fprotocol.py?ref=098931530606d22f867fd121b1dcb3225a43661f\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -204,12 +204,14 @@ def __getitem__(\\\\\\\\n         raise TypeError(f\\\\\\\\\\\\\\\"Indexing with {type(item)} is not supported.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n     def __getstate__(self) -> tuple[bytes, dict[str, NDArray], dict[str, Any]]:\\\\\\\\n-        buffer = io.BytesIO()\\\\\\\\n         if self.batch is not None:\\\\\\\\n-            self.batch: TensorDict = self.batch.contiguous()\\\\\\\\n-            self.batch: TensorDict = self.batch.consolidate()\\\\\\\\n+            batch_to_save: TensorDict = self.batch.contiguous()\\\\\\\\n+            batch_to_save: TensorDict = batch_to_save.consolidate()\\\\\\\\n+        else:\\\\\\\\n+            batch_to_save = None\\\\\\\\n \\\\\\\\n-        torch.save(self.batch, buffer)\\\\\\\\n+        buffer = io.BytesIO()\\\\\\\\n+        torch.save(batch_to_save, buffer)\\\\\\\\n         buffer_bytes = buffer.getvalue()\\\\\\\\n         return buffer_bytes, self.non_tensor_batch, self.meta_info\\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"f1744a969c20f7755c0b39ff12b64d83c7992ce3\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/ray_trainer.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/098931530606d22f867fd121b1dcb3225a43661f/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/098931530606d22f867fd121b1dcb3225a43661f/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fray_trainer.py?ref=098931530606d22f867fd121b1dcb3225a43661f\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -39,7 +39,7 @@\\\\\\\\n from ..utils import torch_functional as VF\\\\\\\\n from ..utils.checkpoint import CHECKPOINT_TRACKER, find_latest_ckpt, remove_obsolete_ckpt\\\\\\\\n from ..utils.logger import Tracker\\\\\\\\n-from ..utils.py_functional import convert_dict_to_str, timer\\\\\\\\n+from ..utils.py_functional import convert_dict_to_str, timer, unflatten_dict\\\\\\\\n from ..utils.seqlen_balancing import get_seqlen_balanced_partitions, log_seqlen_unbalance\\\\\\\\n from ..workers.fsdp_workers import FSDPWorker\\\\\\\\n from ..workers.reward import FunctionRewardManager\\\\\\\\n@@ -694,7 +694,7 @@ def fit(self):\\\\\\\\n                 val_metrics = self._validate()\\\\\\\\n                 self.logger.log(data=val_metrics, step=self.global_step)\\\\\\\\n \\\\\\\\n-            print(f\\\\\\\\\\\\\\\"Final validation metrics: {convert_dict_to_str(val_metrics)}\\\\\\\\\\\\\\\")\\\\\\\\n+            print(f\\\\\\\\\\\\\\\"Final validation metrics:\\\\\\\\\\\\\\\\n{convert_dict_to_str(unflatten_dict(val_metrics))}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         if self.config.trainer.save_freq <= 0 or self.global_step % self.config.trainer.save_freq != 0:\\\\\\\\n             self._save_checkpoint()\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"9e33bf455a4b456e75d72fd2cec1695171627f1a\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/fsdp_workers.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/098931530606d22f867fd121b1dcb3225a43661f/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/098931530606d22f867fd121b1dcb3225a43661f/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Ffsdp_workers.py?ref=098931530606d22f867fd121b1dcb3225a43661f\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -456,7 +456,7 @@ def _process_multi_modal_inputs(self, data: DataProto):\\\\\\\\n             multi_modal_inputs_cache = {}  # avoid repeated processing for n > 1 samples\\\\\\\\n             for index, multi_modal_data in zip(\\\\\\\\n                 data.non_tensor_batch[\\\\\\\\\\\\\\\"uid\\\\\\\\\\\\\\\"], data.non_tensor_batch[\\\\\\\\\\\\\\\"multi_modal_data\\\\\\\\\\\\\\\"]\\\\\\\\n-            ): # process multi modal data per sample\\\\\\\\n+            ):  # process multi modal data per sample\\\\\\\\n                 if index not in multi_modal_inputs_cache:\\\\\\\\n                     images, videos = [], []\\\\\\\\n                     if \\\\\\\\\\\\\\\"images\\\\\\\\\\\\\\\" in multi_modal_data:\\\\\\\"}],\\\\\\\"sha\\\\\\\":\\\\\\\"098931530606d22f867fd121b1dcb3225a43661f\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[misc] fix data proto (#458)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-08-08T12:05:58Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Perfect! I found the target commit! This is the commit with SHA `098931530606d22f867fd121b1dcb3225a43661f` where:\n- `micro_batch_size_per_device_for_update` was changed from `4` to `1` (line 37)\n- `micro_batch_size_per_device_for_experience` was changed from `16` to `2` (line 38)\n\nThe commit details are:\n- SHA: `098931530606d22f867fd121b1dcb3225a43661f`\n- Author: `hiyouga` (Yaowei Zheng)  \n- Date: `2025-08-08T12:05:58Z` (2025-08-08)\n- Message: `[misc] fix data proto (#458)`\n\nNow let me search for issues related to OOM, memory, batch, and GPU memory:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_ISSUES\", \"action_name\": \"list_for_repo\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\"}\", \"query_params\": \"{\\\"state\\\": \\\"all\\\", \\\"per_page\\\": 100}\", \"include_output_fields\": [\"number\", \"title\", \"body\", \"state\"]}",
    "call_id": "toolu_01GfKYhwLhVr5PLAnz6qstBn",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01GfKYhwLhVr5PLAnz6qstBn",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"list_for_repo\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":[{\\\\\\\"number\\\\\\\":50,\\\\\\\"title\\\\\\\":\\\\\\\"log_prob \\\\\\\\u7684\\\\\\\\u8ba1\\\\\\\\u7b97\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5728 https://github.com/hiyouga/EasyR1/blob/51001bd457f4fbab0055b6a07341c4ac468b61e6/verl/utils/torch_functional.py#L67 \\\\\\\\u4e2d\\\\\\\\uff0c\\\\\\\\u8ba1\\\\\\\\u7b97 log_prob \\\\\\\\u7684\\\\\\\\u51fd\\\\\\\\u6570\\\\\\\\u4e3a\\\\\\\\uff1a\\\\\\\\n```\\\\\\\\ndef log_probs_from_logits(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\\\\\\\\n    ...\\\\\\\\n    if FLAH_ATTN_CROSS_ENTROPY_LOSS_AVAILABLE:\\\\\\\\n        output = log_probs_from_logits_flash_attn(logits, labels)\\\\\\\\n    else:  # fall back to torch kernel, upcast logits to fp32\\\\\\\\n        output = F.cross_entropy(logits.float(), labels, reduction=\\\\\\\\\\\\\\\"none\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    return output.view(*batch_dim)\\\\\\\\n```\\\\\\\\n\\\\\\\\n\\\\\\\\u5f53 if \\\\\\\\u89e6\\\\\\\\u53d1\\\\\\\\u7684\\\\\\\\u65f6\\\\\\\\u5019\\\\\\\\uff0c\\\\\\\\u8f93\\\\\\\\u51fa\\\\\\\\uff1a\\\\\\\\n```\\\\\\\\n@torch.compiler.disable()\\\\\\\\ndef log_probs_from_logits_flash_attn(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\\\\\\\\n    output = cross_entropy_loss(logits, labels, inplace_backward=True)\\\\\\\\n    if not isinstance(output, tuple):\\\\\\\\n        raise ValueError(\\\\\\\\n            \\\\\\\\\\\\\\\"please make sure flash-attn>=2.4.3 where cross_entropy_loss returns Tuple[losses, z_losses].\\\\\\\\\\\\\\\"\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    return -output[0]\\\\\\\\n```\\\\\\\\n\\\\\\\\u5f53else\\\\\\\\u89e6\\\\\\\\u53d1\\\\\\\\u5219\\\\\\\\u8fd4\\\\\\\\u56de\\\\\\\\uff1a\\\\\\\\n```\\\\\\\\noutput = F.cross_entropy(logits.float(), labels, reduction=\\\\\\\\\\\\\\\"none\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    return output.view(*batch_dim)\\\\\\\\n```\\\\\\\\n\\\\\\\\u8fd9\\\\\\\\u91cc\\\\\\\\u7684\\\\\\\\u6b63\\\\\\\\u8d1f\\\\\\\\u53f7\\\\\\\\u597d\\\\\\\\u50cf\\\\\\\\u6709\\\\\\\\u4e00\\\\\\\\u4e9b\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\\uff1fcross_entropy\\\\\\\\u4e00\\\\\\\\u822c\\\\\\\\u5f97\\\\\\\\u5230\\\\\\\\u7684\\\\\\\\u662f loss\\\\\\\\uff0c\\\\\\\\u5e94\\\\\\\\u8be5\\\\\\\\u662f\\\\\\\\u8fd4\\\\\\\\u56de\\\\\\\\u975e\\\\\\\\u8d1f\\\\\\\\u6570\\\\\\\\u7684\\\\\\\\uff1f\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":49,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u8bad\\\\\\\\u7ec3Qwen3-8B\\\\\\\\u65f6\\\\\\\\uff0c\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u603b\\\\\\\\u662f\\\\\\\\u8fc7\\\\\\\\u65e9\\\\\\\\u505c\\\\\\\\u6b62\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5728\\\\\\\\u8bad\\\\\\\\u7ec3Qwen3-8B\\\\\\\\u65f6\\\\\\\\uff0c\\\\\\\\u9884\\\\\\\\u8ba1\\\\\\\\u8bad\\\\\\\\u7ec3step\\\\\\\\u67091510\\\\\\\\u4e2a\\\\\\\\uff0c\\\\\\\\u5982\\\\\\\\u4e0b\\\\\\\\u56fe\\\\\\\\u6240\\\\\\\\u793a\\\\\\\\n\\\\\\\\n![Image](https://github.com/user-attachments/assets/7c866b9d-d20e-423d-81a5-013425325ab4)\\\\\\\\n\\\\\\\\n\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u5b9e\\\\\\\\u9645\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u572820\\\\\\\\u591a\\\\\\\\u4e2astep\\\\\\\\u5c31\\\\\\\\u505c\\\\\\\\u6b62\\\\\\\\u4e86\\\\\\\\uff0c\\\\\\\\u65e5\\\\\\\\u5fd7\\\\\\\\u505c\\\\\\\\u5728\\\\\\\\u8fd9\\\\\\\\u91cc\\\\\\\\n\\\\\\\\n![Image](https://github.com/user-attachments/assets/d8fd060c-f882-4114-aef4-0524ffd32a4c)\\\\\\\\n\\\\\\\\nwandb\\\\\\\\u4e0a\\\\\\\\u7684\\\\\\\\u56fe\\\\\\\\u50cf\\\\\\\\u4e5f\\\\\\\\u662f\\\\\\\\u505c\\\\\\\\u7559\\\\\\\\u572820\\\\\\\\u591a\\\\\\\\u4e2astep\\\\\\\\n\\\\\\\\n![Image](https://github.com/user-attachments/assets/d7454b0a-c9a3-46e5-a591-0d2b29569563)\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":48,\\\\\\\"title\\\\\\\":\\\\\\\"Exception ignored in sys.unraisablehook\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I try the demo by running 'bash examples/qwen2_5_vl_7b_geo3k_grpo.sh'.\\\\\\\\nAn exception happens when the training process is finished.\\\\\\\\nI wonder is there anything wrong and how can I correct it?\\\\\\\\nThe following figure is the exception:\\\\\\\\n\\\\\\\\n<img width=\\\\\\\\\\\\\\\"849\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/b64abc66-bdce-47ed-9bd1-bb7249eab076\\\\\\\\\\\\\\\" />\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":47,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u600e\\\\\\\\u4e48\\\\\\\\u52a0\\\\\\\\u901fFiltering overlong prompts\\\\\\\\uff1f\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"![Image](https://github.com/user-attachments/assets/d39062f0-d8db-4468-93c6-96bc7b84b90b)\\\\\\\\n\\\\\\\\u5982\\\\\\\\u56fe\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u8fdb\\\\\\\\u884cFiltering overlong prompts\\\\\\\\u8981\\\\\\\\u8017\\\\\\\\u8d391\\\\\\\\u4e2a\\\\\\\\u5c0f\\\\\\\\u65f6\\\\\\\\u4ee5\\\\\\\\u4e0a\\\\\\\\uff0c\\\\\\\\u800c\\\\\\\\u4e14easyr1\\\\\\\\u597d\\\\\\\\u50cf\\\\\\\\u6ca1\\\\\\\\u6709filter_overlong_prompts_workers\\\\\\\\u7684\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\\uff0c\\\\\\\\u90a3\\\\\\\\u6211\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u52a0\\\\\\\\u901f\\\\\\\\u8fd9\\\\\\\\u4e00\\\\\\\\u8fdb\\\\\\\\u7a0b\\\\\\\\u5462\\\\\\\\uff1f\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":46,\\\\\\\"title\\\\\\\":\\\\\\\"[\\\\\\\\u529f\\\\\\\\u80fd\\\\\\\\u8bf7\\\\\\\\u6c42]\\\\\\\\u4e3a\\\\\\\\u5b9e\\\\\\\\u9a8c\\\\\\\\u4e00\\\\\\\\u5f00\\\\\\\\u59cb\\\\\\\\u7684\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u9636\\\\\\\\u6bb5\\\\\\\\u589e\\\\\\\\u52a0\\\\\\\\u63d0\\\\\\\\u793a\\\\\\\\uff08\\\\\\\\u539f\\\\\\\\u6807\\\\\\\\u9898\\\\\\\\uff1a\\\\\\\\u65b0\\\\\\\\u7248\\\\\\\\u672c\\\\\\\\u7684easyr1\\\\\\\\u662f\\\\\\\\u53d6\\\\\\\\u6d88\\\\\\\\u4e86val\\\\\\\\u9636\\\\\\\\u6bb5\\\\\\\\u7684\\\\\\\\u8fdb\\\\\\\\u5ea6\\\\\\\\u5c55\\\\\\\\u793a\\\\\\\\u5417\\\\\\\\uff1f)\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"![Image](https://github.com/user-attachments/assets/8d7c2de2-9b23-45dd-a207-6bfa351ab40e)\\\\\\\\n\\\\\\\\u663e\\\\\\\\u793aStart validation...\\\\\\\\u540e\\\\\\\\u7ec8\\\\\\\\u7aef\\\\\\\\u4e00\\\\\\\\u76f4\\\\\\\\u6ca1\\\\\\\\u6709\\\\\\\\u8f93\\\\\\\\u51fa\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u5361\\\\\\\\u662f\\\\\\\\u4e00\\\\\\\\u76f4\\\\\\\\u518d\\\\\\\\u8dd1\\\\\\\\u7684\\\\\\\\uff0c\\\\\\\\u76f4\\\\\\\\u5230\\\\\\\\u7206\\\\\\\\u51fa\\\\\\\\u4e86\\\\\\\\u663e\\\\\\\\u5b58\\\\\\\\u9519\\\\\\\\u8bef\\\\\\\\u3002\\\\\\\\n\\\\\\\\u8bf7\\\\\\\\u95ee\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u9636\\\\\\\\u6bb5\\\\\\\\u662f\\\\\\\\u5728\\\\\\\\u4e00\\\\\\\\u76f4\\\\\\\\u8fdb\\\\\\\\u884c\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u5417\\\\\\\\uff1f\\\\\\\\u6211\\\\\\\\u8bb0\\\\\\\\u5f97\\\\\\\\u4e4b\\\\\\\\u524d\\\\\\\\u7684\\\\\\\\u7248\\\\\\\\u672c\\\\\\\\u662f\\\\\\\\u4f1a\\\\\\\\u663e\\\\\\\\u793a\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u8fdb\\\\\\\\u5ea6\\\\\\\\u7684\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u4eca\\\\\\\\u5929\\\\\\\\u62c9\\\\\\\\u4e86\\\\\\\\u6700\\\\\\\\u65b0\\\\\\\\u5206\\\\\\\\u652f\\\\\\\\u540e\\\\\\\\u51fa\\\\\\\\u73b0\\\\\\\\u4e86\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":45,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5fae\\\\\\\\u4fe1\\\\\\\\u7fa4\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u66f4\\\\\\\\u65b0\\\\\\\\u4e00\\\\\\\\u4e0b\\\\\\\\u5fae\\\\\\\\u4fe1\\\\\\\\u7fa4\\\\\\\\u7684\\\\\\\\u540d\\\\\\\\u7247\\\\\\\\uff0c\\\\\\\\u539f\\\\\\\\u6765\\\\\\\\u7684\\\\\\\\u8fc7\\\\\\\\u671f\\\\\\\\u4e86\\\\\\\\u8c22\\\\\\\\u8c22\\\\\\\\uff01\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":44,\\\\\\\"title\\\\\\\":\\\\\\\"Issue : Consider Adding Type Hints for Function Arguments in `protocol.py`\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Adding type hints to the function arguments in `protocol.py` can improve code readability and maintainability. While some arguments already have type hints, extending this to all arguments would provide a more consistent and comprehensive benefit. For example, add type hints to arguments such as `features` in `batch_collate` function or `data_items` in `collate_fn` function to improve type safety and make the function signature more explicit.\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":43,\\\\\\\"title\\\\\\\":\\\\\\\"How to disable flash-attn?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I enocutered a problem:\\\\\\\\n\\\\\\\\nImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2\\\\\\\\n\\\\\\\\nI was able to run it successfully without this package before, but now I don't know why it doesn't work anymore. Can some body give me some solution?\\\\\\\\n\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":42,\\\\\\\"title\\\\\\\":\\\\\\\"DAPO\\\\\\\\u7f13\\\\\\\\u6162\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u60a8\\\\\\\\u597d\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u60f3\\\\\\\\u8bf7\\\\\\\\u6559\\\\\\\\u4e00\\\\\\\\u4e0b\\\\\\\\uff0c\\\\\\\\u4f7f\\\\\\\\u7528\\\\\\\\u539fGRPO\\\\\\\\u65f6\\\\\\\\u4e00\\\\\\\\u4e2astep\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u5f88\\\\\\\\u5feb\\\\\\\\uff0c\\\\\\\\u6539\\\\\\\\u4e3aDAPO\\\\\\\\u540e\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u81f3\\\\\\\\u5c11\\\\\\\\u6162\\\\\\\\u4e8610\\\\\\\\u500d\\\\\\\\u4ee5\\\\\\\\u4e0a\\\\\\\\u3002\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":41,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5173\\\\\\\\u4e8ecuda 12.1\\\\\\\\u652f\\\\\\\\u6301\\\\\\\\u7684docker\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u60a8\\\\\\\\u597d\\\\\\\\uff01\\\\\\\\u76ee\\\\\\\\u524d\\\\\\\\u516c\\\\\\\\u5f00\\\\\\\\u7684docker\\\\\\\\u955c\\\\\\\\u50cf\\\\\\\\u4f3c\\\\\\\\u4e4e\\\\\\\\u5bf9\\\\\\\\u5e94\\\\\\\\u7684\\\\\\\\u662fcuda12.6\\\\\\\\uff0c\\\\\\\\u7136\\\\\\\\u800c\\\\\\\\u6211\\\\\\\\u7684\\\\\\\\u670d\\\\\\\\u52a1\\\\\\\\u5668\\\\\\\\u4e0a\\\\\\\\u53ea\\\\\\\\u652f\\\\\\\\u6301cuda 12.1\\\\\\\\uff0c\\\\\\\\u8bf7\\\\\\\\u95ee\\\\\\\\u6709\\\\\\\\u6ca1\\\\\\\\u6709\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u62c9\\\\\\\\u53d6\\\\\\\\u7684docker\\\\\\\\u955c\\\\\\\\u50cf\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u4f7f\\\\\\\\u7528\\\\\\\\u5462\\\\\\\\uff1f\\\\\\\\u5341\\\\\\\\u5206\\\\\\\\u611f\\\\\\\\u8c22\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":40,\\\\\\\"title\\\\\\\":\\\\\\\"Custom configuration for reward function\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi, is it possible to have some configurable hyperparameters for reward function, especially some custom configurations?\\\\\\\\n\\\\\\\\nIn the current codebase, it seems even format_weight cannot be configured\\\\\\\\n\\\\\\\\nIn my experiments I want to combine two different rewards, and I want to tune the combination weight between them, but I can't see how to configure that, especially if I don't want to modify RewardConfig in the source code (as they have fixed fields)\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":39,\\\\\\\"title\\\\\\\":\\\\\\\"GRPO\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u7528Qwen2.5-VL-7B\\\\\\\\u6a21\\\\\\\\u578b\\\\\\\\u8bad\\\\\\\\u7ec3GRPO\\\\\\\\uff0c\\\\\\\\u6bcf\\\\\\\\u6761\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u7684\\\\\\\\u8f93\\\\\\\\u5165\\\\\\\\u4e3a8\\\\\\\\u5f20336*336\\\\\\\\u7684\\\\\\\\u56fe\\\\\\\\u7247\\\\\\\\uff0c\\\\\\\\u52a0\\\\\\\\u4e00\\\\\\\\u6bb5\\\\\\\\u957f\\\\\\\\u5ea6\\\\\\\\u4e0d\\\\\\\\u8d85\\\\\\\\u8fc71000\\\\\\\\u7684\\\\\\\\u6587\\\\\\\\u672c\\\\\\\\u30028\\\\\\\\u5f20A100 80G\\\\\\\\uff0c800\\\\\\\\u6761\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u6570\\\\\\\\u636e10\\\\\\\\u4e2aepoch\\\\\\\\uff0c\\\\\\\\u663e\\\\\\\\u793a\\\\\\\\u9700\\\\\\\\u898126\\\\\\\\uff5e30\\\\\\\\u4e2a\\\\\\\\u5c0f\\\\\\\\u65f6\\\\\\\\uff0c\\\\\\\\u8bf7\\\\\\\\u95ee\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u6b63\\\\\\\\u5e38\\\\\\\\u5417\\\\\\\\uff1f\\\\\\\\u662f\\\\\\\\u5426\\\\\\\\u6709\\\\\\\\u63d0\\\\\\\\u5347\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u7684\\\\\\\\u65b9\\\\\\\\u6848\\\\\\\\uff1f\\\\\\\\uff08\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u8fc7\\\\\\\\u7a0b\\\\\\\\u4e2d\\\\\\\\u51fa\\\\\\\\u73b0GPU\\\\\\\\u5229\\\\\\\\u7528\\\\\\\\u7387\\\\\\\\u5f88\\\\\\\\u4f4e\\\\\\\\uff0820%\\\\\\\\u5de6\\\\\\\\u53f3\\\\\\\\uff09\\\\\\\\uff0cCPU\\\\\\\\u5229\\\\\\\\u7528\\\\\\\\u7387\\\\\\\\u5f88\\\\\\\\u9ad8\\\\\\\\u7684\\\\\\\\u60c5\\\\\\\\u51b5\\\\\\\\uff09\\\\\\\\u671f\\\\\\\\u5f85\\\\\\\\u60a8\\\\\\\\u7684\\\\\\\\u56de\\\\\\\\u7b54\\\\\\\\uff0c\\\\\\\\u975e\\\\\\\\u5e38\\\\\\\\u611f\\\\\\\\u8c22\\\\\\\\uff01\\\\\\\\n\\\\\\\\n\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\\u8bbe\\\\\\\\u7f6e\\\\\\\\u5982\\\\\\\\u4e0b\\\\\\\\uff1a\\\\\\\\npython3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n    config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n    data.train_files=${TRAIN_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    data.val_files=${VAL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    data.rollout_batch_size=16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.global_batch_size=16 \\\\\\\\\\\\\\\\\\\\\\\\n    data.val_batch_size=16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.limit_images=8 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.experiment_name=${EXP_NAME} \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.n_gpus_per_node=8 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.save_checkpoint_path=${CKPT_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.n=8 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.logger=['console','tensorboard'] \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.micro_batch_size_per_device_for_experience=16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.micro_batch_size_per_device_for_update=4 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.tensor_parallel_size=2 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.gpu_memory_utilization=0.6 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.reward.reward_type=sequential \\\\\\\\\\\\\\\\\\\\\\\\n    worker.reward.reward_function=./examples/reward_function/r1v.py:compute_score \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_prompt_length=5120 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.max_num_batched_tokens=7168 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.val_freq=50 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.save_freq=50 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.save_limit=15 \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_pixels=112896 \\\\\\\\\\\\\\\\\\\\\\\\n    data.min_pixels=112896 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.total_epochs=10 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.val_before_train=false \\\\\\\\\\\\\\\\\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":38,\\\\\\\"title\\\\\\\":\\\\\\\"Possible Typo in fsdp_workers.py:477 for Video Inputs\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi @hiyouga ,\\\\\\\\n\\\\\\\\nhttps://github.com/hiyouga/EasyR1/blob/c19396ea16edc383cb25fedb3da58ae08e08046e/verl/workers/fsdp_workers.py#L477\\\\\\\\n\\\\\\\\nShould this line be updated to:\\\\\\\\nself.processor.image_processor(images=None, videos=video**s**, return_tensors=\\\\\\\\\\\\\\\"pt\\\\\\\\\\\\\\\")?\\\\\\\\n\\\\\\\\nThanks!\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":37,\\\\\\\"title\\\\\\\":\\\\\\\"When training with video (T, C, H, W), ValueError: expected to be in states [<TrainingState.FORWARD_BACKWARD: 2>] but current state is TrainingState.IDLE\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"```\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"<frozen runpy>\\\\\\\\\\\\\\\", line 198, in _run_module_as_main\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"<frozen runpy>\\\\\\\\\\\\\\\", line 88, in _run_code\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/trainer/main.py\\\\\\\\\\\\\\\", line 128, in <module>\\\\\\\\n2025-06-26 03:30:57     main()\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/trainer/main.py\\\\\\\\\\\\\\\", line 124, in main\\\\\\\\n2025-06-26 03:30:57     ray.get(runner.run.remote(ppo_config))\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 22, in auto_init_wrapper\\\\\\\\n2025-06-26 03:30:57     return fn(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 104, in wrapper\\\\\\\\n2025-06-26 03:30:57     return func(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2849, in get\\\\\\\\n2025-06-26 03:30:57     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n2025-06-26 03:30:57                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 937, in get_objects\\\\\\\\n2025-06-26 03:30:57     raise value.as_instanceof_cause()\\\\\\\\n2025-06-26 03:30:57 ray.exceptions.RayTaskError(ValueError): \\\\\\\\u001b[36mray::Runner.run()\\\\\\\\u001b[39m (pid=12325, ip=33.41.79.235, actor_id=f68735385a682039044e373a01000000, repr=<main.Runner object at 0x7f79df2da4d0>)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/trainer/main.py\\\\\\\\\\\\\\\", line 94, in run\\\\\\\\n2025-06-26 03:30:57     trainer.fit()\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 639, in fit\\\\\\\\n2025-06-26 03:30:57     actor_output = self.actor_rollout_ref_wg.update_actor(batch)\\\\\\\\n2025-06-26 03:30:57                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n2025-06-26 03:30:57     output = ray.get(output)\\\\\\\\n2025-06-26 03:30:57              ^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57 ray.exceptions.RayTaskError(ValueError): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_ref_update_actor()\\\\\\\\u001b[39m (pid=596, ip=33.41.91.191, actor_id=f85afa3102d77bfd6615e86d01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f365e12f790>)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-06-26 03:30:57     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-06-26 03:30:57     return func(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 506, in update_actor\\\\\\\\n2025-06-26 03:30:57     metrics = self.actor.update_policy(data=data)\\\\\\\\n2025-06-26 03:30:57               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/workers/actor/dp_actor.py\\\\\\\\\\\\\\\", line 271, in update_policy\\\\\\\\n2025-06-26 03:30:57     loss.backward()\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/_tensor.py\\\\\\\\\\\\\\\", line 648, in backward\\\\\\\\n2025-06-26 03:30:57     torch.autograd.backward(\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/autograd/__init__.py\\\\\\\\\\\\\\\", line 353, in backward\\\\\\\\n2025-06-26 03:30:57     _engine_run_backward(\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/autograd/graph.py\\\\\\\\\\\\\\\", line 824, in _engine_run_backward\\\\\\\\n2025-06-26 03:30:57     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context\\\\\\\\n2025-06-26 03:30:57     return func(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/distributed/fsdp/_runtime_utils.py\\\\\\\\\\\\\\\", line 721, in _post_backward_hook\\\\\\\\n2025-06-26 03:30:57     _assert_in_training_states(state, [TrainingState.FORWARD_BACKWARD])\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/distributed/fsdp/_common_utils.py\\\\\\\\\\\\\\\", line 450, in _assert_in_training_states\\\\\\\\n2025-06-26 03:30:57     raise ValueError(msg)\\\\\\\\n2025-06-26 03:30:57 ValueError: expected to be in states [<TrainingState.FORWARD_BACKWARD: 2>] but current state is TrainingState.IDLE\\\\\\\\n```\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nvllm==0.9.1\\\\\\\\ntorch==2.7.0\\\\\\\\ntransformers==4.51.3\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":36,\\\\\\\"title\\\\\\\":\\\\\\\"The training time becomes slower and slower\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Thank you for your great work. When I attempted to train the multimodal model, the training time became significantly longer, and I found that the process stays at the initialization stage for a long time, showing as ray::WorkerDict. At this point:\\\\\\\\nnvidia-smi shows that the GPU memory is occupied, but the GPU utilization rate is 0%.\\\\\\\\nThe program does not report any errors, but there is no progress for a long time.\\\\\\\\n\\\\\\\\nCan you offer me the solutions for this problem, thank you very much.\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":35,\\\\\\\"title\\\\\\\":\\\\\\\"Training crashes before Computing Log Prob of First step with worker.rollout.temperature>1.0\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Every time I run a training with a worker.rollout.temperature higher than 1.0 the training process crashes in the first step of the training process, is this intended in this way?\\\\\\\\n\\\\\\\\nMany thanks for your work!\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":34,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u6d4b\\\\\\\\u8bd5\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u4f7f\\\\\\\\u7528Qwen2.5VL\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u4e86\\\\\\\\u81ea\\\\\\\\u5df1\\\\\\\\u7684\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\uff0c\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u8fdb\\\\\\\\u884c\\\\\\\\u6d4b\\\\\\\\u8bd5\\\\\\\\uff1f\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":33,\\\\\\\"title\\\\\\\":\\\\\\\"How to imply a rule-based verification function for different tasks during RL?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Thank you for your excellent work!\\\\\\\\n\\\\\\\\nI hava ten different tasks, and I want imply a rule-based verification function for each task during RL training. How are these reward score calculations integrated and utilized within easy-R1?\\\\\\\\n\\\\\\\\nLooking forward to your reply!\\\\\\\\n\\\\\\\\nYours\\\\\\\\n\\\\\\\\nXiaozhe Li\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":32,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u4e4b\\\\\\\\u524d\\\\\\\\u7684\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u8fc7\\\\\\\\u7a0b\\\\\\\\u5361\\\\\\\\u4f4f\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u4e4b\\\\\\\\u524d\\\\\\\\u90fd\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u6b63\\\\\\\\u5e38\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u7684\\\\\\\\uff0c\\\\\\\\u73b0\\\\\\\\u5728\\\\\\\\u4f1a\\\\\\\\u5361\\\\\\\\u5728\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u9636\\\\\\\\u6bb5\\\\\\\\uff0c\\\\\\\\u663e\\\\\\\\u5b58\\\\\\\\u5360\\\\\\\\u7528\\\\\\\\u4f4e\\\\\\\\uff0c\\\\\\\\u5229\\\\\\\\u7528\\\\\\\\u7387\\\\\\\\u4e3a0\\\\\\\\uff0c\\\\\\\\u4e00\\\\\\\\u76f4\\\\\\\\u5361\\\\\\\\u4f4f\\\\\\\\u3002\\\\\\\\n\\\\\\\\n(WorkerDict pid=1768099) NCCL version 2.21.5+cuda12.4\\\\\\\\nProcessed prompts:   0%|          | 0/60 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\\\\\\\\nProcessed prompts:   2%|\\\\\\\\u258f         | 1/60 [00:05<05:39,  5.75s/it, est. speed input: 82.26 toks/s, output: 14.09 toks/s]\\\\\\\\nProcessed prompts:   7%|\\\\\\\\u258b         | 4/60 [00:06<01:05,  1.17s/it, est. speed input: 312.30 toks/s, output: 56.78 toks/s]\\\\\\\\nProcessed prompts:  10%|\\\\\\\\u2588         | 6/60 [00:06<00:37,  1.45it/s, est. speed input: 457.89 toks/s, output: 86.64 toks/s]\\\\\\\\nProcessed prompts:  17%|\\\\\\\\u2588\\\\\\\\u258b        | 10/60 [00:06<00:17,  2.94it/s, est. speed input: 735.35 toks/s, output: 146.91 toks/s]\\\\\\\\nProcessed prompts:  20%|\\\\\\\\u2588\\\\\\\\u2588        | 12/60 [00:06<00:12,  3.71it/s, est. speed input: 858.36 toks/s, output: 177.54 toks/s]\\\\\\\\nProcessed prompts:  27%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b       | 16/60 [00:06<00:07,  6.12it/s, est. speed input: 1122.34 toks/s, output: 244.84 toks/s]\\\\\\\\nProcessed prompts:  30%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588       | 18/60 [00:06<00:05,  7.17it/s, est. speed input: 1238.74 toks/s, output: 276.87 toks/s]\\\\\\\\nProcessed prompts:  38%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258a      | 23/60 [00:06<00:03, 11.79it/s, est. speed input: 1558.40 toks/s, output: 366.72 toks/s]\\\\\\\\nProcessed prompts:  43%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258e     | 26/60 [00:07<00:02, 13.39it/s, est. speed input: 1725.24 toks/s, output: 417.77 toks/s]\\\\\\\\nProcessed prompts:  48%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258a     | 29/60 [00:07<00:02, 12.66it/s, est. speed input: 1854.35 toks/s, output: 464.50 toks/s]\\\\\\\\nProcessed prompts:  53%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258e    | 32/60 [00:07<00:01, 14.94it/s, est. speed input: 2015.29 toks/s, output: 521.66 toks/s]\\\\\\\\nProcessed prompts:  60%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588    | 36/60 [00:07<00:01, 18.33it/s, est. speed input: 2228.90 toks/s, output: 597.67 toks/s]\\\\\\\\nProcessed prompts:  65%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258c   | 39/60 [00:07<00:01, 18.39it/s, est. speed input: 2364.58 toks/s, output: 651.93 toks/s]\\\\\\\\nProcessed prompts:  72%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258f  | 43/60 [00:07<00:00, 22.35it/s, est. speed input: 2572.17 toks/s, output: 734.00 toks/s]\\\\\\\\nProcessed prompts:  77%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b  | 46/60 [00:08<00:00, 17.34it/s, est. speed input: 2658.27 toks/s, output: 779.23 toks/s]\\\\\\\\nProcessed prompts:  82%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258f | 49/60 [00:08<00:00, 16.99it/s, est. speed input: 2768.72 toks/s, output: 835.26 toks/s]\\\\\\\\nProcessed prompts:  88%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258a | 53/60 [00:08<00:00, 20.02it/s, est. speed input: 2948.11 toks/s, output: 922.69 toks/s]\\\\\\\\nProcessed prompts:  93%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258e| 56/60 [00:08<00:00, 12.96it/s, est. speed input: 2957.91 toks/s, output: 952.76 toks/s]\\\\\\\\nProcessed prompts:  97%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b| 58/60 [00:09<00:00, 10.23it/s, est. speed input: 2946.59 toks/s, output: 971.81 toks/s]\\\\\\\\n(WorkerDict pid=1765528) Before vllm offload in sharding manager: 44.16 GB / 79.14 GB.\\\\\\\\n(WorkerDict pid=1765528) INFO 06-27 23:52:18 [block_pool.py:264] Successfully reset prefix cache\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:51:58 [executor_base.py:226] It took 0.711307 seconds to wake up tags ['weights']. [repeated 2x across\\\\\\\\n cluster]\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:51:59 [executor_base.py:226] It took 0.030391 seconds to wake up tags ['kv_cache']. [repeated 2x acros\\\\\\\\ns cluster]\\\\\\\\n(WorkerDict pid=1768098) NCCL version 2.21.5+cuda12.4\\\\\\\\nProcessed prompts: 100%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588| 60/60 [00:09<00:00,  6.06it/s, est. speed input: 2868.58 toks/s, output: 974.38 toks/s]\\\\\\\\n(WorkerDict pid=1765528) INFO 06-27 23:52:19 [gpu_worker.py:81] Sleep mode freed 39.72 GiB memory, 4.44 GiB memory is still in use.\\\\\\\\n(WorkerDict pid=1765528) INFO 06-27 23:52:19 [executor_base.py:210] It took 1.040433 seconds to fall asleep.\\\\\\\\n(WorkerDict pid=1765528) After vllm offload in sharding manager: 4.44 GB / 79.14 GB.\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:52:25 [block_pool.py:264] Successfully reset prefix cache\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:52:27 [gpu_worker.py:81] Sleep mode freed 39.72 GiB memory, 1.77 GiB memory is still in use.\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:52:27 [executor_base.py:210] It took 1.032600 seconds to fall asleep.\\\\\\\\n(WorkerDict pid=1768099) INFO 06-27 23:52:36 [block_pool.py:264] Successfully reset prefix cache\\\\\\\\n(Runner pid=1764900) validation generation end\\\\\\\\n(WorkerDict pid=1768099) INFO 06-27 23:52:37 [gpu_worker.py:81] Sleep mode freed 39.77 GiB memory, 4.54 GiB memory is still in use.\\\\\\\\n(WorkerDict pid=1768099) INFO 06-27 23:52:37 [executor_base.py:210] It took 1.036531 seconds to fall asleep.\\\\\\\\n(Runner pid=1764900) answer: C\\\\\\\\n(Runner pid=1764900) ground_truth: C\\\\\\\\n\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":31,\\\\\\\"title\\\\\\\":\\\\\\\"IndexError: list index out of range from rotary_embedding.py when using vLLM v1\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I can run the example script successfully, but when running on a custimized dataset, and got error in rollout process\\\\\\\\n\\\\\\\\n```\\\\\\\\n(Runner pid=82786)     output = self.rollout.generate_sequences(prompts=prompts)                                                                                                                                                                                                                              \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                              \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context                                                                                                                                                                 \\\\\\\\n(Runner pid=82786)     return func(*args, **kwargs)                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 192, in generate_sequences                                                                                                                                                                                       \\\\\\\\n(Runner pid=82786)     completions: List[RequestOutput] = self.inference_engine.generate(                                                                                                                                                                                                                     \\\\\\\\n(Runner pid=82786)                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                     \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1131, in inner                                                                                                                                                                                        \\\\\\\\n(Runner pid=82786)     return fn(*args, **kwargs)                                                                                                                                                                                                                                                             \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                             \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 465, in generate                                                                                                                                                                            \\\\\\\\n(Runner pid=82786)     outputs = self._run_engine(use_tqdm=use_tqdm)                                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 1384, in _run_engine                                                                                                                                                                        \\\\\\\\n(Runner pid=82786)     step_outputs = self.llm_engine.step()                                                                                                                                                                                                                                                  \\\\\\\\n(Runner pid=82786)                    ^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                  \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 222, in step                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)     outputs = self.engine_core.get_output()                                                                                                                                                                                                                                                \\\\\\\\n(Runner pid=82786)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 200, in get_output                                                                                                                                                                    \\\\\\\\n(Runner pid=82786)     return self.engine_core.step()                                                                                                                                                                                                                                                         \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                         \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 206, in step                                                                                                                                                                                 \\\\\\\\n(Runner pid=82786)     output = self.model_executor.execute_model(scheduler_output)                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\\\\\\\\\\\\\\\", line 77, in execute_model                                                                                                                                                                   \\\\\\\\n(Runner pid=82786)     output = self.collective_rpc(\\\\\\\\\\\\\\\"execute_model\\\\\\\\\\\\\\\",                                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc                                                                                                                                                             \\\\\\\\n(Runner pid=82786)     answer = run_method(self.driver_worker, method, args, kwargs)                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2347, in run_method                                                                                                                                                                                   \\\\\\\\n(Runner pid=82786)     return func(*args, **kwargs)                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context                                                                                                                                                                 \\\\\\\\n(Runner pid=82786)     return func(*args, **kwargs)                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 242, in execute_model                                                                                                                                                                  \\\\\\\\n(Runner pid=82786)     output = self.model_runner.execute_model(scheduler_output)                                                                                                                                                                                                                             \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                             \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context                                                                                                                                                                 \\\\\\\\n(Runner pid=82786)     return func(*args, **kwargs)                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\\\\\\\\\\\\\\\", line 980, in execute_model                                                                                                                                                            \\\\\\\\n(Runner pid=82786)     self._update_states(scheduler_output)                                                                                                                                                                                                                                                  \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\\\\\\\\\\\\\\\", line 369, in _update_states                                                                                                                                                           \\\\\\\\n(Runner pid=82786)     MRotaryEmbedding.get_input_positions_tensor(                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/model_executor/layers/rotary_embedding.py\\\\\\\\\\\\\\\", line 1069, in get_input_positions_tensor                                                                                                                                  \\\\\\\\n(Runner pid=82786)     image_grid_thw[image_index][0],                                                                                                                                                                                                                                                        \\\\\\\\n(Runner pid=82786)     ~~~~~~~~~~~~~~^^^^^^^^^^^^^                                                                                                                                                                                                                                                            \\\\\\\\n(Runner pid=82786) IndexError: list index out of range\\\\\\\\n```\\\\\\\\n\\\\\\\\nThe errors only exist when using VLLM v1, and when using `VLLM_USE_V1=0` the code can run, but the results are totally wrong, seems the model didn't receive the correct images in this case. Wondering why this happends? \\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":30,\\\\\\\"title\\\\\\\":\\\\\\\"large rollout.n encounter OOM in training.\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi, I was training using 4 nodes on a 7B qwen2.5vl model, I tried to enlarge the rollout number, but got OOM error. I wonder how to correctly set the training parameters, especially the batch size in multinode training setup? Here's what i'm using now\\\\\\\\n\\\\\\\\n```\\\\\\\\n  python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n    config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.logger=['console','wandb'] \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_prompt_length=2048 \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_response_length=1024 \\\\\\\\\\\\\\\\\\\\\\\\n    data.rollout_batch_size=512 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.global_batch_size=128 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.micro_batch_size_per_device_for_update=4 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.micro_batch_size_per_device_for_experience=16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.n=128 \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_pixels=1258291 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.save_freq=5 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.n_gpus_per_node=8 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.nnodes=4 \\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\n```\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":29,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u6784\\\\\\\\u5efa\\\\\\\\u81ea\\\\\\\\u5df1\\\\\\\\u7684\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u7528\\\\\\\\u4e8eGRPO\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u679c\\\\\\\\u60f3\\\\\\\\u8981\\\\\\\\u81ea\\\\\\\\u5df1\\\\\\\\u6784\\\\\\\\u5efaVQA\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\uff0c\\\\\\\\u5e94\\\\\\\\u5f53\\\\\\\\u4ee5\\\\\\\\u4ec0\\\\\\\\u4e48\\\\\\\\u683c\\\\\\\\u5f0f\\\\\\\\u7ec4\\\\\\\\u7ec7\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\uff1f\\\\\\\\u662f\\\\\\\\u5426\\\\\\\\u652f\\\\\\\\u6301\\\\\\\\u4f7f\\\\\\\\u7528\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u7684\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u8fdb\\\\\\\\u884cGRPO\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\uff1f\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":28,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5f20\\\\\\\\u91cf\\\\\\\\u7ef4\\\\\\\\u5ea6\\\\\\\\u4e0d\\\\\\\\u7b26\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u8fd0\\\\\\\\u884cGRPO\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u4ee3\\\\\\\\u7801\\\\\\\\uff0c\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\\u5982\\\\\\\\u4e0b\\\\\\\\uff1a\\\\\\\\nTraceback (most recent call last):\\\\\\\\n  File \\\\\\\\\\\\\\\"<frozen runpy>\\\\\\\\\\\\\\\", line 198, in _run_module_as_main                                                                               | 1.00/64.0 [00:01<01:12, 1.16s/it]\\\\\\\\n  File \\\\\\\\\\\\\\\"<frozen runpy>\\\\\\\\\\\\\\\", line 88, in _run_code\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 128, in <module>\\\\\\\\n    main()\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 124, in main\\\\\\\\n    ray.get(runner.run.remote(ppo_config))\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 22, in auto_init_wrapper\\\\\\\\n    return fn(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 104, in wrapper\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2849, in get\\\\\\\\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 937, in get_objects\\\\\\\\n    raise value.as_instanceof_cause()\\\\\\\\nray.exceptions.RayTaskError(RuntimeError): ray::Runner.run() (pid=305610, ip=10.26.2.130, actor_id=8ec8ba19ed64d85dea29943601000000, repr=<main.Runner object at 0x791294b97170>)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 94, in run\\\\\\\\n    trainer.fit()\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 589, in fit\\\\\\\\n    old_log_probs = self.actor_rollout_ref_wg.compute_log_probs(batch)\\\\\\\\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n    output = ray.get(output)\\\\\\\\n             ^^^^^^^^^^^^^^^\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\nray.exceptions.RayTaskError(RuntimeError): ray::WorkerDict.actor_rollout_ref_compute_log_probs() (pid=307930, ip=10.26.2.130, actor_id=0a1d29a3e2cd087a716a932001000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x72b04f5baa80>)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n    return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 585, in compute_log_probs\\\\\\\\n    output = self.actor.compute_log_prob(data=data)\\\\\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/workers/actor/dp_actor.py\\\\\\\\\\\\\\\", line 209, in compute_log_prob\\\\\\\\n    log_probs = self._forward_micro_batch(model_inputs, temperature=temperature)\\\\\\\\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/workers/actor/dp_actor.py\\\\\\\\\\\\\\\", line 124, in _forward_micro_batch\\\\\\\\n    output = self.actor_module(\\\\\\\\n             ^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1751, in _wrapped_call_impl\\\\\\\\n    return self._call_impl(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1762, in _call_impl\\\\\\\\n    return forward_call(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\\\\\\\\\\\\\\\", line 856, in forward\\\\\\\\n    output = self._fsdp_wrapped_module(*args, **kwargs)\\\\\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1751, in _wrapped_call_impl\\\\\\\\n    return self._call_impl(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1762, in _call_impl\\\\\\\\n    return forward_call(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/models/transformers/qwen2_vl.py\\\\\\\\\\\\\\\", line 340, in qwen2_vl_forward_new\\\\\\\\n    outputs = self.model(\\\\\\\\n              ^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1751, in _wrapped_call_impl\\\\\\\\n    return self._call_impl(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1762, in _call_impl\\\\\\\\n    return forward_call(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/models/transformers/qwen2_vl.py\\\\\\\\\\\\\\\", line 307, in qwen2_vl_base_forward_new\\\\\\\\n    outputs = self.language_model(\\\\\\\\n              ^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1751, in _wrapped_call_impl\\\\\\\\n    return self._call_impl(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1762, in _call_impl\\\\\\\\n    return forward_call(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py\\\\\\\\\\\\\\\", line 875, in forward\\\\\\\\n    \\\\\\\\\\\\\\\"full_attention\\\\\\\\\\\\\\\": create_causal_mask(**mask_kwargs),\\\\\\\\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/transformers/masking_utils.py\\\\\\\\\\\\\\\", line 753, in create_causal_mask\\\\\\\\n    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(\\\\\\\\n                                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/transformers/masking_utils.py\\\\\\\\\\\\\\\", line 704, in _preprocess_mask_arguments\\\\\\\\n    position_ids = position_ids.expand(batch_size, -1)\\\\\\\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\nRuntimeError: expand(torch.cuda.LongTensor{[3, 1, 3138]}, size=[1, -1]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)\\\\\\\\n\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u89e3\\\\\\\\u51b3\\\\\\\\u4e0a\\\\\\\\u8ff0\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\\uff1f\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":27,\\\\\\\"title\\\\\\\":\\\\\\\"Unable to get the tutorial working\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hello, I am following the instructions to get started with EasyR1; however, I keep getting errors in the process. These errors change every time a run the docker environment: I have gotten errors ranging from a problem with the number of gpus I am passing in ---\\\\\\\\\\\\\\\"got gpu 0 expected 8\\\\\\\\\\\\\\\" to workers not being synchronized.\\\\\\\\nAny help in understanding what I am doing wrong would be greatly appreciated. I have looked online and in the issues on this repo, but have not found the solution. I am new to docker, so apologies if this is a simple problem to fix or if I did not set up something correctly.\\\\\\\\n\\\\\\\\nThe steps I have done are: \\\\\\\\n1. downloaded the docker enviroment: `docker pull hiyouga/verl:ngc-th2.7.0-cu12.6-vllm0.9.1`\\\\\\\\n2. used `docker run --gpus all --ipc=host --ulimit memlock=-1 -it --rm -v /HOME_DIR/EasyR1:/workspace/EasyR1 -w /workspace db618adc68d5 bash`\\\\\\\\n3. ran `bash examples/qwen2_5_vl_7b_geo3k_grpo.sh`\\\\\\\\n\\\\\\\\nAnd, most recently, I am getting the following traceback/error:\\\\\\\\n```\\\\\\\\n /usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_5_vl.py:930: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)\\\\\\\\n(WorkerDict pid=12946)   sizes = grid_thw.prod(-1) // merge_size // merge_size\\\\\\\\n(WorkerDict pid=12943) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`. [repeated 7x across cluster]\\\\\\\\n[...]\\\\\\\\nNCCL version 2.21.5+cuda12.4\\\\\\\\n(WorkerDict pid=25283) [rank0]:[W713 04:12:56.043733966 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\\\\\\\\nTraceback (most recent call last):\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/lib/python3.10/runpy.py\\\\\\\\\\\\\\\", line 196, in _run_module_as_main\\\\\\\\n    return _run_code(code, main_globals, None,\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/lib/python3.10/runpy.py\\\\\\\\\\\\\\\", line 86, in _run_code\\\\\\\\n    exec(code, run_globals)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 128, in <module>\\\\\\\\n    main()\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 124, in main\\\\\\\\n    ray.get(runner.run.remote(ppo_config))\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 21, in auto_init_wrapper\\\\\\\\n    return fn(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 103, in wrapper\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2771, in get\\\\\\\\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 919, in get_objects\\\\\\\\n    raise value.as_instanceof_cause()\\\\\\\\nray.exceptions.RayTaskError(DistBackendError): ray::Runner.run() (pid=24423, ip=192.168.0.2, actor_id=971cb4efa74e3acac3b5b28a01000000, repr=<main.Runner object at 0x7f9a701eca90>)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 93, in run\\\\\\\\n    trainer.init_workers()\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 302, in init_workers\\\\\\\\n    self.actor_rollout_ref_wg.init_model()\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n    output = ray.get(output)\\\\\\\\nray.exceptions.RayTaskError(DistBackendError): ray::WorkerDict.actor_rollout_ref_init_model() (pid=29344, ip=192.168.0.2, actor_id=a4a7e38e959ee13c17167d8501000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f5dbc82b310>)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n    return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 366, in init_model\\\\\\\\n    self._build_model_optimizer(\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 242, in _build_model_optimizer\\\\\\\\n    dist.barrier()\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\\\\\\\\\\\\\\\", line 81, in wrapper\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\\\\\\\\\\\\\\\", line 4551, in barrier\\\\\\\\n    work = group.barrier(opts=opts)\\\\\\\\ntorch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\\\\\\\\nncclUnhandledCudaError: Call to CUDA function failed.\\\\\\\\nLast error:\\\\\\\\nCuda failure 'invalid argument'\\\\\\\\n(WorkerDict pid=29344) \\\\\\\\n(WorkerDict pid=29344) e4077bb80b9c:29344:30132 [0] transport/p2p.cc:275 NCCL WARN Cuda failure 'invalid argument'\\\\\\\\n\\\\\\\\n```\\\\\\\\n\\\\\\\\nAnother time I ran this I got:\\\\\\\\n`ValueError: Total available GPUs 0 is less than total desired GPUs 8.`\\\\\\\\n\\\\\\\\nAnother time i get:\\\\\\\\n`(Runner pid=7873)   File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/vllm/compilation/backends.py\\\\\\\\\\\\\\\", line 677, in __call__\\\\\\\\n(Runner pid=7873)     with torch.cuda.graph(cudagraph, pool=self.graph_pool):\\\\\\\\n(Runner pid=7873)   File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/cuda/graphs.py\\\\\\\\\\\\\\\", line 186, in __exit__\\\\\\\\n(Runner pid=7873)     self.cuda_graph.capture_end()\\\\\\\\n(Runner pid=7873)   File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/cuda/graphs.py\\\\\\\\\\\\\\\", line 84, in capture_end\\\\\\\\n(Runner pid=7873)     super().capture_end()\\\\\\\\n(Runner pid=7873) RuntimeError: CUDA error: operation not permitted\\\\\\\\n(Runner pid=7873) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\\\\\\\n(Runner pid=7873) For debugging consider passing CUDA_LAUNCH_BLOCKING=1\\\\\\\\n(Runner pid=7873) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\\\\\\\n(Runner pid=7873) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WorkerDict.actor_rollout_ref_init_model() (pid=12947, ip=192.168.0.3, actor_id=92b10155f368aac73ed7f31601000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f2ce366b2e0>)`\\\\\\\\n\\\\\\\\nI am not sure what I am doing wrong. Shouldn't this script run without any issues? At least, that is what the tutorial makes it seem like. \\\\\\\\n\\\\\\\\nThank you!\\\\\\\\n\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":26,\\\\\\\"title\\\\\\\":\\\\\\\"tokenized_path \\\\\\\\u652f\\\\\\\\u6301token\\\\\\\\u4fdd\\\\\\\\u5b58\\\\\\\\u4e48\\\\\\\\uff1f\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5927\\\\\\\\u4f6c\\\\\\\\u4f60\\\\\\\\u597d\\\\\\\\uff0c\\\\\\\\u60f3\\\\\\\\u77e5\\\\\\\\u9053EasyR1\\\\\\\\u6709\\\\\\\\u7c7b\\\\\\\\u4f3cllamafactory\\\\\\\\u4e2d\\\\\\\\uff0c\\\\\\\\u652f\\\\\\\\u6301tokenized_path\\\\\\\\u8fdb\\\\\\\\u884c\\\\\\\\u8bad\\\\\\\\u7ec3token\\\\\\\\u4fdd\\\\\\\\u5b58\\\\\\\\u7684\\\\\\\\u8bbe\\\\\\\\u7f6e\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\\u4e48\\\\\\\\uff1f\\\\\\\\n\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":25,\\\\\\\"title\\\\\\\":\\\\\\\"any plan for supporting Flow-GRPO?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"https://github.com/yifan123/flow_grpo\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":24,\\\\\\\"title\\\\\\\":\\\\\\\"Some bug with training\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I tried to run the tutorial on my server recently. It is a 8*A100 GPUs with 80GB RAM for each. As the cuda version in the host in below the lowest requirement and it is hard to install another version. I choose to use docker to build the environment, I pull the container and start the container. The script in the examples work proporly in the first and second step. However, when I get to the third step of updating the policy, it just clash.\\\\\\\\n\\\\\\\\n                                                                                                                           \\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 22, in auto_init_wrapper             \\\\\\\\nray.get(runner.run.remote(ppo_config))                                                                                     \\\\\\\\n                                                                                                                           \\\\\\\\n                                                                                                                           \\\\\\\\n                                                                                                                           \\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 104, in wrapper                    \\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2849, in get                                 \\\\\\\\n(Runner pid=16098) wandb:              critic/rewards/mean 0.33543                                                         \\\\\\\\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 937, in get_objects\\\\\\\\n(pid=16098) Running step 0:   5%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b                                                   | 3.00/60.0 [16:06<4:30:39, 285s/it]\\\\\\\\n\\\\\\\\n(pid=33233) Train mini-batches 1: 100%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588| 4.00/4.00 [12:25<00:00, 22.8s/it]\\\\\\\\n    raise value.as_instanceof_cause()\\\\\\\\n\\\\\\\\n(pid=33233) Update policy 2: 100%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588| 5.00/5.00 [11:53<00:00, 4.64s/it]ray.exceptions.RayTaskError(IndexError): ray::Runner.run() (pid=16098, ip=172.17.0.2, actor_id=462c3f5e6c3ec7441f02a99e01000000, repr=<main.Runner object at 0x707158542f20>)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 94, in run\\\\\\\\n    trainer.fit()\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 607, in fit\\\\\\\\n    old_log_probs = self.actor_rollout_ref_wg.compute_log_probs(batch)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n    output = ray.get(output)\\\\\\\\nray.exceptions.RayTaskError(IndexError): ray::WorkerDict.actor_rollout_ref_compute_log_probs() (pid=33458, ip=172.17.0.2, actor_id=11af29bb9fed89a5977131d901000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x72fdd7d62290>)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n    return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 585, in compute_log_probs\\\\\\\\n    output = self.actor.compute_log_prob(data=data)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/workers/actor/dp_actor.py\\\\\\\\\\\\\\\", line 199, in compute_log_prob\\\\\\\\n    micro_batches, batch_idx_list = prepare_dynamic_batch(data, max_token_len=max_token_len)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/utils/seqlen_balancing.py\\\\\\\\\\\\\\\", line 307, in prepare_dynamic_batch\\\\\\\\n    batch, batch_idx_list = rearrange_micro_batches(data.batch, max_token_len=max_token_len)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/utils/seqlen_balancing.py\\\\\\\\\\\\\\\", line 261, in rearrange_micro_batches\\\\\\\\n    micro_bsz_idx = get_seqlen_balanced_partitions(effective_seqlen, num_micro_batches, equal_size=False)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/utils/seqlen_balancing.py\\\\\\\\\\\\\\\", line 184, in get_seqlen_balanced_partitions\\\\\\\\n    partitions = karmarkar_karp(seqlen_list=seqlen_list, k_partitions=k_partitions, equal_size=equal_size)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/utils/seqlen_balancing.py\\\\\\\\\\\\\\\", line 114, in karmarkar_karp\\\\\\\\n    heapq.heappush(states_pq, State(items=[(idx, seqlen)], k=k_partitions))\\\\\\\\n(Runner pid=16098) wandb:               critic/rewards/min 0                                                               \\\\\\\\n(Runner pid=16098) wandb:                 critic/score/max 1                                                               \\\\\\\\n(Runner pid=16098) wandb:                critic/score/mean 0.33543                                                         \\\\\\\\n(Runner pid=16098) wandb:                 critic/score/min 0                                                               \\\\\\\\n(Runner pid=16098) wandb:       global_seqlen/balanced_max 247670                                                          \\\\\\\\n(Runner pid=16098) wandb:       global_seqlen/balanced_min 247090                                                          \\\\\\\\n(Runner pid=16098) wandb:                global_seqlen/max 254919                                                          \\\\\\\\n(Runner pid=16098) wandb:               global_seqlen/mean 247163.25                                                       \\\\\\\\n(Runner pid=16098) wandb:                global_seqlen/min 240316                                                          \\\\\\\\n(Runner pid=16098) wandb:        global_seqlen/minmax_diff 14603                                                           \\\\\\\\n(Runner pid=16098) wandb:          perf/cpu_memory_used_gb 107.23108                                                       \\\\\\\\n(Runner pid=16098) wandb:     perf/max_memory_allocated_gb 42.98667                                                        \\\\\\\\n(Runner pid=16098) wandb:      perf/max_memory_reserved_gb 64.66187                                                        \\\\\\\\n(Runner pid=16098) wandb:                   perf/mfu_actor 0.33784                                                         \\\\\\\\n(Runner pid=16098) wandb:                  perf/throughput 955.20544                                                       \\\\\\\\n(Runner pid=16098) wandb:               perf/time_per_step 258.75402                                                       \\\\\\\\n(Runner pid=16098) wandb:            perf/total_num_tokens 1977306                                                         \\\\\\\\n(Runner pid=16098) wandb:         prompt_length/clip_ratio 0                                                               \\\\\\\\n(Runner pid=16098) wandb:                prompt_length/max 776                                                             \\\\\\\\n(Runner pid=16098) wandb:               prompt_length/mean 425.51953                                                       \\\\\\\\n(Runner pid=16098) wandb:                prompt_length/min 387                                                             \\\\\\\\n(Runner pid=16098) wandb:       response_length/clip_ratio 0.00039                                                         \\\\\\\\n(Runner pid=16098) wandb:              response_length/max 2048                                                            \\\\\\\\n(Runner pid=16098) wandb:             response_length/mean 346.86563                                                       \\\\\\\\n(Runner pid=16098) wandb:              response_length/min 56                                                              \\\\\\\\n(Runner pid=16098) wandb:                  reward/accuracy 0.325                                                           \\\\\\\\n(Runner pid=16098) wandb:                    reward/format 0.4293                                                          \\\\\\\\n(Runner pid=16098) wandb:                   reward/overall 0.33543                                                         \\\\\\\\n(Runner pid=16098) wandb:          timing_per_token_ms/adv 6e-05                                                           \\\\\\\\n(Runner pid=16098) wandb:          timing_per_token_ms/gen 0.08111                                                         \\\\\\\\n(Runner pid=16098) wandb:          timing_per_token_ms/old 0.01803                                                         \\\\\\\\n(Runner pid=16098) wandb:          timing_per_token_ms/ref 0.01467                                                         \\\\\\\\n(Runner pid=16098) wandb:       timing_per_token_ms/reward 0.00238                                                         \\\\\\\\n(Runner pid=16098) wandb: timing_per_token_ms/update_actor 0.0604                                                          \\\\\\\\n(Runner pid=16098) wandb:                     timing_s/adv 0.12637                                                         \\\\\\\\n(Runner pid=16098) wandb:                     timing_s/gen 72.02265                                                        \\\\\\\\n(Runner pid=16098) wandb:                     timing_s/old 35.64964                                                        \\\\\\\\n(Runner pid=16098) wandb:                     timing_s/ref 29.00215                                                        \\\\\\\\n(Runner pid=16098) wandb:                  timing_s/reward 2.11218                                                         \\\\\\\\n(Runner pid=16098) wandb:                    timing_s/step 258.75402                                                       \\\\\\\\n(Runner pid=16098) wandb:            timing_s/update_actor 119.43438                                                       \\\\\\\\n(Runner pid=16098) wandb:              val/accuracy_reward 0.3827                                                          \\\\\\\\n(Runner pid=16098) wandb:                val/format_reward 0.1797                                                          \\\\\\\\n(Runner pid=16098) wandb:               val/overall_reward 0.3624                                                          \\\\\\\\n(Runner pid=16098) wandb:                 val/reward_score 0.3624                                                          \\\\\\\\n(Runner pid=16098) wandb:     val_prompt_length/clip_ratio 0                                                               \\\\\\\\n(Runner pid=16098) wandb:            val_prompt_length/max 784                                                             \\\\\\\\n(Runner pid=16098) wandb:           val_prompt_length/mean 428.5574                                                        \\\\\\\\n(Runner pid=16098) wandb:            val_prompt_length/min 387                                                             \\\\\\\\n(Runner pid=16098) wandb:   val_response_length/clip_ratio 0.00333                                                         \\\\\\\\n(Runner pid=16098) wandb:          val_response_length/max 2048                                                            \\\\\\\\n(Runner pid=16098) wandb:         val_response_length/mean 371.5441                                                        \\\\\\\\n(Runner pid=16098) wandb:          val_response_length/min 60                                                              \\\\\\\\n(Runner pid=16098) wandb:                                                                                                  \\\\\\\\n(Runner pid=16098) wandb: \\\\\\\\ud83d\\\\\\\\ude80 View run qwen2_5_vl_7b_geo_grpo at: https://wandb.ai/GreatResearchTakesTime/easy_r1/runs/zabor1wb                                                                                                                        \\\\\\\\n(Runner pid=16098) wandb: \\\\\\\\u2b50\\\\\\\\ufe0f View project at: https://wandb.ai/GreatResearchTakesTime/easy_r1                              \\\\\\\\n(Runner pid=16098) wandb: Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)                    \\\\\\\\n(Runner pid=16098) wandb: Find logs at: ./wandb/run-20250716_071305-zabor1wb/logs                                          \\\\\\\\n(pid=16098) Running step 0:   5%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b                                                   | 3.00/60.0 [16:08<4:30:39, 285s/itException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookroot@60b7a9c\\\\\\\\nroot@60b7a9cfca50:/workspace/EasyR1# \\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":23,\\\\\\\"title\\\\\\\":\\\\\\\"tensor_parallel_size\\\\\\\\u8bbe\\\\\\\\u7f6e\\\\\\\\u4e3a2\\\\\\\\u65f6\\\\\\\\u5019\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u9ed8\\\\\\\\u8ba4\\\\\\\\u7684tensor_parallel_size\\\\\\\\u4e3a1\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u60f3\\\\\\\\u4fee\\\\\\\\u6539\\\\\\\\u62102\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\\uff1a\\\\\\\\n```\\\\\\\\n2025-07-17 17:00:39 | INFO | stdout | \\\\\\\\u001b[36m(pid=47761)\\\\\\\\u001b[0m NCCL version 2.21.5+cuda11.0\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | Traceback (most recent call last):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/runpy.py\\\\\\\\\\\\\\\", line 196, in _run_module_as_main\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return _run_code(code, main_globals, None,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/runpy.py\\\\\\\\\\\\\\\", line 86, in _run_code\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     exec(code, run_globals)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 216, in <module>\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     main()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 212, in main\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     ray.get(runner.run.remote(ppo_config))\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 21, in auto_init_wrapper\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 103, in wrapper\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2822, in get\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 930, in get_objects\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     raise value.as_instanceof_cause()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | ray.exceptions.RayTaskError(KeyError): \\\\\\\\u001b[36mray::Runner.run()\\\\\\\\u001b[39m (pid=41424, ip=10.140.37.138, actor_id=aac3cbdc33303e9fb12a736501000000, repr=<main.Runner object at 0x7fc5f5d2cee0>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 99, in run\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     trainer.init_workers()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 443, in init_workers\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.actor_rollout_wg.init_model()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     output = ray.get(output)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | ray.exceptions.RayTaskError(KeyError): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_init_model()\\\\\\\\u001b[39m (pid=47760, ip=10.140.37.138, actor_id=c03b337d86fc2c524621f42701000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f31ad170c10>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 384, in init_model\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self._build_rollout()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 315, in _build_rollout\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.rollout = vLLMRollout(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 76, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.inference_engine = LLM(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1161, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 247, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.llm_engine = LLMEngine.from_engine_args(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/engine/llm_engine.py\\\\\\\\\\\\\\\", line 510, in from_engine_args\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return engine_cls.from_vllm_config(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 112, in from_vllm_config\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return cls(vllm_config=vllm_config,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 92, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCoreClient.make_client(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 75, in make_client\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return InprocClient(vllm_config, executor_class, log_stats)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 198, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCore(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 64, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.model_executor = executor_class(vllm_config)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/executor_base.py\\\\\\\\\\\\\\\", line 52, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self._init_executor()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 121, in _init_executor\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.collective_rpc(\\\\\\\\\\\\\\\"init_device\\\\\\\\\\\\\\\")\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     answer = run_method(self.driver_worker, method, args, kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2456, in run_method\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/worker/worker_base.py\\\\\\\\\\\\\\\", line 604, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.worker.init_device()  # type: ignore\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 135, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     init_worker_distributed_environment(self.vllm_config, self.rank,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 326, in init_worker_distributed_environment\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 1022, in ensure_model_parallel_initialized\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     initialize_model_parallel(tensor_model_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 975, in initialize_model_parallel\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     _TP = init_model_parallel_group(group_ranks,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 788, in init_model_parallel_group\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return GroupCoordinator(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 252, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.device_communicator = device_comm_cls(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py\\\\\\\\\\\\\\\", line 47, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.ca_comm = CustomAllreduce(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 145, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     if not current_platform.is_rocm() and not _can_p2p(rank, world_size):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 37, in _can_p2p\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     if not gpu_p2p_access_check(rank, i):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\\\\\\\\\\\\\\\", line 248, in gpu_p2p_access_check\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return _gpu_p2p_access_cache[f\\\\\\\\\\\\\\\"{src}->{tgt}\\\\\\\\\\\\\\\"]\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | KeyError: '1->0'\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 16:59:58 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 16:59:59 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 16:59:59 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 17:00:05 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 17:00:06 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 17:00:07 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 17:00:07 | ERROR | stderr |\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_init_model()\\\\\\\\u001b[39m (pid=47762, ip=10.140.37.138, actor_id=fd8e5e2f1f8107d072c83c9801000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fc572d50c10>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 384, in init_model\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._build_rollout()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 315, in _build_rollout\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.rollout = vLLMRollout(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 76, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.inference_engine = LLM(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1161, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 247, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.llm_engine = LLMEngine.from_engine_args(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/engine/llm_engine.py\\\\\\\\\\\\\\\", line 510, in from_engine_args\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return engine_cls.from_vllm_config(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 112, in from_vllm_config\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return cls(vllm_config=vllm_config,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 92, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCoreClient.make_client(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 75, in make_client\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return InprocClient(vllm_config, executor_class, log_stats)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 198, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCore(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 64, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.model_executor = executor_class(vllm_config)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/executor_base.py\\\\\\\\\\\\\\\", line 52, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._init_executor()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 121, in _init_executor\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.collective_rpc(\\\\\\\\\\\\\\\"init_device\\\\\\\\\\\\\\\")\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     answer = run_method(self.driver_worker, method, args, kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2456, in run_method\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/worker/worker_base.py\\\\\\\\\\\\\\\", line 604, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.worker.init_device()  # type: ignore\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 135, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     init_worker_distributed_environment(self.vllm_config, self.rank,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 326, in init_worker_distributed_environment\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 1022, in ensure_model_parallel_initialized\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     initialize_model_parallel(tensor_model_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 975, in initialize_model_parallel\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     _TP = init_model_parallel_group(group_ranks,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 788, in init_model_parallel_group\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return GroupCoordinator(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 252, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.device_communicator = device_comm_cls(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py\\\\\\\\\\\\\\\", line 47, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.ca_comm = CustomAllreduce(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 145, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not current_platform.is_rocm() and not _can_p2p(rank, world_size):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 37, in _can_p2p\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not gpu_p2p_access_check(rank, i):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\\\\\\\\\\\\\\\", line 248, in gpu_p2p_access_check\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return _gpu_p2p_access_cache[f\\\\\\\\\\\\\\\"{src}->{tgt}\\\\\\\\\\\\\\\"]\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | KeyError: '1->0'\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_init_model()\\\\\\\\u001b[39m (pid=47761, ip=10.140.37.138, actor_id=47ed97652b87fc5762ebbb2401000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7eeb7b44f0a0>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 384, in init_model\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._build_rollout()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 315, in _build_rollout\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.rollout = vLLMRollout(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 76, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.inference_engine = LLM(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1161, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 247, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.llm_engine = LLMEngine.from_engine_args(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/engine/llm_engine.py\\\\\\\\\\\\\\\", line 510, in from_engine_args\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return engine_cls.from_vllm_config(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 112, in from_vllm_config\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return cls(vllm_config=vllm_config,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 92, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCoreClient.make_client(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 75, in make_client\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return InprocClient(vllm_config, executor_class, log_stats)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 198, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCore(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 64, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.model_executor = executor_class(vllm_config)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/executor_base.py\\\\\\\\\\\\\\\", line 52, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._init_executor()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 121, in _init_executor\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.collective_rpc(\\\\\\\\\\\\\\\"init_device\\\\\\\\\\\\\\\")\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     answer = run_method(self.driver_worker, method, args, kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2456, in run_method\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/worker/worker_base.py\\\\\\\\\\\\\\\", line 604, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.worker.init_device()  # type: ignore\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 135, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     init_worker_distributed_environment(self.vllm_config, self.rank,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 326, in init_worker_distributed_environment\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 1022, in ensure_model_parallel_initialized\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     initialize_model_parallel(tensor_model_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 975, in initialize_model_parallel\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     _TP = init_model_parallel_group(group_ranks,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 788, in init_model_parallel_group\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return GroupCoordinator(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 252, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.device_communicator = device_comm_cls(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py\\\\\\\\\\\\\\\", line 47, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.ca_comm = CustomAllreduce(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 145, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not current_platform.is_rocm() and not _can_p2p(rank, world_size):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 37, in _can_p2p\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not gpu_p2p_access_check(rank, i):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\\\\\\\\\\\\\\\", line 248, in gpu_p2p_access_check\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return _gpu_p2p_access_cache[f\\\\\\\\\\\\\\\"{src}->{tgt}\\\\\\\\\\\\\\\"]\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | KeyError: '0->1'\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_init_model()\\\\\\\\u001b[39m (pid=46880, ip=10.140.37.138, actor_id=f0ee4094614e96a389af02d101000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fca14d141f0>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 384, in init_model\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._build_rollout()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 315, in _build_rollout\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.rollout = vLLMRollout(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 76, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.inference_engine = LLM(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1161, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 247, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.llm_engine = LLMEngine.from_engine_args(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/engine/llm_engine.py\\\\\\\\\\\\\\\", line 510, in from_engine_args\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return engine_cls.from_vllm_config(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 112, in from_vllm_config\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return cls(vllm_config=vllm_config,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 92, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCoreClient.make_client(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 75, in make_client\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return InprocClient(vllm_config, executor_class, log_stats)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 198, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCore(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 64, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.model_executor = executor_class(vllm_config)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/executor_base.py\\\\\\\\\\\\\\\", line 52, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._init_executor()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 121, in _init_executor\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.collective_rpc(\\\\\\\\\\\\\\\"init_device\\\\\\\\\\\\\\\")\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     answer = run_method(self.driver_worker, method, args, kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2456, in run_method\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/worker/worker_base.py\\\\\\\\\\\\\\\", line 604, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.worker.init_device()  # type: ignore\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 135, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     init_worker_distributed_environment(self.vllm_config, self.rank,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 326, in init_worker_distributed_environment\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 1022, in ensure_model_parallel_initialized\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     initialize_model_parallel(tensor_model_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 975, in initialize_model_parallel\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     _TP = init_model_parallel_group(group_ranks,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 788, in init_model_parallel_group\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return GroupCoordinator(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 252, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.device_communicator = device_comm_cls(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py\\\\\\\\\\\\\\\", line 47, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.ca_comm = CustomAllreduce(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 145, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not current_platform.is_rocm() and not _can_p2p(rank, world_size):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 37, in _can_p2p\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not gpu_p2p_access_check(rank, i):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\\\\\\\\\\\\\\\", line 248, in gpu_p2p_access_check\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return _gpu_p2p_access_cache[f\\\\\\\\\\\\\\\"{src}->{tgt}\\\\\\\\\\\\\\\"]\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | KeyError: '0->1'```\\\\\\\\n\\\\\\\\n\\\\\\\\u6211\\\\\\\\u5c1d\\\\\\\\u8bd5\\\\\\\\u4e86\\\\\\\\u4e00\\\\\\\\u4e9b\\\\\\\\u65b9\\\\\\\\u6cd5\\\\\\\\uff0c\\\\\\\\u6bd4\\\\\\\\u5982\\\\\\\\u8bbe\\\\\\\\u7f6eexport VLLM_RPC_BASE_PATH=/dev/shm\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u6ca1\\\\\\\\u6709\\\\\\\\u7528\\\\\\\\uff0c\\\\\\\\u4ee3\\\\\\\\u7801\\\\\\\\u672c\\\\\\\\u8eab\\\\\\\\u662f\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u8fd0\\\\\\\\u884c\\\\\\\\u7684\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u53ea\\\\\\\\u6539\\\\\\\\u4e86\\\\\\\\u8fd9\\\\\\\\u4e00\\\\\\\\u4e2a\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":22,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u52a0\\\\\\\\u8f7d\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\\uff0cv3.0.1\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u52a0\\\\\\\\u8f7d\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\uff1f\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"v3.0.1\\\\\\\\u4e2d\\\\\\\\u7684\\\\\\\\uff1a\\\\\\\\nEasyR1/verl/utils/dataset.py line 132\\\\\\\\n...\\\\\\\\n        if os.path.isdir(data_path):\\\\\\\\n            # when we use dataset builder, we should always refer to the train split\\\\\\\\n            **file_type = os.path.splitext(os.listdir(data_path)[0])[-1][1:].replace(\\\\\\\\\\\\\\\"jsonl\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\")**\\\\\\\\n            self.dataset = load_dataset(file_type, data_dir=data_path, split=data_split)\\\\\\\\n        elif os.path.isfile(data_path):\\\\\\\\n            file_type = os.path.splitext(data_path)[-1][1:].replace(\\\\\\\\\\\\\\\"jsonl\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\")\\\\\\\\n            self.dataset = load_dataset(file_type, data_files=data_path, split=data_split)\\\\\\\\n        else:\\\\\\\\n            # load remote dataset from huggingface hub\\\\\\\\n            self.dataset = load_dataset(data_path, split=data_split)\\\\\\\\n...\\\\\\\\n\\\\\\\\n\\\\\\\\u8fd9\\\\\\\\u4e00\\\\\\\\u884c\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\\uff0c\\\\\\\\u5904\\\\\\\\u7406\\\\\\\\u4e0d\\\\\\\\u4e86\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u7684./geomery3k \\\\\\\\u8def\\\\\\\\u5f84\\\\\\\\u3002\\\\\\\\n\\\\\\\\u6539\\\\\\\\u6210v3.0.0\\\\\\\\u4e2d\\\\\\\\u7684\\\\\\\\nself.dataset = load_dataset(\\\\\\\\\\\\\\\"parquet\\\\\\\\\\\\\\\", data_dir=data_path, split=data_split)\\\\\\\\n\\\\\\\\u5c31\\\\\\\\u6b63\\\\\\\\u5e38\\\\\\\\u4e86\\\\\\\\u3002\\\\\\\\n\\\\\\\\n\\\\\\\\u662f\\\\\\\\u6211\\\\\\\\u5b9a\\\\\\\\u4e49\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u7684\\\\\\\\u65b9\\\\\\\\u5f0f\\\\\\\\u4e0d\\\\\\\\u5bf9\\\\\\\\u4e48\\\\\\\\uff1f\\\\\\\\n\\\\\\\\n\\\\\\\\n```bash\\\\\\\\nMODEL_PATH=/home/xxx/models/Qwen2.5-VL-3B-Instruct/  # replace it with your local file path\\\\\\\\n\\\\\\\\npython3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n    config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n    data.train_files=./geometry3k@train \\\\\\\\\\\\\\\\\\\\\\\\n    data.val_files=./geometry3k@test \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.tensor_parallel_size=2 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.experiment_name=XiaomiMiMo_7B_RL_geo_grpo \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.fsdp.torch_dtype=bf16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.optim.strategy=adamw_bf16 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.logger=['console'] \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.n_gpus_per_node=2\\\\\\\\n```\\\\\\\\n\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":21,\\\\\\\"title\\\\\\\":\\\\\\\"v100\\\\\\\\u4e0d\\\\\\\\u517c\\\\\\\\u5bb9flash atten\\\\\\\\uff0c\\\\\\\\u4f7f\\\\\\\\u7528xformer\\\\\\\\u4ecd\\\\\\\\u7136\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u4e4b\\\\\\\\u524d\\\\\\\\u7528H100\\\\\\\\u8dd1\\\\\\\\u901a\\\\\\\\u4e86\\\\\\\\u4ee3\\\\\\\\u7801\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u5207\\\\\\\\u5230V100\\\\\\\\u663e\\\\\\\\u793a\\\\\\\\u4e0d\\\\\\\\u652f\\\\\\\\u6301flash atten2\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u5728\\\\\\\\u65b0\\\\\\\\u7684\\\\\\\\u73af\\\\\\\\u5883\\\\\\\\u4e0b\\\\\\\\u6ca1\\\\\\\\u6709\\\\\\\\u4e0b\\\\\\\\u8f7dflash atten\\\\\\\\uff0c\\\\\\\\u5e76\\\\\\\\u5728\\\\\\\\u914d\\\\\\\\u7f6e\\\\\\\\u6587\\\\\\\\u4ef6\\\\\\\\u4e2d\\\\\\\\u6307\\\\\\\\u5b9aattn_implementation: xformers\\\\\\\\uff0c\\\\\\\\u7981\\\\\\\\u7528\\\\\\\\u6240\\\\\\\\u6709atten\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\\u540e\\\\\\\\u8fd8\\\\\\\\u662f\\\\\\\\u62a5\\\\\\\\u540c\\\\\\\\u6837\\\\\\\\u7684\\\\\\\\u4e0d\\\\\\\\u652f\\\\\\\\u6301flash atten2\\\\\\\\u7684\\\\\\\\u9519\\\\\\\\u8bef\\\\\\\\uff0c\\\\\\\\u611f\\\\\\\\u8c22\\\\\\\\u56de\\\\\\\\u590d\\\\\\\\n raise ImportError(f\\\\\\\\\\\\\\\"{preface} the package flash_attn seems to be not installed. {install_message}\\\\\\\\\\\\\\\")\\\\\\\\n(Runner pid=320551) ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":20,\\\\\\\"title\\\\\\\":\\\\\\\"nnodes>1 Software caused connection abort\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"<img width=\\\\\\\\\\\\\\\"2478\\\\\\\\\\\\\\\" height=\\\\\\\\\\\\\\\"1114\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/14086594-9e11-4b81-b528-f922219dd7c5\\\\\\\\\\\\\\\" />\\\\\\\\nyaml nnodes: 2or3or4 will report above error\\\\\\\\ni have checked ray status, all 4 nodes are settled. i run the bash script in the master node, it seems always try to connect to a node that do not exist in my ray list.\\\\\\\\n\\\\\\\\n<!-- Failed to upload \\\\\\\\\\\\\\\"image.png\\\\\\\\\\\\\\\" -->\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":19,\\\\\\\"title\\\\\\\":\\\\\\\"video-image mixed dataset\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"video or image \\\\\\\\u5355\\\\\\\\u72ec\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u662f\\\\\\\\u6b63\\\\\\\\u5e38\\\\\\\\u7684\\\\\\\\n\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u6df7\\\\\\\\u5408\\\\\\\\u8bad\\\\\\\\u4f1a\\\\\\\\u5361\\\\\\\\u5728Update policy\\\\\\\\n\\\\\\\\n\\\\\\\\u91c7\\\\\\\\u7528\\\\\\\\u7684\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u5f62\\\\\\\\u5f0f\\\\\\\\u662f\\\\\\\\uff1a\\\\\\\\n ```\\\\\\\\nif example['data_type'] == 'image':\\\\\\\\n                data = {\\\\\\\\n                    \\\\\\\\\\\\\\\"problem_type\\\\\\\\\\\\\\\": example['problem_type'],\\\\\\\\n                    \\\\\\\\\\\\\\\"data_source\\\\\\\\\\\\\\\": data_source,\\\\\\\\n                    \\\\\\\\\\\\\\\"problem\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"<image>\\\\\\\\\\\\\\\" + QUESTION_TEMPLATE.format(Question=question) + TYPE_TEMPLATE[example['problem_type']],\\\\\\\\n                    \\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\": answer,\\\\\\\\n                    \\\\\\\\\\\\\\\"images\\\\\\\\\\\\\\\": [video_path],\\\\\\\\n                    \\\\\\\\\\\\\\\"videos\\\\\\\\\\\\\\\": [],\\\\\\\\n                    \\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\": answer_raw\\\\\\\\n                }\\\\\\\\n            else:\\\\\\\\n                data = {\\\\\\\\n                    \\\\\\\\\\\\\\\"problem_type\\\\\\\\\\\\\\\": example['problem_type'],\\\\\\\\n                    \\\\\\\\\\\\\\\"data_source\\\\\\\\\\\\\\\": data_source,\\\\\\\\n                    \\\\\\\\\\\\\\\"problem\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"<video>\\\\\\\\\\\\\\\" + QUESTION_TEMPLATE.format(Question=question) + TYPE_TEMPLATE[example['problem_type']],\\\\\\\\n                    \\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\": answer,\\\\\\\\n                    \\\\\\\\\\\\\\\"images\\\\\\\\\\\\\\\": [],\\\\\\\\n                    \\\\\\\\\\\\\\\"videos\\\\\\\\\\\\\\\": [image_path],\\\\\\\\n                    \\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\": answer_raw\\\\\\\\n                }\\\\\\\\n```\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":18,\\\\\\\"title\\\\\\\":\\\\\\\"During debugging, the program cannot enter the reward function, even though a breakpoint is set.\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi, Thanks for your great work. \\\\\\\\n\\\\\\\\nI want to incorporate my reward function into easyr1. However, the program cannot enter the reward function, even though a breakpoint is set.\\\\\\\\n\\\\\\\\nIt seems that the program can only enter the line without further getting into the reward function. \\\\\\\\n\\\\\\\\nreward_tensor, reward_metrics = ray.get(self.val_reward_fn.compute_reward.remote(test_batch))\\\\\\\\nhttps://github.com/hiyouga/EasyR1/blob/1bdeb1296c120ab9875ce3f4f083eb8b59c829c6/verl/trainer/ray_trainer.py#L420\\\\\\\\n\\\\\\\\nI use the Ray Distributed Debugger following verl's docs. Could you help me to take a check?\\\\\\\\n\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":17,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5e0c\\\\\\\\u671b\\\\\\\\u80fd\\\\\\\\u591f\\\\\\\\u548cLLama factory\\\\\\\\u4e00\\\\\\\\u6837\\\\\\\\u63a8\\\\\\\\u51faweb ui\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5e0c\\\\\\\\u671b\\\\\\\\u80fd\\\\\\\\u591f\\\\\\\\u548cLLama factory\\\\\\\\u4e00\\\\\\\\u6837\\\\\\\\u63a8\\\\\\\\u51faweb ui \\\\\\\\u671f\\\\\\\\u5f85\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u80fd\\\\\\\\u529b\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":16,\\\\\\\"title\\\\\\\":\\\\\\\"Is it possible to support the SGlang Rollout?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Sglang rollout is also a very convenient inference engine. I'd like to ask if you've considered officially supporting the Sglang backend. \\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":15,\\\\\\\"title\\\\\\\":\\\\\\\"Support for 3D MLLM (e.g., PointLLM) Integration\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi team,\\\\\\\\n\\\\\\\\nI'm wondering if EasyR1 currently supports 3D multimodal large language models like PointLLM?\\\\\\\\n\\\\\\\\nIf this feature isn't available yet or not in the roadmap, could you provide some guidance on how to integrate 3D MLLMs with the GRPO algorithm? Any development tips or architectural considerations would be greatly appreciated.\\\\\\\\n\\\\\\\\nThank you for your time and the excellent work on this project!\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":14,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u5728\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u7684\\\\\\\\u65f6\\\\\\\\u5019\\\\\\\\u51bb\\\\\\\\u7ed3\\\\\\\\u8bed\\\\\\\\u8a00\\\\\\\\u6a21\\\\\\\\u5757\\\\\\\\uff0c\\\\\\\\u53ea\\\\\\\\u8bad\\\\\\\\u7ec3vision tower\\\\\\\\uff1f\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u6211\\\\\\\\u5728https://github.com/hiyouga/EasyR1/blob/main/verl/workers/fsdp_workers.py#L239 \\\\\\\\u4e4b\\\\\\\\u540e\\\\\\\\u52a0\\\\\\\\u4e86\\\\\\\\u8fd9\\\\\\\\u6837\\\\\\\\u7684\\\\\\\\u4ee3\\\\\\\\u7801\\\\\\\\uff0c\\\\\\\\u4e0d\\\\\\\\u77e5\\\\\\\\u9053\\\\\\\\u5bf9\\\\\\\\u4e0d\\\\\\\\u5bf9\\\\\\\\uff1f\\\\\\\\n\\\\\\\\n        if model_config.freeze_language_backbone:\\\\\\\\n            model.requires_grad_(False)\\\\\\\\n            \\\\\\\\n            if not model_config.freeze_vision_tower:\\\\\\\\n                 if hasattr(model, \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\") and hasattr(model.model, \\\\\\\\\\\\\\\"visual\\\\\\\\\\\\\\\"):\\\\\\\\n                    model.model.visual.requires_grad_(True)\\\\\\\\n                 elif hasattr(model, \\\\\\\\\\\\\\\"visual\\\\\\\\\\\\\\\"):\\\\\\\\n                    model.visual.requires_grad_(True)\\\\\\\\n\\\\\\\\n            if hasattr(model, \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\") and hasattr(model.model, \\\\\\\\\\\\\\\"mm_projector\\\\\\\\\\\\\\\"):\\\\\\\\n                model.model.mm_projector.requires_grad_(True)\\\\\\\\n            if hasattr(model, \\\\\\\\\\\\\\\"lm_head\\\\\\\\\\\\\\\"):\\\\\\\\n                model.lm_head.requires_grad_(True)\\\\\\\\n            \\\\\\\\n            fsdp_config.use_orig_params = True\\\\\\\\n            self.print_rank0(\\\\\\\\\\\\\\\"Language backbone is frozen. Only specified layers are trainable.\\\\\\\\\\\\\\\")\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":13,\\\\\\\"title\\\\\\\":\\\\\\\"AttributeError: module 'torch.library' has no attribute 'wrap_triton'\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"What is the version of torch, torchvision, and torchaudio?\\\\\\\\n\\\\\\\\nMy version:\\\\\\\\ntorch==2.5.1\\\\\\\\ntorchvision==0.20.1\\\\\\\\ntorchaudio==2.5.1\\\\\\\\n\\\\\\\\nAnd the error: AttributeError: module 'torch.library' has no attribute 'wrap_triton'\\\\\\\\n\\\\\\\\nThanks for help!!\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":12,\\\\\\\"title\\\\\\\":\\\\\\\"core_worker.cc:492: Failed to register worker to Raylet: IOError: [RayletClient] Unable to register worker with raylet. Failed to read data from the socket: End of file worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"When I run `bash examples/qwen2_5_vl_7b_geo3k_grpo.sh`, it raises error `core_worker.cc:492: Failed to register worker to Raylet: IOError: [RayletClient] Unable to register worker with raylet. Failed to read data from the socket: End of file worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff`. \\\\\\\\n\\\\\\\\nThe whole output is:\\\\\\\\n```\\\\\\\\nINFO 07-25 14:15:49 [__init__.py:256] Automatically detected platform cuda.\\\\\\\\n2025-07-25 14:16:34,565 ERROR services.py:1350 -- Failed to start the dashboard \\\\\\\\n2025-07-25 14:16:34,566 ERROR services.py:1375 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure' to find where the log file is.\\\\\\\\n2025-07-25 14:16:34,567 ERROR services.py:1419 -- \\\\\\\\nThe last 20 lines of /root/tmp/ray/session_2025-07-25_14-16-09_760500_504098/logs/dashboard.log (it contains the error message from the dashboard): \\\\\\\\n2025-07-25 14:16:23,921 INFO utils.py:337 -- Available modules: [<class 'ray.dashboard.modules.usage_stats.usage_stats_head.UsageStatsHead'>]\\\\\\\\n2025-07-25 14:16:23,922 INFO head.py:234 -- DashboardHeadModules to load: None.\\\\\\\\n2025-07-25 14:16:23,922 INFO head.py:237 -- Loading DashboardHeadModule: <class 'ray.dashboard.modules.usage_stats.usage_stats_head.UsageStatsHead'>.\\\\\\\\n2025-07-25 14:16:23,922 INFO head.py:241 -- Loaded 1 dashboard head modules: [<ray.dashboard.modules.usage_stats.usage_stats_head.UsageStatsHead object at 0x7fa85d74a2c0>].\\\\\\\\n2025-07-25 14:16:23,922 INFO utils.py:304 -- Get all modules by type: SubprocessModule\\\\\\\\n2025-07-25 14:16:24,019 INFO utils.py:337 -- Available modules: [<class 'ray.dashboard.modules.metrics.metrics_head.MetricsHead'>, <class 'ray.dashboard.modules.data.data_head.DataHead'>, <class 'ray.dashboard.modules.event.event_head.EventHead'>, <class 'ray.dashboard.modules.job.job_head.JobHead'>, <class 'ray.dashboard.modules.node.node_head.NodeHead'>, <class 'ray.dashboard.modules.reporter.reporter_head.ReportHead'>, <class 'ray.dashboard.modules.serve.serve_head.ServeHead'>, <class 'ray.dashboard.modules.state.state_head.StateHead'>, <class 'ray.dashboard.modules.train.train_head.TrainHead'>]\\\\\\\\n2025-07-25 14:16:24,020 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.metrics.metrics_head.MetricsHead'>.\\\\\\\\n2025-07-25 14:16:24,020 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.data.data_head.DataHead'>.\\\\\\\\n2025-07-25 14:16:24,020 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.event.event_head.EventHead'>.\\\\\\\\n2025-07-25 14:16:24,020 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.job.job_head.JobHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.node.node_head.NodeHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.reporter.reporter_head.ReportHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.serve.serve_head.ServeHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.state.state_head.StateHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.train.train_head.TrainHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:295 -- Loaded 9 subprocess modules: [<ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa84a5aba30>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa8521e0fa0>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa8521e0f40>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848070c10>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848073820>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848073f70>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848073fd0>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848072890>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848073f40>].\\\\\\\\n\\\\\\\\n2025-07-25 14:16:37,063 INFO worker.py:1927 -- Started a local Ray instance.\\\\\\\\n[2025-07-25 14:17:08,382 E 504098 504098] core_worker.cc:492: Failed to register worker to Raylet: IOError: [RayletClient] Unable to register worker with raylet. Failed to read data from the socket: End of file worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff\\\\\\\\n```\\\\\\\\n\\\\\\\\nThe version of the packages:\\\\\\\\ntorch 2.6.0\\\\\\\\nray 2.48.0\\\\\\\\nvllm 0.8.0\\\\\\\\ntransformers 4.52.4\\\\\\\\n\\\\\\\\n\\\\\\\\nActually, I met this error under all the environments based on `verl`, but I can't solve it by searching online.\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":11,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u66f4\\\\\\\\u65b0\\\\\\\\u5fae\\\\\\\\u4fe1\\\\\\\\u7fa4\\\\\\\\u4e8c\\\\\\\\u7ef4\\\\\\\\u7801\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u60a8\\\\\\\\u597d\\\\\\\\uff0c\\\\\\\\u5fae\\\\\\\\u4fe1\\\\\\\\u7fa4\\\\\\\\u4e8c\\\\\\\\u7ef4\\\\\\\\u7801\\\\\\\\u8fc7\\\\\\\\u671f\\\\\\\\u4e86\\\\\\\\uff0c\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u66f4\\\\\\\\u65b0\\\\\\\\u4e00\\\\\\\\u4e0b\\\\\\\\u5417\\\\\\\\uff0c\\\\\\\\u8c22\\\\\\\\u8c22\\\\\\\\uff01\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\"},{\\\\\\\"number\\\\\\\":10,\\\\\\\"title\\\\\\\":\\\\\\\"PG loss does not converge\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I attempted to train the Qwen 2.5 7B math model on the Math12K dataset, but the results show severe divergence. In my experiments, I disabled the KL divergence terms since current research suggests they have negligible impact (all other parameters remained unchanged). Has anyone encountered a similar issue?\\\\\\\\n\\\\\\\\n<img width=\\\\\\\\\\\\\\\"1099\\\\\\\\\\\\\\\" height=\\\\\\\\\\\\\\\"324\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/ff7f55b3-908a-4f1d-8ddc-bf5294ccdea6\\\\\\\\\\\\\\\" />\\\\\\\\n\\\\\\\\n<img width=\\\\\\\\\\\\\\\"1111\\\\\\\\\\\\\\\" height=\\\\\\\\\\\\\\\"329\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/109c2c10-266a-4239-a218-89c02d2d7b7c\\\\\\\\\\\\\\\" />\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":9,\\\\\\\"title\\\\\\\":\\\\\\\"Training Memory Leak at various steps\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"While training with a larger batch size, I encounter consistently a OOM issue when the second my train-batch is started, while with lower batch sizes the OOM error happens at various steps between step 3 and 7. What could be the reason for this?\\\\\\\\n\\\\\\\\nI using the config.yaml as basis with the following adjustments on 4 H100s for the lower batch size:\\\\\\\\n\\\\\\\\n#!/bin/bash\\\\\\\\n\\\\\\\\nset -x\\\\\\\\n\\\\\\\\nexport PYTHONUNBUFFERED=1\\\\\\\\n\\\\\\\\nMODEL_PATH=Qwen/Qwen2.5-VL-7B-Instruct\\\\\\\\n\\\\\\\\npython3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n    config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.n_gpus_per_node=4 \\\\\\\\\\\\\\\\\\\\\\\\n    data.rollout_batch_size=128 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.n=20 \\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\n\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":8,\\\\\\\"title\\\\\\\":\\\\\\\"Does GRPO training of VLM support using a Hugging Face model as the rollout-inference model?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Does it support using Hugging Face to load the VLM model for inference during the rollout phase of GRPO training, instead of using the vLLM engine?\\\\\\\\nIs it convenient to make this modification?\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":7,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u8bf7\\\\\\\\u95ee\\\\\\\\u4e0b\\\\\\\\u5956\\\\\\\\u52b1\\\\\\\\u51fd\\\\\\\\u6570\\\\\\\\u80fd\\\\\\\\u4f20\\\\\\\\u5165\\\\\\\\u4e00\\\\\\\\u4e9b\\\\\\\\u5916\\\\\\\\u90e8\\\\\\\\u4fe1\\\\\\\\u606f\\\\\\\\u5417\\\\\\\\uff1f\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5c31\\\\\\\\u7c7b\\\\\\\\u4f3c\\\\\\\\u4e8everl\\\\\\\\u4e2d\\\\\\\\u7684extra_info\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":6,\\\\\\\"title\\\\\\\":\\\\\\\"memory need for training Qwen-2.5VL-instruct-3B?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"as suggested, i use BF16 and training Qwen-2.5VL-instruct-3B on a 48GB L20. To enable bf16 training, I added `torch_dtype: bf16` and `strategy: adamw_bf16` to the `examples/config.yaml` file. However, when I subsequently ran the training command `bash examples/qwen2_5_vl_3b_geo3k_grpo.sh` for the 3B model, I encountered an out-of-memory (OOM) error. I'm using a single L20 GPU with 48GB VRAM. According to the official GitHub project documentation, training the 3B model (BF) should require only 40GB of memory. I'm unclear why this OOM error occurred.\\\\\\\\n\\\\\\\\n<img width=\\\\\\\\\\\\\\\"909\\\\\\\\\\\\\\\" height=\\\\\\\\\\\\\\\"488\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/2665b497-e084-44fb-a7ca-c8ed666d8be6\\\\\\\\\\\\\\\" />\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":5,\\\\\\\"title\\\\\\\":\\\\\\\"Why use qwen2-vl for qwen2.5-vl model?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi! \\\\\\\\nI noticed that your framework supports the training of QWEN2.5-VL, but in dictionary verl/models/transformers, why do you use qwen2-vl for modeling qwen2.5-vl, wouldn't this lead to incompatibility with the 2.5 model function?\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":4,\\\\\\\"title\\\\\\\":\\\\\\\"How to add reward model to compute reward signal?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I want to load an additional reward model (such as CLIP or a smaller-scale model) to calculate rewards for the response. How should I proceed?\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":3,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u76ee\\\\\\\\u524d\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u652f\\\\\\\\u6301\\\\\\\\u89c6\\\\\\\\u9891\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u5417\\\\\\\\uff0c\\\\\\\\u6837\\\\\\\\u4f8b\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u53ea\\\\\\\\u770b\\\\\\\\u5230\\\\\\\\u56fe\\\\\\\\u7247\\\\\\\\u6ca1\\\\\\\\u770b\\\\\\\\u5230\\\\\\\\u89c6\\\\\\\\u9891\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u9898\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":2,\\\\\\\"title\\\\\\\":\\\\\\\"Verl version\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Which version of verl is used in the EasyR1 project?\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"},{\\\\\\\"number\\\\\\\":1,\\\\\\\"title\\\\\\\":\\\\\\\"Support GSPO\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"GSPO seems to be very effective. \\\\\\\\nWhen will GSPO be supported?\\\\\\\",\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\"}],\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Now I need to search through all issues to find ones containing the target keywords. Let me analyze the issues I found. Looking through these issues, I can see several that mention keywords related to memory and GPU issues:\n\n- Issue #9: \"Training Memory Leak at various steps\" - contains OOM and memory\n- Issue #30: \"large rollout.n encounter OOM in training.\" - contains OOM \n- Issue #46: \"[功能请求]为实验一开始的推理阶段增加提示（原标题：新版本的easyr1是取消了val阶段的进度展示吗?)\" - contains \"显存错误\" (GPU memory error)\n\nLet me search for more issues by getting all issues with different states to make sure I capture all:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_ISSUES\", \"action_name\": \"list_for_repo\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\"}\", \"query_params\": \"{\\\"state\\\": \\\"all\\\", \\\"per_page\\\": 100, \\\"page\\\": 1}\", \"include_output_fields\": [\"number\", \"title\", \"body\"]}",
    "call_id": "toolu_01GqZyqUqtrtYW7EctBoNWQ6",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01GqZyqUqtrtYW7EctBoNWQ6",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"list_for_repo\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":[{\\\\\\\"number\\\\\\\":50,\\\\\\\"title\\\\\\\":\\\\\\\"log_prob \\\\\\\\u7684\\\\\\\\u8ba1\\\\\\\\u7b97\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5728 https://github.com/hiyouga/EasyR1/blob/51001bd457f4fbab0055b6a07341c4ac468b61e6/verl/utils/torch_functional.py#L67 \\\\\\\\u4e2d\\\\\\\\uff0c\\\\\\\\u8ba1\\\\\\\\u7b97 log_prob \\\\\\\\u7684\\\\\\\\u51fd\\\\\\\\u6570\\\\\\\\u4e3a\\\\\\\\uff1a\\\\\\\\n```\\\\\\\\ndef log_probs_from_logits(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\\\\\\\\n    ...\\\\\\\\n    if FLAH_ATTN_CROSS_ENTROPY_LOSS_AVAILABLE:\\\\\\\\n        output = log_probs_from_logits_flash_attn(logits, labels)\\\\\\\\n    else:  # fall back to torch kernel, upcast logits to fp32\\\\\\\\n        output = F.cross_entropy(logits.float(), labels, reduction=\\\\\\\\\\\\\\\"none\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    return output.view(*batch_dim)\\\\\\\\n```\\\\\\\\n\\\\\\\\n\\\\\\\\u5f53 if \\\\\\\\u89e6\\\\\\\\u53d1\\\\\\\\u7684\\\\\\\\u65f6\\\\\\\\u5019\\\\\\\\uff0c\\\\\\\\u8f93\\\\\\\\u51fa\\\\\\\\uff1a\\\\\\\\n```\\\\\\\\n@torch.compiler.disable()\\\\\\\\ndef log_probs_from_logits_flash_attn(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\\\\\\\\n    output = cross_entropy_loss(logits, labels, inplace_backward=True)\\\\\\\\n    if not isinstance(output, tuple):\\\\\\\\n        raise ValueError(\\\\\\\\n            \\\\\\\\\\\\\\\"please make sure flash-attn>=2.4.3 where cross_entropy_loss returns Tuple[losses, z_losses].\\\\\\\\\\\\\\\"\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    return -output[0]\\\\\\\\n```\\\\\\\\n\\\\\\\\u5f53else\\\\\\\\u89e6\\\\\\\\u53d1\\\\\\\\u5219\\\\\\\\u8fd4\\\\\\\\u56de\\\\\\\\uff1a\\\\\\\\n```\\\\\\\\noutput = F.cross_entropy(logits.float(), labels, reduction=\\\\\\\\\\\\\\\"none\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    return output.view(*batch_dim)\\\\\\\\n```\\\\\\\\n\\\\\\\\u8fd9\\\\\\\\u91cc\\\\\\\\u7684\\\\\\\\u6b63\\\\\\\\u8d1f\\\\\\\\u53f7\\\\\\\\u597d\\\\\\\\u50cf\\\\\\\\u6709\\\\\\\\u4e00\\\\\\\\u4e9b\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\\uff1fcross_entropy\\\\\\\\u4e00\\\\\\\\u822c\\\\\\\\u5f97\\\\\\\\u5230\\\\\\\\u7684\\\\\\\\u662f loss\\\\\\\\uff0c\\\\\\\\u5e94\\\\\\\\u8be5\\\\\\\\u662f\\\\\\\\u8fd4\\\\\\\\u56de\\\\\\\\u975e\\\\\\\\u8d1f\\\\\\\\u6570\\\\\\\\u7684\\\\\\\\uff1f\\\\\\\"},{\\\\\\\"number\\\\\\\":49,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u8bad\\\\\\\\u7ec3Qwen3-8B\\\\\\\\u65f6\\\\\\\\uff0c\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u603b\\\\\\\\u662f\\\\\\\\u8fc7\\\\\\\\u65e9\\\\\\\\u505c\\\\\\\\u6b62\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5728\\\\\\\\u8bad\\\\\\\\u7ec3Qwen3-8B\\\\\\\\u65f6\\\\\\\\uff0c\\\\\\\\u9884\\\\\\\\u8ba1\\\\\\\\u8bad\\\\\\\\u7ec3step\\\\\\\\u67091510\\\\\\\\u4e2a\\\\\\\\uff0c\\\\\\\\u5982\\\\\\\\u4e0b\\\\\\\\u56fe\\\\\\\\u6240\\\\\\\\u793a\\\\\\\\n\\\\\\\\n![Image](https://github.com/user-attachments/assets/7c866b9d-d20e-423d-81a5-013425325ab4)\\\\\\\\n\\\\\\\\n\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u5b9e\\\\\\\\u9645\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u572820\\\\\\\\u591a\\\\\\\\u4e2astep\\\\\\\\u5c31\\\\\\\\u505c\\\\\\\\u6b62\\\\\\\\u4e86\\\\\\\\uff0c\\\\\\\\u65e5\\\\\\\\u5fd7\\\\\\\\u505c\\\\\\\\u5728\\\\\\\\u8fd9\\\\\\\\u91cc\\\\\\\\n\\\\\\\\n![Image](https://github.com/user-attachments/assets/d8fd060c-f882-4114-aef4-0524ffd32a4c)\\\\\\\\n\\\\\\\\nwandb\\\\\\\\u4e0a\\\\\\\\u7684\\\\\\\\u56fe\\\\\\\\u50cf\\\\\\\\u4e5f\\\\\\\\u662f\\\\\\\\u505c\\\\\\\\u7559\\\\\\\\u572820\\\\\\\\u591a\\\\\\\\u4e2astep\\\\\\\\n\\\\\\\\n![Image](https://github.com/user-attachments/assets/d7454b0a-c9a3-46e5-a591-0d2b29569563)\\\\\\\"},{\\\\\\\"number\\\\\\\":48,\\\\\\\"title\\\\\\\":\\\\\\\"Exception ignored in sys.unraisablehook\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I try the demo by running 'bash examples/qwen2_5_vl_7b_geo3k_grpo.sh'.\\\\\\\\nAn exception happens when the training process is finished.\\\\\\\\nI wonder is there anything wrong and how can I correct it?\\\\\\\\nThe following figure is the exception:\\\\\\\\n\\\\\\\\n<img width=\\\\\\\\\\\\\\\"849\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/b64abc66-bdce-47ed-9bd1-bb7249eab076\\\\\\\\\\\\\\\" />\\\\\\\"},{\\\\\\\"number\\\\\\\":47,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u600e\\\\\\\\u4e48\\\\\\\\u52a0\\\\\\\\u901fFiltering overlong prompts\\\\\\\\uff1f\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"![Image](https://github.com/user-attachments/assets/d39062f0-d8db-4468-93c6-96bc7b84b90b)\\\\\\\\n\\\\\\\\u5982\\\\\\\\u56fe\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u8fdb\\\\\\\\u884cFiltering overlong prompts\\\\\\\\u8981\\\\\\\\u8017\\\\\\\\u8d391\\\\\\\\u4e2a\\\\\\\\u5c0f\\\\\\\\u65f6\\\\\\\\u4ee5\\\\\\\\u4e0a\\\\\\\\uff0c\\\\\\\\u800c\\\\\\\\u4e14easyr1\\\\\\\\u597d\\\\\\\\u50cf\\\\\\\\u6ca1\\\\\\\\u6709filter_overlong_prompts_workers\\\\\\\\u7684\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\\uff0c\\\\\\\\u90a3\\\\\\\\u6211\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u52a0\\\\\\\\u901f\\\\\\\\u8fd9\\\\\\\\u4e00\\\\\\\\u8fdb\\\\\\\\u7a0b\\\\\\\\u5462\\\\\\\\uff1f\\\\\\\"},{\\\\\\\"number\\\\\\\":46,\\\\\\\"title\\\\\\\":\\\\\\\"[\\\\\\\\u529f\\\\\\\\u80fd\\\\\\\\u8bf7\\\\\\\\u6c42]\\\\\\\\u4e3a\\\\\\\\u5b9e\\\\\\\\u9a8c\\\\\\\\u4e00\\\\\\\\u5f00\\\\\\\\u59cb\\\\\\\\u7684\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u9636\\\\\\\\u6bb5\\\\\\\\u589e\\\\\\\\u52a0\\\\\\\\u63d0\\\\\\\\u793a\\\\\\\\uff08\\\\\\\\u539f\\\\\\\\u6807\\\\\\\\u9898\\\\\\\\uff1a\\\\\\\\u65b0\\\\\\\\u7248\\\\\\\\u672c\\\\\\\\u7684easyr1\\\\\\\\u662f\\\\\\\\u53d6\\\\\\\\u6d88\\\\\\\\u4e86val\\\\\\\\u9636\\\\\\\\u6bb5\\\\\\\\u7684\\\\\\\\u8fdb\\\\\\\\u5ea6\\\\\\\\u5c55\\\\\\\\u793a\\\\\\\\u5417\\\\\\\\uff1f)\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"![Image](https://github.com/user-attachments/assets/8d7c2de2-9b23-45dd-a207-6bfa351ab40e)\\\\\\\\n\\\\\\\\u663e\\\\\\\\u793aStart validation...\\\\\\\\u540e\\\\\\\\u7ec8\\\\\\\\u7aef\\\\\\\\u4e00\\\\\\\\u76f4\\\\\\\\u6ca1\\\\\\\\u6709\\\\\\\\u8f93\\\\\\\\u51fa\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u5361\\\\\\\\u662f\\\\\\\\u4e00\\\\\\\\u76f4\\\\\\\\u518d\\\\\\\\u8dd1\\\\\\\\u7684\\\\\\\\uff0c\\\\\\\\u76f4\\\\\\\\u5230\\\\\\\\u7206\\\\\\\\u51fa\\\\\\\\u4e86\\\\\\\\u663e\\\\\\\\u5b58\\\\\\\\u9519\\\\\\\\u8bef\\\\\\\\u3002\\\\\\\\n\\\\\\\\u8bf7\\\\\\\\u95ee\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u9636\\\\\\\\u6bb5\\\\\\\\u662f\\\\\\\\u5728\\\\\\\\u4e00\\\\\\\\u76f4\\\\\\\\u8fdb\\\\\\\\u884c\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u5417\\\\\\\\uff1f\\\\\\\\u6211\\\\\\\\u8bb0\\\\\\\\u5f97\\\\\\\\u4e4b\\\\\\\\u524d\\\\\\\\u7684\\\\\\\\u7248\\\\\\\\u672c\\\\\\\\u662f\\\\\\\\u4f1a\\\\\\\\u663e\\\\\\\\u793a\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u8fdb\\\\\\\\u5ea6\\\\\\\\u7684\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u4eca\\\\\\\\u5929\\\\\\\\u62c9\\\\\\\\u4e86\\\\\\\\u6700\\\\\\\\u65b0\\\\\\\\u5206\\\\\\\\u652f\\\\\\\\u540e\\\\\\\\u51fa\\\\\\\\u73b0\\\\\\\\u4e86\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\"},{\\\\\\\"number\\\\\\\":45,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5fae\\\\\\\\u4fe1\\\\\\\\u7fa4\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u66f4\\\\\\\\u65b0\\\\\\\\u4e00\\\\\\\\u4e0b\\\\\\\\u5fae\\\\\\\\u4fe1\\\\\\\\u7fa4\\\\\\\\u7684\\\\\\\\u540d\\\\\\\\u7247\\\\\\\\uff0c\\\\\\\\u539f\\\\\\\\u6765\\\\\\\\u7684\\\\\\\\u8fc7\\\\\\\\u671f\\\\\\\\u4e86\\\\\\\\u8c22\\\\\\\\u8c22\\\\\\\\uff01\\\\\\\"},{\\\\\\\"number\\\\\\\":44,\\\\\\\"title\\\\\\\":\\\\\\\"Issue : Consider Adding Type Hints for Function Arguments in `protocol.py`\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Adding type hints to the function arguments in `protocol.py` can improve code readability and maintainability. While some arguments already have type hints, extending this to all arguments would provide a more consistent and comprehensive benefit. For example, add type hints to arguments such as `features` in `batch_collate` function or `data_items` in `collate_fn` function to improve type safety and make the function signature more explicit.\\\\\\\"},{\\\\\\\"number\\\\\\\":43,\\\\\\\"title\\\\\\\":\\\\\\\"How to disable flash-attn?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I enocutered a problem:\\\\\\\\n\\\\\\\\nImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2\\\\\\\\n\\\\\\\\nI was able to run it successfully without this package before, but now I don't know why it doesn't work anymore. Can some body give me some solution?\\\\\\\\n\\\\\\\"},{\\\\\\\"number\\\\\\\":42,\\\\\\\"title\\\\\\\":\\\\\\\"DAPO\\\\\\\\u7f13\\\\\\\\u6162\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u60a8\\\\\\\\u597d\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u60f3\\\\\\\\u8bf7\\\\\\\\u6559\\\\\\\\u4e00\\\\\\\\u4e0b\\\\\\\\uff0c\\\\\\\\u4f7f\\\\\\\\u7528\\\\\\\\u539fGRPO\\\\\\\\u65f6\\\\\\\\u4e00\\\\\\\\u4e2astep\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u5f88\\\\\\\\u5feb\\\\\\\\uff0c\\\\\\\\u6539\\\\\\\\u4e3aDAPO\\\\\\\\u540e\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u81f3\\\\\\\\u5c11\\\\\\\\u6162\\\\\\\\u4e8610\\\\\\\\u500d\\\\\\\\u4ee5\\\\\\\\u4e0a\\\\\\\\u3002\\\\\\\"},{\\\\\\\"number\\\\\\\":41,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5173\\\\\\\\u4e8ecuda 12.1\\\\\\\\u652f\\\\\\\\u6301\\\\\\\\u7684docker\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u60a8\\\\\\\\u597d\\\\\\\\uff01\\\\\\\\u76ee\\\\\\\\u524d\\\\\\\\u516c\\\\\\\\u5f00\\\\\\\\u7684docker\\\\\\\\u955c\\\\\\\\u50cf\\\\\\\\u4f3c\\\\\\\\u4e4e\\\\\\\\u5bf9\\\\\\\\u5e94\\\\\\\\u7684\\\\\\\\u662fcuda12.6\\\\\\\\uff0c\\\\\\\\u7136\\\\\\\\u800c\\\\\\\\u6211\\\\\\\\u7684\\\\\\\\u670d\\\\\\\\u52a1\\\\\\\\u5668\\\\\\\\u4e0a\\\\\\\\u53ea\\\\\\\\u652f\\\\\\\\u6301cuda 12.1\\\\\\\\uff0c\\\\\\\\u8bf7\\\\\\\\u95ee\\\\\\\\u6709\\\\\\\\u6ca1\\\\\\\\u6709\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u62c9\\\\\\\\u53d6\\\\\\\\u7684docker\\\\\\\\u955c\\\\\\\\u50cf\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u4f7f\\\\\\\\u7528\\\\\\\\u5462\\\\\\\\uff1f\\\\\\\\u5341\\\\\\\\u5206\\\\\\\\u611f\\\\\\\\u8c22\\\\\\\"},{\\\\\\\"number\\\\\\\":40,\\\\\\\"title\\\\\\\":\\\\\\\"Custom configuration for reward function\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi, is it possible to have some configurable hyperparameters for reward function, especially some custom configurations?\\\\\\\\n\\\\\\\\nIn the current codebase, it seems even format_weight cannot be configured\\\\\\\\n\\\\\\\\nIn my experiments I want to combine two different rewards, and I want to tune the combination weight between them, but I can't see how to configure that, especially if I don't want to modify RewardConfig in the source code (as they have fixed fields)\\\\\\\"},{\\\\\\\"number\\\\\\\":39,\\\\\\\"title\\\\\\\":\\\\\\\"GRPO\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u7528Qwen2.5-VL-7B\\\\\\\\u6a21\\\\\\\\u578b\\\\\\\\u8bad\\\\\\\\u7ec3GRPO\\\\\\\\uff0c\\\\\\\\u6bcf\\\\\\\\u6761\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u7684\\\\\\\\u8f93\\\\\\\\u5165\\\\\\\\u4e3a8\\\\\\\\u5f20336*336\\\\\\\\u7684\\\\\\\\u56fe\\\\\\\\u7247\\\\\\\\uff0c\\\\\\\\u52a0\\\\\\\\u4e00\\\\\\\\u6bb5\\\\\\\\u957f\\\\\\\\u5ea6\\\\\\\\u4e0d\\\\\\\\u8d85\\\\\\\\u8fc71000\\\\\\\\u7684\\\\\\\\u6587\\\\\\\\u672c\\\\\\\\u30028\\\\\\\\u5f20A100 80G\\\\\\\\uff0c800\\\\\\\\u6761\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u6570\\\\\\\\u636e10\\\\\\\\u4e2aepoch\\\\\\\\uff0c\\\\\\\\u663e\\\\\\\\u793a\\\\\\\\u9700\\\\\\\\u898126\\\\\\\\uff5e30\\\\\\\\u4e2a\\\\\\\\u5c0f\\\\\\\\u65f6\\\\\\\\uff0c\\\\\\\\u8bf7\\\\\\\\u95ee\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u6b63\\\\\\\\u5e38\\\\\\\\u5417\\\\\\\\uff1f\\\\\\\\u662f\\\\\\\\u5426\\\\\\\\u6709\\\\\\\\u63d0\\\\\\\\u5347\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u7684\\\\\\\\u65b9\\\\\\\\u6848\\\\\\\\uff1f\\\\\\\\uff08\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u8fc7\\\\\\\\u7a0b\\\\\\\\u4e2d\\\\\\\\u51fa\\\\\\\\u73b0GPU\\\\\\\\u5229\\\\\\\\u7528\\\\\\\\u7387\\\\\\\\u5f88\\\\\\\\u4f4e\\\\\\\\uff0820%\\\\\\\\u5de6\\\\\\\\u53f3\\\\\\\\uff09\\\\\\\\uff0cCPU\\\\\\\\u5229\\\\\\\\u7528\\\\\\\\u7387\\\\\\\\u5f88\\\\\\\\u9ad8\\\\\\\\u7684\\\\\\\\u60c5\\\\\\\\u51b5\\\\\\\\uff09\\\\\\\\u671f\\\\\\\\u5f85\\\\\\\\u60a8\\\\\\\\u7684\\\\\\\\u56de\\\\\\\\u7b54\\\\\\\\uff0c\\\\\\\\u975e\\\\\\\\u5e38\\\\\\\\u611f\\\\\\\\u8c22\\\\\\\\uff01\\\\\\\\n\\\\\\\\n\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\\u8bbe\\\\\\\\u7f6e\\\\\\\\u5982\\\\\\\\u4e0b\\\\\\\\uff1a\\\\\\\\npython3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n    config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n    data.train_files=${TRAIN_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    data.val_files=${VAL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    data.rollout_batch_size=16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.global_batch_size=16 \\\\\\\\\\\\\\\\\\\\\\\\n    data.val_batch_size=16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.limit_images=8 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.experiment_name=${EXP_NAME} \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.n_gpus_per_node=8 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.save_checkpoint_path=${CKPT_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.n=8 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.logger=['console','tensorboard'] \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.micro_batch_size_per_device_for_experience=16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.micro_batch_size_per_device_for_update=4 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.tensor_parallel_size=2 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.gpu_memory_utilization=0.6 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.reward.reward_type=sequential \\\\\\\\\\\\\\\\\\\\\\\\n    worker.reward.reward_function=./examples/reward_function/r1v.py:compute_score \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_prompt_length=5120 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.max_num_batched_tokens=7168 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.val_freq=50 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.save_freq=50 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.save_limit=15 \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_pixels=112896 \\\\\\\\\\\\\\\\\\\\\\\\n    data.min_pixels=112896 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.total_epochs=10 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.val_before_train=false \\\\\\\\\\\\\\\\\\\\\\\"},{\\\\\\\"number\\\\\\\":38,\\\\\\\"title\\\\\\\":\\\\\\\"Possible Typo in fsdp_workers.py:477 for Video Inputs\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi @hiyouga ,\\\\\\\\n\\\\\\\\nhttps://github.com/hiyouga/EasyR1/blob/c19396ea16edc383cb25fedb3da58ae08e08046e/verl/workers/fsdp_workers.py#L477\\\\\\\\n\\\\\\\\nShould this line be updated to:\\\\\\\\nself.processor.image_processor(images=None, videos=video**s**, return_tensors=\\\\\\\\\\\\\\\"pt\\\\\\\\\\\\\\\")?\\\\\\\\n\\\\\\\\nThanks!\\\\\\\"},{\\\\\\\"number\\\\\\\":37,\\\\\\\"title\\\\\\\":\\\\\\\"When training with video (T, C, H, W), ValueError: expected to be in states [<TrainingState.FORWARD_BACKWARD: 2>] but current state is TrainingState.IDLE\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"```\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"<frozen runpy>\\\\\\\\\\\\\\\", line 198, in _run_module_as_main\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"<frozen runpy>\\\\\\\\\\\\\\\", line 88, in _run_code\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/trainer/main.py\\\\\\\\\\\\\\\", line 128, in <module>\\\\\\\\n2025-06-26 03:30:57     main()\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/trainer/main.py\\\\\\\\\\\\\\\", line 124, in main\\\\\\\\n2025-06-26 03:30:57     ray.get(runner.run.remote(ppo_config))\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 22, in auto_init_wrapper\\\\\\\\n2025-06-26 03:30:57     return fn(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 104, in wrapper\\\\\\\\n2025-06-26 03:30:57     return func(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2849, in get\\\\\\\\n2025-06-26 03:30:57     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n2025-06-26 03:30:57                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 937, in get_objects\\\\\\\\n2025-06-26 03:30:57     raise value.as_instanceof_cause()\\\\\\\\n2025-06-26 03:30:57 ray.exceptions.RayTaskError(ValueError): \\\\\\\\u001b[36mray::Runner.run()\\\\\\\\u001b[39m (pid=12325, ip=33.41.79.235, actor_id=f68735385a682039044e373a01000000, repr=<main.Runner object at 0x7f79df2da4d0>)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/trainer/main.py\\\\\\\\\\\\\\\", line 94, in run\\\\\\\\n2025-06-26 03:30:57     trainer.fit()\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 639, in fit\\\\\\\\n2025-06-26 03:30:57     actor_output = self.actor_rollout_ref_wg.update_actor(batch)\\\\\\\\n2025-06-26 03:30:57                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n2025-06-26 03:30:57     output = ray.get(output)\\\\\\\\n2025-06-26 03:30:57              ^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57 ray.exceptions.RayTaskError(ValueError): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_ref_update_actor()\\\\\\\\u001b[39m (pid=596, ip=33.41.91.191, actor_id=f85afa3102d77bfd6615e86d01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f365e12f790>)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-06-26 03:30:57     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-06-26 03:30:57     return func(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 506, in update_actor\\\\\\\\n2025-06-26 03:30:57     metrics = self.actor.update_policy(data=data)\\\\\\\\n2025-06-26 03:30:57               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/workers/actor/dp_actor.py\\\\\\\\\\\\\\\", line 271, in update_policy\\\\\\\\n2025-06-26 03:30:57     loss.backward()\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/_tensor.py\\\\\\\\\\\\\\\", line 648, in backward\\\\\\\\n2025-06-26 03:30:57     torch.autograd.backward(\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/autograd/__init__.py\\\\\\\\\\\\\\\", line 353, in backward\\\\\\\\n2025-06-26 03:30:57     _engine_run_backward(\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/autograd/graph.py\\\\\\\\\\\\\\\", line 824, in _engine_run_backward\\\\\\\\n2025-06-26 03:30:57     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context\\\\\\\\n2025-06-26 03:30:57     return func(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/distributed/fsdp/_runtime_utils.py\\\\\\\\\\\\\\\", line 721, in _post_backward_hook\\\\\\\\n2025-06-26 03:30:57     _assert_in_training_states(state, [TrainingState.FORWARD_BACKWARD])\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/distributed/fsdp/_common_utils.py\\\\\\\\\\\\\\\", line 450, in _assert_in_training_states\\\\\\\\n2025-06-26 03:30:57     raise ValueError(msg)\\\\\\\\n2025-06-26 03:30:57 ValueError: expected to be in states [<TrainingState.FORWARD_BACKWARD: 2>] but current state is TrainingState.IDLE\\\\\\\\n```\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nvllm==0.9.1\\\\\\\\ntorch==2.7.0\\\\\\\\ntransformers==4.51.3\\\\\\\"},{\\\\\\\"number\\\\\\\":36,\\\\\\\"title\\\\\\\":\\\\\\\"The training time becomes slower and slower\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Thank you for your great work. When I attempted to train the multimodal model, the training time became significantly longer, and I found that the process stays at the initialization stage for a long time, showing as ray::WorkerDict. At this point:\\\\\\\\nnvidia-smi shows that the GPU memory is occupied, but the GPU utilization rate is 0%.\\\\\\\\nThe program does not report any errors, but there is no progress for a long time.\\\\\\\\n\\\\\\\\nCan you offer me the solutions for this problem, thank you very much.\\\\\\\"},{\\\\\\\"number\\\\\\\":35,\\\\\\\"title\\\\\\\":\\\\\\\"Training crashes before Computing Log Prob of First step with worker.rollout.temperature>1.0\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Every time I run a training with a worker.rollout.temperature higher than 1.0 the training process crashes in the first step of the training process, is this intended in this way?\\\\\\\\n\\\\\\\\nMany thanks for your work!\\\\\\\"},{\\\\\\\"number\\\\\\\":34,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u6d4b\\\\\\\\u8bd5\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u4f7f\\\\\\\\u7528Qwen2.5VL\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u4e86\\\\\\\\u81ea\\\\\\\\u5df1\\\\\\\\u7684\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\uff0c\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u8fdb\\\\\\\\u884c\\\\\\\\u6d4b\\\\\\\\u8bd5\\\\\\\\uff1f\\\\\\\"},{\\\\\\\"number\\\\\\\":33,\\\\\\\"title\\\\\\\":\\\\\\\"How to imply a rule-based verification function for different tasks during RL?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Thank you for your excellent work!\\\\\\\\n\\\\\\\\nI hava ten different tasks, and I want imply a rule-based verification function for each task during RL training. How are these reward score calculations integrated and utilized within easy-R1?\\\\\\\\n\\\\\\\\nLooking forward to your reply!\\\\\\\\n\\\\\\\\nYours\\\\\\\\n\\\\\\\\nXiaozhe Li\\\\\\\"},{\\\\\\\"number\\\\\\\":32,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u4e4b\\\\\\\\u524d\\\\\\\\u7684\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u8fc7\\\\\\\\u7a0b\\\\\\\\u5361\\\\\\\\u4f4f\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u4e4b\\\\\\\\u524d\\\\\\\\u90fd\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u6b63\\\\\\\\u5e38\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u7684\\\\\\\\uff0c\\\\\\\\u73b0\\\\\\\\u5728\\\\\\\\u4f1a\\\\\\\\u5361\\\\\\\\u5728\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u9636\\\\\\\\u6bb5\\\\\\\\uff0c\\\\\\\\u663e\\\\\\\\u5b58\\\\\\\\u5360\\\\\\\\u7528\\\\\\\\u4f4e\\\\\\\\uff0c\\\\\\\\u5229\\\\\\\\u7528\\\\\\\\u7387\\\\\\\\u4e3a0\\\\\\\\uff0c\\\\\\\\u4e00\\\\\\\\u76f4\\\\\\\\u5361\\\\\\\\u4f4f\\\\\\\\u3002\\\\\\\\n\\\\\\\\n(WorkerDict pid=1768099) NCCL version 2.21.5+cuda12.4\\\\\\\\nProcessed prompts:   0%|          | 0/60 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\\\\\\\\nProcessed prompts:   2%|\\\\\\\\u258f         | 1/60 [00:05<05:39,  5.75s/it, est. speed input: 82.26 toks/s, output: 14.09 toks/s]\\\\\\\\nProcessed prompts:   7%|\\\\\\\\u258b         | 4/60 [00:06<01:05,  1.17s/it, est. speed input: 312.30 toks/s, output: 56.78 toks/s]\\\\\\\\nProcessed prompts:  10%|\\\\\\\\u2588         | 6/60 [00:06<00:37,  1.45it/s, est. speed input: 457.89 toks/s, output: 86.64 toks/s]\\\\\\\\nProcessed prompts:  17%|\\\\\\\\u2588\\\\\\\\u258b        | 10/60 [00:06<00:17,  2.94it/s, est. speed input: 735.35 toks/s, output: 146.91 toks/s]\\\\\\\\nProcessed prompts:  20%|\\\\\\\\u2588\\\\\\\\u2588        | 12/60 [00:06<00:12,  3.71it/s, est. speed input: 858.36 toks/s, output: 177.54 toks/s]\\\\\\\\nProcessed prompts:  27%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b       | 16/60 [00:06<00:07,  6.12it/s, est. speed input: 1122.34 toks/s, output: 244.84 toks/s]\\\\\\\\nProcessed prompts:  30%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588       | 18/60 [00:06<00:05,  7.17it/s, est. speed input: 1238.74 toks/s, output: 276.87 toks/s]\\\\\\\\nProcessed prompts:  38%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258a      | 23/60 [00:06<00:03, 11.79it/s, est. speed input: 1558.40 toks/s, output: 366.72 toks/s]\\\\\\\\nProcessed prompts:  43%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258e     | 26/60 [00:07<00:02, 13.39it/s, est. speed input: 1725.24 toks/s, output: 417.77 toks/s]\\\\\\\\nProcessed prompts:  48%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258a     | 29/60 [00:07<00:02, 12.66it/s, est. speed input: 1854.35 toks/s, output: 464.50 toks/s]\\\\\\\\nProcessed prompts:  53%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258e    | 32/60 [00:07<00:01, 14.94it/s, est. speed input: 2015.29 toks/s, output: 521.66 toks/s]\\\\\\\\nProcessed prompts:  60%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588    | 36/60 [00:07<00:01, 18.33it/s, est. speed input: 2228.90 toks/s, output: 597.67 toks/s]\\\\\\\\nProcessed prompts:  65%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258c   | 39/60 [00:07<00:01, 18.39it/s, est. speed input: 2364.58 toks/s, output: 651.93 toks/s]\\\\\\\\nProcessed prompts:  72%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258f  | 43/60 [00:07<00:00, 22.35it/s, est. speed input: 2572.17 toks/s, output: 734.00 toks/s]\\\\\\\\nProcessed prompts:  77%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b  | 46/60 [00:08<00:00, 17.34it/s, est. speed input: 2658.27 toks/s, output: 779.23 toks/s]\\\\\\\\nProcessed prompts:  82%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258f | 49/60 [00:08<00:00, 16.99it/s, est. speed input: 2768.72 toks/s, output: 835.26 toks/s]\\\\\\\\nProcessed prompts:  88%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258a | 53/60 [00:08<00:00, 20.02it/s, est. speed input: 2948.11 toks/s, output: 922.69 toks/s]\\\\\\\\nProcessed prompts:  93%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258e| 56/60 [00:08<00:00, 12.96it/s, est. speed input: 2957.91 toks/s, output: 952.76 toks/s]\\\\\\\\nProcessed prompts:  97%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b| 58/60 [00:09<00:00, 10.23it/s, est. speed input: 2946.59 toks/s, output: 971.81 toks/s]\\\\\\\\n(WorkerDict pid=1765528) Before vllm offload in sharding manager: 44.16 GB / 79.14 GB.\\\\\\\\n(WorkerDict pid=1765528) INFO 06-27 23:52:18 [block_pool.py:264] Successfully reset prefix cache\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:51:58 [executor_base.py:226] It took 0.711307 seconds to wake up tags ['weights']. [repeated 2x across\\\\\\\\n cluster]\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:51:59 [executor_base.py:226] It took 0.030391 seconds to wake up tags ['kv_cache']. [repeated 2x acros\\\\\\\\ns cluster]\\\\\\\\n(WorkerDict pid=1768098) NCCL version 2.21.5+cuda12.4\\\\\\\\nProcessed prompts: 100%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588| 60/60 [00:09<00:00,  6.06it/s, est. speed input: 2868.58 toks/s, output: 974.38 toks/s]\\\\\\\\n(WorkerDict pid=1765528) INFO 06-27 23:52:19 [gpu_worker.py:81] Sleep mode freed 39.72 GiB memory, 4.44 GiB memory is still in use.\\\\\\\\n(WorkerDict pid=1765528) INFO 06-27 23:52:19 [executor_base.py:210] It took 1.040433 seconds to fall asleep.\\\\\\\\n(WorkerDict pid=1765528) After vllm offload in sharding manager: 4.44 GB / 79.14 GB.\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:52:25 [block_pool.py:264] Successfully reset prefix cache\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:52:27 [gpu_worker.py:81] Sleep mode freed 39.72 GiB memory, 1.77 GiB memory is still in use.\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:52:27 [executor_base.py:210] It took 1.032600 seconds to fall asleep.\\\\\\\\n(WorkerDict pid=1768099) INFO 06-27 23:52:36 [block_pool.py:264] Successfully reset prefix cache\\\\\\\\n(Runner pid=1764900) validation generation end\\\\\\\\n(WorkerDict pid=1768099) INFO 06-27 23:52:37 [gpu_worker.py:81] Sleep mode freed 39.77 GiB memory, 4.54 GiB memory is still in use.\\\\\\\\n(WorkerDict pid=1768099) INFO 06-27 23:52:37 [executor_base.py:210] It took 1.036531 seconds to fall asleep.\\\\\\\\n(Runner pid=1764900) answer: C\\\\\\\\n(Runner pid=1764900) ground_truth: C\\\\\\\\n\\\\\\\"},{\\\\\\\"number\\\\\\\":31,\\\\\\\"title\\\\\\\":\\\\\\\"IndexError: list index out of range from rotary_embedding.py when using vLLM v1\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I can run the example script successfully, but when running on a custimized dataset, and got error in rollout process\\\\\\\\n\\\\\\\\n```\\\\\\\\n(Runner pid=82786)     output = self.rollout.generate_sequences(prompts=prompts)                                                                                                                                                                                                                              \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                              \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context                                                                                                                                                                 \\\\\\\\n(Runner pid=82786)     return func(*args, **kwargs)                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 192, in generate_sequences                                                                                                                                                                                       \\\\\\\\n(Runner pid=82786)     completions: List[RequestOutput] = self.inference_engine.generate(                                                                                                                                                                                                                     \\\\\\\\n(Runner pid=82786)                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                     \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1131, in inner                                                                                                                                                                                        \\\\\\\\n(Runner pid=82786)     return fn(*args, **kwargs)                                                                                                                                                                                                                                                             \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                             \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 465, in generate                                                                                                                                                                            \\\\\\\\n(Runner pid=82786)     outputs = self._run_engine(use_tqdm=use_tqdm)                                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 1384, in _run_engine                                                                                                                                                                        \\\\\\\\n(Runner pid=82786)     step_outputs = self.llm_engine.step()                                                                                                                                                                                                                                                  \\\\\\\\n(Runner pid=82786)                    ^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                  \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 222, in step                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)     outputs = self.engine_core.get_output()                                                                                                                                                                                                                                                \\\\\\\\n(Runner pid=82786)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 200, in get_output                                                                                                                                                                    \\\\\\\\n(Runner pid=82786)     return self.engine_core.step()                                                                                                                                                                                                                                                         \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                         \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 206, in step                                                                                                                                                                                 \\\\\\\\n(Runner pid=82786)     output = self.model_executor.execute_model(scheduler_output)                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\\\\\\\\\\\\\\\", line 77, in execute_model                                                                                                                                                                   \\\\\\\\n(Runner pid=82786)     output = self.collective_rpc(\\\\\\\\\\\\\\\"execute_model\\\\\\\\\\\\\\\",                                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc                                                                                                                                                             \\\\\\\\n(Runner pid=82786)     answer = run_method(self.driver_worker, method, args, kwargs)                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2347, in run_method                                                                                                                                                                                   \\\\\\\\n(Runner pid=82786)     return func(*args, **kwargs)                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context                                                                                                                                                                 \\\\\\\\n(Runner pid=82786)     return func(*args, **kwargs)                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 242, in execute_model                                                                                                                                                                  \\\\\\\\n(Runner pid=82786)     output = self.model_runner.execute_model(scheduler_output)                                                                                                                                                                                                                             \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                             \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context                                                                                                                                                                 \\\\\\\\n(Runner pid=82786)     return func(*args, **kwargs)                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\\\\\\\\\\\\\\\", line 980, in execute_model                                                                                                                                                            \\\\\\\\n(Runner pid=82786)     self._update_states(scheduler_output)                                                                                                                                                                                                                                                  \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\\\\\\\\\\\\\\\", line 369, in _update_states                                                                                                                                                           \\\\\\\\n(Runner pid=82786)     MRotaryEmbedding.get_input_positions_tensor(                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/model_executor/layers/rotary_embedding.py\\\\\\\\\\\\\\\", line 1069, in get_input_positions_tensor                                                                                                                                  \\\\\\\\n(Runner pid=82786)     image_grid_thw[image_index][0],                                                                                                                                                                                                                                                        \\\\\\\\n(Runner pid=82786)     ~~~~~~~~~~~~~~^^^^^^^^^^^^^                                                                                                                                                                                                                                                            \\\\\\\\n(Runner pid=82786) IndexError: list index out of range\\\\\\\\n```\\\\\\\\n\\\\\\\\nThe errors only exist when using VLLM v1, and when using `VLLM_USE_V1=0` the code can run, but the results are totally wrong, seems the model didn't receive the correct images in this case. Wondering why this happends? \\\\\\\"},{\\\\\\\"number\\\\\\\":30,\\\\\\\"title\\\\\\\":\\\\\\\"large rollout.n encounter OOM in training.\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi, I was training using 4 nodes on a 7B qwen2.5vl model, I tried to enlarge the rollout number, but got OOM error. I wonder how to correctly set the training parameters, especially the batch size in multinode training setup? Here's what i'm using now\\\\\\\\n\\\\\\\\n```\\\\\\\\n  python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n    config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.logger=['console','wandb'] \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_prompt_length=2048 \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_response_length=1024 \\\\\\\\\\\\\\\\\\\\\\\\n    data.rollout_batch_size=512 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.global_batch_size=128 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.micro_batch_size_per_device_for_update=4 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.micro_batch_size_per_device_for_experience=16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.n=128 \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_pixels=1258291 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.save_freq=5 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.n_gpus_per_node=8 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.nnodes=4 \\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\n```\\\\\\\"},{\\\\\\\"number\\\\\\\":29,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u6784\\\\\\\\u5efa\\\\\\\\u81ea\\\\\\\\u5df1\\\\\\\\u7684\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u7528\\\\\\\\u4e8eGRPO\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u679c\\\\\\\\u60f3\\\\\\\\u8981\\\\\\\\u81ea\\\\\\\\u5df1\\\\\\\\u6784\\\\\\\\u5efaVQA\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\uff0c\\\\\\\\u5e94\\\\\\\\u5f53\\\\\\\\u4ee5\\\\\\\\u4ec0\\\\\\\\u4e48\\\\\\\\u683c\\\\\\\\u5f0f\\\\\\\\u7ec4\\\\\\\\u7ec7\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\uff1f\\\\\\\\u662f\\\\\\\\u5426\\\\\\\\u652f\\\\\\\\u6301\\\\\\\\u4f7f\\\\\\\\u7528\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u7684\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u8fdb\\\\\\\\u884cGRPO\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\uff1f\\\\\\\"},{\\\\\\\"number\\\\\\\":28,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5f20\\\\\\\\u91cf\\\\\\\\u7ef4\\\\\\\\u5ea6\\\\\\\\u4e0d\\\\\\\\u7b26\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u8fd0\\\\\\\\u884cGRPO\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u4ee3\\\\\\\\u7801\\\\\\\\uff0c\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\\u5982\\\\\\\\u4e0b\\\\\\\\uff1a\\\\\\\\nTraceback (most recent call last):\\\\\\\\n  File \\\\\\\\\\\\\\\"<frozen runpy>\\\\\\\\\\\\\\\", line 198, in _run_module_as_main                                                                               | 1.00/64.0 [00:01<01:12, 1.16s/it]\\\\\\\\n  File \\\\\\\\\\\\\\\"<frozen runpy>\\\\\\\\\\\\\\\", line 88, in _run_code\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 128, in <module>\\\\\\\\n    main()\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 124, in main\\\\\\\\n    ray.get(runner.run.remote(ppo_config))\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 22, in auto_init_wrapper\\\\\\\\n    return fn(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 104, in wrapper\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2849, in get\\\\\\\\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 937, in get_objects\\\\\\\\n    raise value.as_instanceof_cause()\\\\\\\\nray.exceptions.RayTaskError(RuntimeError): ray::Runner.run() (pid=305610, ip=10.26.2.130, actor_id=8ec8ba19ed64d85dea29943601000000, repr=<main.Runner object at 0x791294b97170>)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 94, in run\\\\\\\\n    trainer.fit()\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 589, in fit\\\\\\\\n    old_log_probs = self.actor_rollout_ref_wg.compute_log_probs(batch)\\\\\\\\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n    output = ray.get(output)\\\\\\\\n             ^^^^^^^^^^^^^^^\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\nray.exceptions.RayTaskError(RuntimeError): ray::WorkerDict.actor_rollout_ref_compute_log_probs() (pid=307930, ip=10.26.2.130, actor_id=0a1d29a3e2cd087a716a932001000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x72b04f5baa80>)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n    return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 585, in compute_log_probs\\\\\\\\n    output = self.actor.compute_log_prob(data=data)\\\\\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/workers/actor/dp_actor.py\\\\\\\\\\\\\\\", line 209, in compute_log_prob\\\\\\\\n    log_probs = self._forward_micro_batch(model_inputs, temperature=temperature)\\\\\\\\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/workers/actor/dp_actor.py\\\\\\\\\\\\\\\", line 124, in _forward_micro_batch\\\\\\\\n    output = self.actor_module(\\\\\\\\n             ^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1751, in _wrapped_call_impl\\\\\\\\n    return self._call_impl(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1762, in _call_impl\\\\\\\\n    return forward_call(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\\\\\\\\\\\\\\\", line 856, in forward\\\\\\\\n    output = self._fsdp_wrapped_module(*args, **kwargs)\\\\\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1751, in _wrapped_call_impl\\\\\\\\n    return self._call_impl(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1762, in _call_impl\\\\\\\\n    return forward_call(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/models/transformers/qwen2_vl.py\\\\\\\\\\\\\\\", line 340, in qwen2_vl_forward_new\\\\\\\\n    outputs = self.model(\\\\\\\\n              ^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1751, in _wrapped_call_impl\\\\\\\\n    return self._call_impl(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1762, in _call_impl\\\\\\\\n    return forward_call(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/models/transformers/qwen2_vl.py\\\\\\\\\\\\\\\", line 307, in qwen2_vl_base_forward_new\\\\\\\\n    outputs = self.language_model(\\\\\\\\n              ^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1751, in _wrapped_call_impl\\\\\\\\n    return self._call_impl(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1762, in _call_impl\\\\\\\\n    return forward_call(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py\\\\\\\\\\\\\\\", line 875, in forward\\\\\\\\n    \\\\\\\\\\\\\\\"full_attention\\\\\\\\\\\\\\\": create_causal_mask(**mask_kwargs),\\\\\\\\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/transformers/masking_utils.py\\\\\\\\\\\\\\\", line 753, in create_causal_mask\\\\\\\\n    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(\\\\\\\\n                                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/transformers/masking_utils.py\\\\\\\\\\\\\\\", line 704, in _preprocess_mask_arguments\\\\\\\\n    position_ids = position_ids.expand(batch_size, -1)\\\\\\\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\nRuntimeError: expand(torch.cuda.LongTensor{[3, 1, 3138]}, size=[1, -1]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)\\\\\\\\n\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u89e3\\\\\\\\u51b3\\\\\\\\u4e0a\\\\\\\\u8ff0\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\\uff1f\\\\\\\"},{\\\\\\\"number\\\\\\\":27,\\\\\\\"title\\\\\\\":\\\\\\\"Unable to get the tutorial working\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hello, I am following the instructions to get started with EasyR1; however, I keep getting errors in the process. These errors change every time a run the docker environment: I have gotten errors ranging from a problem with the number of gpus I am passing in ---\\\\\\\\\\\\\\\"got gpu 0 expected 8\\\\\\\\\\\\\\\" to workers not being synchronized.\\\\\\\\nAny help in understanding what I am doing wrong would be greatly appreciated. I have looked online and in the issues on this repo, but have not found the solution. I am new to docker, so apologies if this is a simple problem to fix or if I did not set up something correctly.\\\\\\\\n\\\\\\\\nThe steps I have done are: \\\\\\\\n1. downloaded the docker enviroment: `docker pull hiyouga/verl:ngc-th2.7.0-cu12.6-vllm0.9.1`\\\\\\\\n2. used `docker run --gpus all --ipc=host --ulimit memlock=-1 -it --rm -v /HOME_DIR/EasyR1:/workspace/EasyR1 -w /workspace db618adc68d5 bash`\\\\\\\\n3. ran `bash examples/qwen2_5_vl_7b_geo3k_grpo.sh`\\\\\\\\n\\\\\\\\nAnd, most recently, I am getting the following traceback/error:\\\\\\\\n```\\\\\\\\n /usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_5_vl.py:930: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)\\\\\\\\n(WorkerDict pid=12946)   sizes = grid_thw.prod(-1) // merge_size // merge_size\\\\\\\\n(WorkerDict pid=12943) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`. [repeated 7x across cluster]\\\\\\\\n[...]\\\\\\\\nNCCL version 2.21.5+cuda12.4\\\\\\\\n(WorkerDict pid=25283) [rank0]:[W713 04:12:56.043733966 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\\\\\\\\nTraceback (most recent call last):\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/lib/python3.10/runpy.py\\\\\\\\\\\\\\\", line 196, in _run_module_as_main\\\\\\\\n    return _run_code(code, main_globals, None,\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/lib/python3.10/runpy.py\\\\\\\\\\\\\\\", line 86, in _run_code\\\\\\\\n    exec(code, run_globals)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 128, in <module>\\\\\\\\n    main()\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 124, in main\\\\\\\\n    ray.get(runner.run.remote(ppo_config))\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 21, in auto_init_wrapper\\\\\\\\n    return fn(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 103, in wrapper\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2771, in get\\\\\\\\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 919, in get_objects\\\\\\\\n    raise value.as_instanceof_cause()\\\\\\\\nray.exceptions.RayTaskError(DistBackendError): ray::Runner.run() (pid=24423, ip=192.168.0.2, actor_id=971cb4efa74e3acac3b5b28a01000000, repr=<main.Runner object at 0x7f9a701eca90>)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 93, in run\\\\\\\\n    trainer.init_workers()\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 302, in init_workers\\\\\\\\n    self.actor_rollout_ref_wg.init_model()\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n    output = ray.get(output)\\\\\\\\nray.exceptions.RayTaskError(DistBackendError): ray::WorkerDict.actor_rollout_ref_init_model() (pid=29344, ip=192.168.0.2, actor_id=a4a7e38e959ee13c17167d8501000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f5dbc82b310>)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n    return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 366, in init_model\\\\\\\\n    self._build_model_optimizer(\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 242, in _build_model_optimizer\\\\\\\\n    dist.barrier()\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\\\\\\\\\\\\\\\", line 81, in wrapper\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\\\\\\\\\\\\\\\", line 4551, in barrier\\\\\\\\n    work = group.barrier(opts=opts)\\\\\\\\ntorch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\\\\\\\\nncclUnhandledCudaError: Call to CUDA function failed.\\\\\\\\nLast error:\\\\\\\\nCuda failure 'invalid argument'\\\\\\\\n(WorkerDict pid=29344) \\\\\\\\n(WorkerDict pid=29344) e4077bb80b9c:29344:30132 [0] transport/p2p.cc:275 NCCL WARN Cuda failure 'invalid argument'\\\\\\\\n\\\\\\\\n```\\\\\\\\n\\\\\\\\nAnother time I ran this I got:\\\\\\\\n`ValueError: Total available GPUs 0 is less than total desired GPUs 8.`\\\\\\\\n\\\\\\\\nAnother time i get:\\\\\\\\n`(Runner pid=7873)   File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/vllm/compilation/backends.py\\\\\\\\\\\\\\\", line 677, in __call__\\\\\\\\n(Runner pid=7873)     with torch.cuda.graph(cudagraph, pool=self.graph_pool):\\\\\\\\n(Runner pid=7873)   File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/cuda/graphs.py\\\\\\\\\\\\\\\", line 186, in __exit__\\\\\\\\n(Runner pid=7873)     self.cuda_graph.capture_end()\\\\\\\\n(Runner pid=7873)   File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/cuda/graphs.py\\\\\\\\\\\\\\\", line 84, in capture_end\\\\\\\\n(Runner pid=7873)     super().capture_end()\\\\\\\\n(Runner pid=7873) RuntimeError: CUDA error: operation not permitted\\\\\\\\n(Runner pid=7873) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\\\\\\\n(Runner pid=7873) For debugging consider passing CUDA_LAUNCH_BLOCKING=1\\\\\\\\n(Runner pid=7873) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\\\\\\\n(Runner pid=7873) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WorkerDict.actor_rollout_ref_init_model() (pid=12947, ip=192.168.0.3, actor_id=92b10155f368aac73ed7f31601000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f2ce366b2e0>)`\\\\\\\\n\\\\\\\\nI am not sure what I am doing wrong. Shouldn't this script run without any issues? At least, that is what the tutorial makes it seem like. \\\\\\\\n\\\\\\\\nThank you!\\\\\\\\n\\\\\\\"},{\\\\\\\"number\\\\\\\":26,\\\\\\\"title\\\\\\\":\\\\\\\"tokenized_path \\\\\\\\u652f\\\\\\\\u6301token\\\\\\\\u4fdd\\\\\\\\u5b58\\\\\\\\u4e48\\\\\\\\uff1f\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5927\\\\\\\\u4f6c\\\\\\\\u4f60\\\\\\\\u597d\\\\\\\\uff0c\\\\\\\\u60f3\\\\\\\\u77e5\\\\\\\\u9053EasyR1\\\\\\\\u6709\\\\\\\\u7c7b\\\\\\\\u4f3cllamafactory\\\\\\\\u4e2d\\\\\\\\uff0c\\\\\\\\u652f\\\\\\\\u6301tokenized_path\\\\\\\\u8fdb\\\\\\\\u884c\\\\\\\\u8bad\\\\\\\\u7ec3token\\\\\\\\u4fdd\\\\\\\\u5b58\\\\\\\\u7684\\\\\\\\u8bbe\\\\\\\\u7f6e\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\\u4e48\\\\\\\\uff1f\\\\\\\\n\\\\\\\"},{\\\\\\\"number\\\\\\\":25,\\\\\\\"title\\\\\\\":\\\\\\\"any plan for supporting Flow-GRPO?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"https://github.com/yifan123/flow_grpo\\\\\\\"},{\\\\\\\"number\\\\\\\":24,\\\\\\\"title\\\\\\\":\\\\\\\"Some bug with training\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I tried to run the tutorial on my server recently. It is a 8*A100 GPUs with 80GB RAM for each. As the cuda version in the host in below the lowest requirement and it is hard to install another version. I choose to use docker to build the environment, I pull the container and start the container. The script in the examples work proporly in the first and second step. However, when I get to the third step of updating the policy, it just clash.\\\\\\\\n\\\\\\\\n                                                                                                                           \\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 22, in auto_init_wrapper             \\\\\\\\nray.get(runner.run.remote(ppo_config))                                                                                     \\\\\\\\n                                                                                                                           \\\\\\\\n                                                                                                                           \\\\\\\\n                                                                                                                           \\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 104, in wrapper                    \\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2849, in get                                 \\\\\\\\n(Runner pid=16098) wandb:              critic/rewards/mean 0.33543                                                         \\\\\\\\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 937, in get_objects\\\\\\\\n(pid=16098) Running step 0:   5%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b                                                   | 3.00/60.0 [16:06<4:30:39, 285s/it]\\\\\\\\n\\\\\\\\n(pid=33233) Train mini-batches 1: 100%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588| 4.00/4.00 [12:25<00:00, 22.8s/it]\\\\\\\\n    raise value.as_instanceof_cause()\\\\\\\\n\\\\\\\\n(pid=33233) Update policy 2: 100%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588| 5.00/5.00 [11:53<00:00, 4.64s/it]ray.exceptions.RayTaskError(IndexError): ray::Runner.run() (pid=16098, ip=172.17.0.2, actor_id=462c3f5e6c3ec7441f02a99e01000000, repr=<main.Runner object at 0x707158542f20>)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 94, in run\\\\\\\\n    trainer.fit()\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 607, in fit\\\\\\\\n    old_log_probs = self.actor_rollout_ref_wg.compute_log_probs(batch)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n    output = ray.get(output)\\\\\\\\nray.exceptions.RayTaskError(IndexError): ray::WorkerDict.actor_rollout_ref_compute_log_probs() (pid=33458, ip=172.17.0.2, actor_id=11af29bb9fed89a5977131d901000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x72fdd7d62290>)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n    return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 585, in compute_log_probs\\\\\\\\n    output = self.actor.compute_log_prob(data=data)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/workers/actor/dp_actor.py\\\\\\\\\\\\\\\", line 199, in compute_log_prob\\\\\\\\n    micro_batches, batch_idx_list = prepare_dynamic_batch(data, max_token_len=max_token_len)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/utils/seqlen_balancing.py\\\\\\\\\\\\\\\", line 307, in prepare_dynamic_batch\\\\\\\\n    batch, batch_idx_list = rearrange_micro_batches(data.batch, max_token_len=max_token_len)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/utils/seqlen_balancing.py\\\\\\\\\\\\\\\", line 261, in rearrange_micro_batches\\\\\\\\n    micro_bsz_idx = get_seqlen_balanced_partitions(effective_seqlen, num_micro_batches, equal_size=False)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/utils/seqlen_balancing.py\\\\\\\\\\\\\\\", line 184, in get_seqlen_balanced_partitions\\\\\\\\n    partitions = karmarkar_karp(seqlen_list=seqlen_list, k_partitions=k_partitions, equal_size=equal_size)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/utils/seqlen_balancing.py\\\\\\\\\\\\\\\", line 114, in karmarkar_karp\\\\\\\\n    heapq.heappush(states_pq, State(items=[(idx, seqlen)], k=k_partitions))\\\\\\\\n(Runner pid=16098) wandb:               critic/rewards/min 0                                                               \\\\\\\\n(Runner pid=16098) wandb:                 critic/score/max 1                                                               \\\\\\\\n(Runner pid=16098) wandb:                critic/score/mean 0.33543                                                         \\\\\\\\n(Runner pid=16098) wandb:                 critic/score/min 0                                                               \\\\\\\\n(Runner pid=16098) wandb:       global_seqlen/balanced_max 247670                                                          \\\\\\\\n(Runner pid=16098) wandb:       global_seqlen/balanced_min 247090                                                          \\\\\\\\n(Runner pid=16098) wandb:                global_seqlen/max 254919                                                          \\\\\\\\n(Runner pid=16098) wandb:               global_seqlen/mean 247163.25                                                       \\\\\\\\n(Runner pid=16098) wandb:                global_seqlen/min 240316                                                          \\\\\\\\n(Runner pid=16098) wandb:        global_seqlen/minmax_diff 14603                                                           \\\\\\\\n(Runner pid=16098) wandb:          perf/cpu_memory_used_gb 107.23108                                                       \\\\\\\\n(Runner pid=16098) wandb:     perf/max_memory_allocated_gb 42.98667                                                        \\\\\\\\n(Runner pid=16098) wandb:      perf/max_memory_reserved_gb 64.66187                                                        \\\\\\\\n(Runner pid=16098) wandb:                   perf/mfu_actor 0.33784                                                         \\\\\\\\n(Runner pid=16098) wandb:                  perf/throughput 955.20544                                                       \\\\\\\\n(Runner pid=16098) wandb:               perf/time_per_step 258.75402                                                       \\\\\\\\n(Runner pid=16098) wandb:            perf/total_num_tokens 1977306                                                         \\\\\\\\n(Runner pid=16098) wandb:         prompt_length/clip_ratio 0                                                               \\\\\\\\n(Runner pid=16098) wandb:                prompt_length/max 776                                                             \\\\\\\\n(Runner pid=16098) wandb:               prompt_length/mean 425.51953                                                       \\\\\\\\n(Runner pid=16098) wandb:                prompt_length/min 387                                                             \\\\\\\\n(Runner pid=16098) wandb:       response_length/clip_ratio 0.00039                                                         \\\\\\\\n(Runner pid=16098) wandb:              response_length/max 2048                                                            \\\\\\\\n(Runner pid=16098) wandb:             response_length/mean 346.86563                                                       \\\\\\\\n(Runner pid=16098) wandb:              response_length/min 56                                                              \\\\\\\\n(Runner pid=16098) wandb:                  reward/accuracy 0.325                                                           \\\\\\\\n(Runner pid=16098) wandb:                    reward/format 0.4293                                                          \\\\\\\\n(Runner pid=16098) wandb:                   reward/overall 0.33543                                                         \\\\\\\\n(Runner pid=16098) wandb:          timing_per_token_ms/adv 6e-05                                                           \\\\\\\\n(Runner pid=16098) wandb:          timing_per_token_ms/gen 0.08111                                                         \\\\\\\\n(Runner pid=16098) wandb:          timing_per_token_ms/old 0.01803                                                         \\\\\\\\n(Runner pid=16098) wandb:          timing_per_token_ms/ref 0.01467                                                         \\\\\\\\n(Runner pid=16098) wandb:       timing_per_token_ms/reward 0.00238                                                         \\\\\\\\n(Runner pid=16098) wandb: timing_per_token_ms/update_actor 0.0604                                                          \\\\\\\\n(Runner pid=16098) wandb:                     timing_s/adv 0.12637                                                         \\\\\\\\n(Runner pid=16098) wandb:                     timing_s/gen 72.02265                                                        \\\\\\\\n(Runner pid=16098) wandb:                     timing_s/old 35.64964                                                        \\\\\\\\n(Runner pid=16098) wandb:                     timing_s/ref 29.00215                                                        \\\\\\\\n(Runner pid=16098) wandb:                  timing_s/reward 2.11218                                                         \\\\\\\\n(Runner pid=16098) wandb:                    timing_s/step 258.75402                                                       \\\\\\\\n(Runner pid=16098) wandb:            timing_s/update_actor 119.43438                                                       \\\\\\\\n(Runner pid=16098) wandb:              val/accuracy_reward 0.3827                                                          \\\\\\\\n(Runner pid=16098) wandb:                val/format_reward 0.1797                                                          \\\\\\\\n(Runner pid=16098) wandb:               val/overall_reward 0.3624                                                          \\\\\\\\n(Runner pid=16098) wandb:                 val/reward_score 0.3624                                                          \\\\\\\\n(Runner pid=16098) wandb:     val_prompt_length/clip_ratio 0                                                               \\\\\\\\n(Runner pid=16098) wandb:            val_prompt_length/max 784                                                             \\\\\\\\n(Runner pid=16098) wandb:           val_prompt_length/mean 428.5574                                                        \\\\\\\\n(Runner pid=16098) wandb:            val_prompt_length/min 387                                                             \\\\\\\\n(Runner pid=16098) wandb:   val_response_length/clip_ratio 0.00333                                                         \\\\\\\\n(Runner pid=16098) wandb:          val_response_length/max 2048                                                            \\\\\\\\n(Runner pid=16098) wandb:         val_response_length/mean 371.5441                                                        \\\\\\\\n(Runner pid=16098) wandb:          val_response_length/min 60                                                              \\\\\\\\n(Runner pid=16098) wandb:                                                                                                  \\\\\\\\n(Runner pid=16098) wandb: \\\\\\\\ud83d\\\\\\\\ude80 View run qwen2_5_vl_7b_geo_grpo at: https://wandb.ai/GreatResearchTakesTime/easy_r1/runs/zabor1wb                                                                                                                        \\\\\\\\n(Runner pid=16098) wandb: \\\\\\\\u2b50\\\\\\\\ufe0f View project at: https://wandb.ai/GreatResearchTakesTime/easy_r1                              \\\\\\\\n(Runner pid=16098) wandb: Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)                    \\\\\\\\n(Runner pid=16098) wandb: Find logs at: ./wandb/run-20250716_071305-zabor1wb/logs                                          \\\\\\\\n(pid=16098) Running step 0:   5%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b                                                   | 3.00/60.0 [16:08<4:30:39, 285s/itException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookroot@60b7a9c\\\\\\\\nroot@60b7a9cfca50:/workspace/EasyR1# \\\\\\\"},{\\\\\\\"number\\\\\\\":23,\\\\\\\"title\\\\\\\":\\\\\\\"tensor_parallel_size\\\\\\\\u8bbe\\\\\\\\u7f6e\\\\\\\\u4e3a2\\\\\\\\u65f6\\\\\\\\u5019\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u9ed8\\\\\\\\u8ba4\\\\\\\\u7684tensor_parallel_size\\\\\\\\u4e3a1\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u60f3\\\\\\\\u4fee\\\\\\\\u6539\\\\\\\\u62102\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\\uff1a\\\\\\\\n```\\\\\\\\n2025-07-17 17:00:39 | INFO | stdout | \\\\\\\\u001b[36m(pid=47761)\\\\\\\\u001b[0m NCCL version 2.21.5+cuda11.0\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | Traceback (most recent call last):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/runpy.py\\\\\\\\\\\\\\\", line 196, in _run_module_as_main\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return _run_code(code, main_globals, None,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/runpy.py\\\\\\\\\\\\\\\", line 86, in _run_code\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     exec(code, run_globals)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 216, in <module>\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     main()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 212, in main\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     ray.get(runner.run.remote(ppo_config))\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 21, in auto_init_wrapper\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 103, in wrapper\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2822, in get\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 930, in get_objects\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     raise value.as_instanceof_cause()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | ray.exceptions.RayTaskError(KeyError): \\\\\\\\u001b[36mray::Runner.run()\\\\\\\\u001b[39m (pid=41424, ip=10.140.37.138, actor_id=aac3cbdc33303e9fb12a736501000000, repr=<main.Runner object at 0x7fc5f5d2cee0>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 99, in run\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     trainer.init_workers()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 443, in init_workers\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.actor_rollout_wg.init_model()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     output = ray.get(output)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | ray.exceptions.RayTaskError(KeyError): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_init_model()\\\\\\\\u001b[39m (pid=47760, ip=10.140.37.138, actor_id=c03b337d86fc2c524621f42701000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f31ad170c10>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 384, in init_model\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self._build_rollout()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 315, in _build_rollout\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.rollout = vLLMRollout(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 76, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.inference_engine = LLM(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1161, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 247, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.llm_engine = LLMEngine.from_engine_args(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/engine/llm_engine.py\\\\\\\\\\\\\\\", line 510, in from_engine_args\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return engine_cls.from_vllm_config(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 112, in from_vllm_config\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return cls(vllm_config=vllm_config,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 92, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCoreClient.make_client(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 75, in make_client\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return InprocClient(vllm_config, executor_class, log_stats)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 198, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCore(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 64, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.model_executor = executor_class(vllm_config)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/executor_base.py\\\\\\\\\\\\\\\", line 52, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self._init_executor()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 121, in _init_executor\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.collective_rpc(\\\\\\\\\\\\\\\"init_device\\\\\\\\\\\\\\\")\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     answer = run_method(self.driver_worker, method, args, kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2456, in run_method\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/worker/worker_base.py\\\\\\\\\\\\\\\", line 604, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.worker.init_device()  # type: ignore\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 135, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     init_worker_distributed_environment(self.vllm_config, self.rank,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 326, in init_worker_distributed_environment\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 1022, in ensure_model_parallel_initialized\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     initialize_model_parallel(tensor_model_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 975, in initialize_model_parallel\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     _TP = init_model_parallel_group(group_ranks,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 788, in init_model_parallel_group\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return GroupCoordinator(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 252, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.device_communicator = device_comm_cls(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py\\\\\\\\\\\\\\\", line 47, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.ca_comm = CustomAllreduce(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 145, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     if not current_platform.is_rocm() and not _can_p2p(rank, world_size):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 37, in _can_p2p\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     if not gpu_p2p_access_check(rank, i):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\\\\\\\\\\\\\\\", line 248, in gpu_p2p_access_check\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return _gpu_p2p_access_cache[f\\\\\\\\\\\\\\\"{src}->{tgt}\\\\\\\\\\\\\\\"]\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | KeyError: '1->0'\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 16:59:58 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 16:59:59 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 16:59:59 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 17:00:05 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 17:00:06 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 17:00:07 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 17:00:07 | ERROR | stderr |\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_init_model()\\\\\\\\u001b[39m (pid=47762, ip=10.140.37.138, actor_id=fd8e5e2f1f8107d072c83c9801000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fc572d50c10>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 384, in init_model\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._build_rollout()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 315, in _build_rollout\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.rollout = vLLMRollout(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 76, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.inference_engine = LLM(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1161, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 247, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.llm_engine = LLMEngine.from_engine_args(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/engine/llm_engine.py\\\\\\\\\\\\\\\", line 510, in from_engine_args\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return engine_cls.from_vllm_config(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 112, in from_vllm_config\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return cls(vllm_config=vllm_config,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 92, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCoreClient.make_client(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 75, in make_client\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return InprocClient(vllm_config, executor_class, log_stats)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 198, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCore(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 64, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.model_executor = executor_class(vllm_config)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/executor_base.py\\\\\\\\\\\\\\\", line 52, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._init_executor()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 121, in _init_executor\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.collective_rpc(\\\\\\\\\\\\\\\"init_device\\\\\\\\\\\\\\\")\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     answer = run_method(self.driver_worker, method, args, kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2456, in run_method\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/worker/worker_base.py\\\\\\\\\\\\\\\", line 604, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.worker.init_device()  # type: ignore\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 135, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     init_worker_distributed_environment(self.vllm_config, self.rank,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 326, in init_worker_distributed_environment\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 1022, in ensure_model_parallel_initialized\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     initialize_model_parallel(tensor_model_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 975, in initialize_model_parallel\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     _TP = init_model_parallel_group(group_ranks,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 788, in init_model_parallel_group\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return GroupCoordinator(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 252, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.device_communicator = device_comm_cls(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py\\\\\\\\\\\\\\\", line 47, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.ca_comm = CustomAllreduce(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 145, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not current_platform.is_rocm() and not _can_p2p(rank, world_size):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 37, in _can_p2p\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not gpu_p2p_access_check(rank, i):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\\\\\\\\\\\\\\\", line 248, in gpu_p2p_access_check\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return _gpu_p2p_access_cache[f\\\\\\\\\\\\\\\"{src}->{tgt}\\\\\\\\\\\\\\\"]\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | KeyError: '1->0'\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_init_model()\\\\\\\\u001b[39m (pid=47761, ip=10.140.37.138, actor_id=47ed97652b87fc5762ebbb2401000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7eeb7b44f0a0>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 384, in init_model\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._build_rollout()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 315, in _build_rollout\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.rollout = vLLMRollout(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 76, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.inference_engine = LLM(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1161, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 247, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.llm_engine = LLMEngine.from_engine_args(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/engine/llm_engine.py\\\\\\\\\\\\\\\", line 510, in from_engine_args\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return engine_cls.from_vllm_config(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 112, in from_vllm_config\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return cls(vllm_config=vllm_config,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 92, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCoreClient.make_client(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 75, in make_client\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return InprocClient(vllm_config, executor_class, log_stats)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 198, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCore(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 64, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.model_executor = executor_class(vllm_config)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/executor_base.py\\\\\\\\\\\\\\\", line 52, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._init_executor()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 121, in _init_executor\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.collective_rpc(\\\\\\\\\\\\\\\"init_device\\\\\\\\\\\\\\\")\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     answer = run_method(self.driver_worker, method, args, kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2456, in run_method\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/worker/worker_base.py\\\\\\\\\\\\\\\", line 604, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.worker.init_device()  # type: ignore\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 135, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     init_worker_distributed_environment(self.vllm_config, self.rank,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 326, in init_worker_distributed_environment\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 1022, in ensure_model_parallel_initialized\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     initialize_model_parallel(tensor_model_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 975, in initialize_model_parallel\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     _TP = init_model_parallel_group(group_ranks,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 788, in init_model_parallel_group\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return GroupCoordinator(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 252, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.device_communicator = device_comm_cls(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py\\\\\\\\\\\\\\\", line 47, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.ca_comm = CustomAllreduce(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 145, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not current_platform.is_rocm() and not _can_p2p(rank, world_size):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 37, in _can_p2p\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not gpu_p2p_access_check(rank, i):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\\\\\\\\\\\\\\\", line 248, in gpu_p2p_access_check\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return _gpu_p2p_access_cache[f\\\\\\\\\\\\\\\"{src}->{tgt}\\\\\\\\\\\\\\\"]\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | KeyError: '0->1'\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_init_model()\\\\\\\\u001b[39m (pid=46880, ip=10.140.37.138, actor_id=f0ee4094614e96a389af02d101000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fca14d141f0>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 384, in init_model\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._build_rollout()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 315, in _build_rollout\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.rollout = vLLMRollout(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 76, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.inference_engine = LLM(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1161, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 247, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.llm_engine = LLMEngine.from_engine_args(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/engine/llm_engine.py\\\\\\\\\\\\\\\", line 510, in from_engine_args\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return engine_cls.from_vllm_config(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 112, in from_vllm_config\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return cls(vllm_config=vllm_config,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 92, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCoreClient.make_client(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 75, in make_client\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return InprocClient(vllm_config, executor_class, log_stats)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 198, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCore(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 64, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.model_executor = executor_class(vllm_config)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/executor_base.py\\\\\\\\\\\\\\\", line 52, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._init_executor()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 121, in _init_executor\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.collective_rpc(\\\\\\\\\\\\\\\"init_device\\\\\\\\\\\\\\\")\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     answer = run_method(self.driver_worker, method, args, kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2456, in run_method\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/worker/worker_base.py\\\\\\\\\\\\\\\", line 604, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.worker.init_device()  # type: ignore\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 135, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     init_worker_distributed_environment(self.vllm_config, self.rank,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 326, in init_worker_distributed_environment\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 1022, in ensure_model_parallel_initialized\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     initialize_model_parallel(tensor_model_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 975, in initialize_model_parallel\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     _TP = init_model_parallel_group(group_ranks,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 788, in init_model_parallel_group\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return GroupCoordinator(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 252, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.device_communicator = device_comm_cls(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py\\\\\\\\\\\\\\\", line 47, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.ca_comm = CustomAllreduce(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 145, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not current_platform.is_rocm() and not _can_p2p(rank, world_size):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 37, in _can_p2p\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not gpu_p2p_access_check(rank, i):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\\\\\\\\\\\\\\\", line 248, in gpu_p2p_access_check\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return _gpu_p2p_access_cache[f\\\\\\\\\\\\\\\"{src}->{tgt}\\\\\\\\\\\\\\\"]\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | KeyError: '0->1'```\\\\\\\\n\\\\\\\\n\\\\\\\\u6211\\\\\\\\u5c1d\\\\\\\\u8bd5\\\\\\\\u4e86\\\\\\\\u4e00\\\\\\\\u4e9b\\\\\\\\u65b9\\\\\\\\u6cd5\\\\\\\\uff0c\\\\\\\\u6bd4\\\\\\\\u5982\\\\\\\\u8bbe\\\\\\\\u7f6eexport VLLM_RPC_BASE_PATH=/dev/shm\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u6ca1\\\\\\\\u6709\\\\\\\\u7528\\\\\\\\uff0c\\\\\\\\u4ee3\\\\\\\\u7801\\\\\\\\u672c\\\\\\\\u8eab\\\\\\\\u662f\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u8fd0\\\\\\\\u884c\\\\\\\\u7684\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u53ea\\\\\\\\u6539\\\\\\\\u4e86\\\\\\\\u8fd9\\\\\\\\u4e00\\\\\\\\u4e2a\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\"},{\\\\\\\"number\\\\\\\":22,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u52a0\\\\\\\\u8f7d\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\\uff0cv3.0.1\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u52a0\\\\\\\\u8f7d\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\uff1f\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"v3.0.1\\\\\\\\u4e2d\\\\\\\\u7684\\\\\\\\uff1a\\\\\\\\nEasyR1/verl/utils/dataset.py line 132\\\\\\\\n...\\\\\\\\n        if os.path.isdir(data_path):\\\\\\\\n            # when we use dataset builder, we should always refer to the train split\\\\\\\\n            **file_type = os.path.splitext(os.listdir(data_path)[0])[-1][1:].replace(\\\\\\\\\\\\\\\"jsonl\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\")**\\\\\\\\n            self.dataset = load_dataset(file_type, data_dir=data_path, split=data_split)\\\\\\\\n        elif os.path.isfile(data_path):\\\\\\\\n            file_type = os.path.splitext(data_path)[-1][1:].replace(\\\\\\\\\\\\\\\"jsonl\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\")\\\\\\\\n            self.dataset = load_dataset(file_type, data_files=data_path, split=data_split)\\\\\\\\n        else:\\\\\\\\n            # load remote dataset from huggingface hub\\\\\\\\n            self.dataset = load_dataset(data_path, split=data_split)\\\\\\\\n...\\\\\\\\n\\\\\\\\n\\\\\\\\u8fd9\\\\\\\\u4e00\\\\\\\\u884c\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\\uff0c\\\\\\\\u5904\\\\\\\\u7406\\\\\\\\u4e0d\\\\\\\\u4e86\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u7684./geomery3k \\\\\\\\u8def\\\\\\\\u5f84\\\\\\\\u3002\\\\\\\\n\\\\\\\\u6539\\\\\\\\u6210v3.0.0\\\\\\\\u4e2d\\\\\\\\u7684\\\\\\\\nself.dataset = load_dataset(\\\\\\\\\\\\\\\"parquet\\\\\\\\\\\\\\\", data_dir=data_path, split=data_split)\\\\\\\\n\\\\\\\\u5c31\\\\\\\\u6b63\\\\\\\\u5e38\\\\\\\\u4e86\\\\\\\\u3002\\\\\\\\n\\\\\\\\n\\\\\\\\u662f\\\\\\\\u6211\\\\\\\\u5b9a\\\\\\\\u4e49\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u7684\\\\\\\\u65b9\\\\\\\\u5f0f\\\\\\\\u4e0d\\\\\\\\u5bf9\\\\\\\\u4e48\\\\\\\\uff1f\\\\\\\\n\\\\\\\\n\\\\\\\\n```bash\\\\\\\\nMODEL_PATH=/home/xxx/models/Qwen2.5-VL-3B-Instruct/  # replace it with your local file path\\\\\\\\n\\\\\\\\npython3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n    config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n    data.train_files=./geometry3k@train \\\\\\\\\\\\\\\\\\\\\\\\n    data.val_files=./geometry3k@test \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.tensor_parallel_size=2 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.experiment_name=XiaomiMiMo_7B_RL_geo_grpo \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.fsdp.torch_dtype=bf16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.optim.strategy=adamw_bf16 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.logger=['console'] \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.n_gpus_per_node=2\\\\\\\\n```\\\\\\\\n\\\\\\\"},{\\\\\\\"number\\\\\\\":21,\\\\\\\"title\\\\\\\":\\\\\\\"v100\\\\\\\\u4e0d\\\\\\\\u517c\\\\\\\\u5bb9flash atten\\\\\\\\uff0c\\\\\\\\u4f7f\\\\\\\\u7528xformer\\\\\\\\u4ecd\\\\\\\\u7136\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u4e4b\\\\\\\\u524d\\\\\\\\u7528H100\\\\\\\\u8dd1\\\\\\\\u901a\\\\\\\\u4e86\\\\\\\\u4ee3\\\\\\\\u7801\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u5207\\\\\\\\u5230V100\\\\\\\\u663e\\\\\\\\u793a\\\\\\\\u4e0d\\\\\\\\u652f\\\\\\\\u6301flash atten2\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u5728\\\\\\\\u65b0\\\\\\\\u7684\\\\\\\\u73af\\\\\\\\u5883\\\\\\\\u4e0b\\\\\\\\u6ca1\\\\\\\\u6709\\\\\\\\u4e0b\\\\\\\\u8f7dflash atten\\\\\\\\uff0c\\\\\\\\u5e76\\\\\\\\u5728\\\\\\\\u914d\\\\\\\\u7f6e\\\\\\\\u6587\\\\\\\\u4ef6\\\\\\\\u4e2d\\\\\\\\u6307\\\\\\\\u5b9aattn_implementation: xformers\\\\\\\\uff0c\\\\\\\\u7981\\\\\\\\u7528\\\\\\\\u6240\\\\\\\\u6709atten\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\\u540e\\\\\\\\u8fd8\\\\\\\\u662f\\\\\\\\u62a5\\\\\\\\u540c\\\\\\\\u6837\\\\\\\\u7684\\\\\\\\u4e0d\\\\\\\\u652f\\\\\\\\u6301flash atten2\\\\\\\\u7684\\\\\\\\u9519\\\\\\\\u8bef\\\\\\\\uff0c\\\\\\\\u611f\\\\\\\\u8c22\\\\\\\\u56de\\\\\\\\u590d\\\\\\\\n raise ImportError(f\\\\\\\\\\\\\\\"{preface} the package flash_attn seems to be not installed. {install_message}\\\\\\\\\\\\\\\")\\\\\\\\n(Runner pid=320551) ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\\\\\\\"},{\\\\\\\"number\\\\\\\":20,\\\\\\\"title\\\\\\\":\\\\\\\"nnodes>1 Software caused connection abort\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"<img width=\\\\\\\\\\\\\\\"2478\\\\\\\\\\\\\\\" height=\\\\\\\\\\\\\\\"1114\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/14086594-9e11-4b81-b528-f922219dd7c5\\\\\\\\\\\\\\\" />\\\\\\\\nyaml nnodes: 2or3or4 will report above error\\\\\\\\ni have checked ray status, all 4 nodes are settled. i run the bash script in the master node, it seems always try to connect to a node that do not exist in my ray list.\\\\\\\\n\\\\\\\\n<!-- Failed to upload \\\\\\\\\\\\\\\"image.png\\\\\\\\\\\\\\\" -->\\\\\\\"},{\\\\\\\"number\\\\\\\":19,\\\\\\\"title\\\\\\\":\\\\\\\"video-image mixed dataset\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"video or image \\\\\\\\u5355\\\\\\\\u72ec\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u662f\\\\\\\\u6b63\\\\\\\\u5e38\\\\\\\\u7684\\\\\\\\n\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u6df7\\\\\\\\u5408\\\\\\\\u8bad\\\\\\\\u4f1a\\\\\\\\u5361\\\\\\\\u5728Update policy\\\\\\\\n\\\\\\\\n\\\\\\\\u91c7\\\\\\\\u7528\\\\\\\\u7684\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u5f62\\\\\\\\u5f0f\\\\\\\\u662f\\\\\\\\uff1a\\\\\\\\n ```\\\\\\\\nif example['data_type'] == 'image':\\\\\\\\n                data = {\\\\\\\\n                    \\\\\\\\\\\\\\\"problem_type\\\\\\\\\\\\\\\": example['problem_type'],\\\\\\\\n                    \\\\\\\\\\\\\\\"data_source\\\\\\\\\\\\\\\": data_source,\\\\\\\\n                    \\\\\\\\\\\\\\\"problem\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"<image>\\\\\\\\\\\\\\\" + QUESTION_TEMPLATE.format(Question=question) + TYPE_TEMPLATE[example['problem_type']],\\\\\\\\n                    \\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\": answer,\\\\\\\\n                    \\\\\\\\\\\\\\\"images\\\\\\\\\\\\\\\": [video_path],\\\\\\\\n                    \\\\\\\\\\\\\\\"videos\\\\\\\\\\\\\\\": [],\\\\\\\\n                    \\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\": answer_raw\\\\\\\\n                }\\\\\\\\n            else:\\\\\\\\n                data = {\\\\\\\\n                    \\\\\\\\\\\\\\\"problem_type\\\\\\\\\\\\\\\": example['problem_type'],\\\\\\\\n                    \\\\\\\\\\\\\\\"data_source\\\\\\\\\\\\\\\": data_source,\\\\\\\\n                    \\\\\\\\\\\\\\\"problem\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"<video>\\\\\\\\\\\\\\\" + QUESTION_TEMPLATE.format(Question=question) + TYPE_TEMPLATE[example['problem_type']],\\\\\\\\n                    \\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\": answer,\\\\\\\\n                    \\\\\\\\\\\\\\\"images\\\\\\\\\\\\\\\": [],\\\\\\\\n                    \\\\\\\\\\\\\\\"videos\\\\\\\\\\\\\\\": [image_path],\\\\\\\\n                    \\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\": answer_raw\\\\\\\\n                }\\\\\\\\n```\\\\\\\"},{\\\\\\\"number\\\\\\\":18,\\\\\\\"title\\\\\\\":\\\\\\\"During debugging, the program cannot enter the reward function, even though a breakpoint is set.\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi, Thanks for your great work. \\\\\\\\n\\\\\\\\nI want to incorporate my reward function into easyr1. However, the program cannot enter the reward function, even though a breakpoint is set.\\\\\\\\n\\\\\\\\nIt seems that the program can only enter the line without further getting into the reward function. \\\\\\\\n\\\\\\\\nreward_tensor, reward_metrics = ray.get(self.val_reward_fn.compute_reward.remote(test_batch))\\\\\\\\nhttps://github.com/hiyouga/EasyR1/blob/1bdeb1296c120ab9875ce3f4f083eb8b59c829c6/verl/trainer/ray_trainer.py#L420\\\\\\\\n\\\\\\\\nI use the Ray Distributed Debugger following verl's docs. Could you help me to take a check?\\\\\\\\n\\\\\\\"},{\\\\\\\"number\\\\\\\":17,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5e0c\\\\\\\\u671b\\\\\\\\u80fd\\\\\\\\u591f\\\\\\\\u548cLLama factory\\\\\\\\u4e00\\\\\\\\u6837\\\\\\\\u63a8\\\\\\\\u51faweb ui\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5e0c\\\\\\\\u671b\\\\\\\\u80fd\\\\\\\\u591f\\\\\\\\u548cLLama factory\\\\\\\\u4e00\\\\\\\\u6837\\\\\\\\u63a8\\\\\\\\u51faweb ui \\\\\\\\u671f\\\\\\\\u5f85\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u80fd\\\\\\\\u529b\\\\\\\"},{\\\\\\\"number\\\\\\\":16,\\\\\\\"title\\\\\\\":\\\\\\\"Is it possible to support the SGlang Rollout?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Sglang rollout is also a very convenient inference engine. I'd like to ask if you've considered officially supporting the Sglang backend. \\\\\\\"},{\\\\\\\"number\\\\\\\":15,\\\\\\\"title\\\\\\\":\\\\\\\"Support for 3D MLLM (e.g., PointLLM) Integration\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi team,\\\\\\\\n\\\\\\\\nI'm wondering if EasyR1 currently supports 3D multimodal large language models like PointLLM?\\\\\\\\n\\\\\\\\nIf this feature isn't available yet or not in the roadmap, could you provide some guidance on how to integrate 3D MLLMs with the GRPO algorithm? Any development tips or architectural considerations would be greatly appreciated.\\\\\\\\n\\\\\\\\nThank you for your time and the excellent work on this project!\\\\\\\"},{\\\\\\\"number\\\\\\\":14,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u5728\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u7684\\\\\\\\u65f6\\\\\\\\u5019\\\\\\\\u51bb\\\\\\\\u7ed3\\\\\\\\u8bed\\\\\\\\u8a00\\\\\\\\u6a21\\\\\\\\u5757\\\\\\\\uff0c\\\\\\\\u53ea\\\\\\\\u8bad\\\\\\\\u7ec3vision tower\\\\\\\\uff1f\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u6211\\\\\\\\u5728https://github.com/hiyouga/EasyR1/blob/main/verl/workers/fsdp_workers.py#L239 \\\\\\\\u4e4b\\\\\\\\u540e\\\\\\\\u52a0\\\\\\\\u4e86\\\\\\\\u8fd9\\\\\\\\u6837\\\\\\\\u7684\\\\\\\\u4ee3\\\\\\\\u7801\\\\\\\\uff0c\\\\\\\\u4e0d\\\\\\\\u77e5\\\\\\\\u9053\\\\\\\\u5bf9\\\\\\\\u4e0d\\\\\\\\u5bf9\\\\\\\\uff1f\\\\\\\\n\\\\\\\\n        if model_config.freeze_language_backbone:\\\\\\\\n            model.requires_grad_(False)\\\\\\\\n            \\\\\\\\n            if not model_config.freeze_vision_tower:\\\\\\\\n                 if hasattr(model, \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\") and hasattr(model.model, \\\\\\\\\\\\\\\"visual\\\\\\\\\\\\\\\"):\\\\\\\\n                    model.model.visual.requires_grad_(True)\\\\\\\\n                 elif hasattr(model, \\\\\\\\\\\\\\\"visual\\\\\\\\\\\\\\\"):\\\\\\\\n                    model.visual.requires_grad_(True)\\\\\\\\n\\\\\\\\n            if hasattr(model, \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\") and hasattr(model.model, \\\\\\\\\\\\\\\"mm_projector\\\\\\\\\\\\\\\"):\\\\\\\\n                model.model.mm_projector.requires_grad_(True)\\\\\\\\n            if hasattr(model, \\\\\\\\\\\\\\\"lm_head\\\\\\\\\\\\\\\"):\\\\\\\\n                model.lm_head.requires_grad_(True)\\\\\\\\n            \\\\\\\\n            fsdp_config.use_orig_params = True\\\\\\\\n            self.print_rank0(\\\\\\\\\\\\\\\"Language backbone is frozen. Only specified layers are trainable.\\\\\\\\\\\\\\\")\\\\\\\"},{\\\\\\\"number\\\\\\\":13,\\\\\\\"title\\\\\\\":\\\\\\\"AttributeError: module 'torch.library' has no attribute 'wrap_triton'\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"What is the version of torch, torchvision, and torchaudio?\\\\\\\\n\\\\\\\\nMy version:\\\\\\\\ntorch==2.5.1\\\\\\\\ntorchvision==0.20.1\\\\\\\\ntorchaudio==2.5.1\\\\\\\\n\\\\\\\\nAnd the error: AttributeError: module 'torch.library' has no attribute 'wrap_triton'\\\\\\\\n\\\\\\\\nThanks for help!!\\\\\\\"},{\\\\\\\"number\\\\\\\":12,\\\\\\\"title\\\\\\\":\\\\\\\"core_worker.cc:492: Failed to register worker to Raylet: IOError: [RayletClient] Unable to register worker with raylet. Failed to read data from the socket: End of file worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"When I run `bash examples/qwen2_5_vl_7b_geo3k_grpo.sh`, it raises error `core_worker.cc:492: Failed to register worker to Raylet: IOError: [RayletClient] Unable to register worker with raylet. Failed to read data from the socket: End of file worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff`. \\\\\\\\n\\\\\\\\nThe whole output is:\\\\\\\\n```\\\\\\\\nINFO 07-25 14:15:49 [__init__.py:256] Automatically detected platform cuda.\\\\\\\\n2025-07-25 14:16:34,565 ERROR services.py:1350 -- Failed to start the dashboard \\\\\\\\n2025-07-25 14:16:34,566 ERROR services.py:1375 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure' to find where the log file is.\\\\\\\\n2025-07-25 14:16:34,567 ERROR services.py:1419 -- \\\\\\\\nThe last 20 lines of /root/tmp/ray/session_2025-07-25_14-16-09_760500_504098/logs/dashboard.log (it contains the error message from the dashboard): \\\\\\\\n2025-07-25 14:16:23,921 INFO utils.py:337 -- Available modules: [<class 'ray.dashboard.modules.usage_stats.usage_stats_head.UsageStatsHead'>]\\\\\\\\n2025-07-25 14:16:23,922 INFO head.py:234 -- DashboardHeadModules to load: None.\\\\\\\\n2025-07-25 14:16:23,922 INFO head.py:237 -- Loading DashboardHeadModule: <class 'ray.dashboard.modules.usage_stats.usage_stats_head.UsageStatsHead'>.\\\\\\\\n2025-07-25 14:16:23,922 INFO head.py:241 -- Loaded 1 dashboard head modules: [<ray.dashboard.modules.usage_stats.usage_stats_head.UsageStatsHead object at 0x7fa85d74a2c0>].\\\\\\\\n2025-07-25 14:16:23,922 INFO utils.py:304 -- Get all modules by type: SubprocessModule\\\\\\\\n2025-07-25 14:16:24,019 INFO utils.py:337 -- Available modules: [<class 'ray.dashboard.modules.metrics.metrics_head.MetricsHead'>, <class 'ray.dashboard.modules.data.data_head.DataHead'>, <class 'ray.dashboard.modules.event.event_head.EventHead'>, <class 'ray.dashboard.modules.job.job_head.JobHead'>, <class 'ray.dashboard.modules.node.node_head.NodeHead'>, <class 'ray.dashboard.modules.reporter.reporter_head.ReportHead'>, <class 'ray.dashboard.modules.serve.serve_head.ServeHead'>, <class 'ray.dashboard.modules.state.state_head.StateHead'>, <class 'ray.dashboard.modules.train.train_head.TrainHead'>]\\\\\\\\n2025-07-25 14:16:24,020 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.metrics.metrics_head.MetricsHead'>.\\\\\\\\n2025-07-25 14:16:24,020 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.data.data_head.DataHead'>.\\\\\\\\n2025-07-25 14:16:24,020 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.event.event_head.EventHead'>.\\\\\\\\n2025-07-25 14:16:24,020 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.job.job_head.JobHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.node.node_head.NodeHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.reporter.reporter_head.ReportHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.serve.serve_head.ServeHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.state.state_head.StateHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.train.train_head.TrainHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:295 -- Loaded 9 subprocess modules: [<ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa84a5aba30>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa8521e0fa0>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa8521e0f40>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848070c10>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848073820>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848073f70>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848073fd0>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848072890>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848073f40>].\\\\\\\\n\\\\\\\\n2025-07-25 14:16:37,063 INFO worker.py:1927 -- Started a local Ray instance.\\\\\\\\n[2025-07-25 14:17:08,382 E 504098 504098] core_worker.cc:492: Failed to register worker to Raylet: IOError: [RayletClient] Unable to register worker with raylet. Failed to read data from the socket: End of file worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff\\\\\\\\n```\\\\\\\\n\\\\\\\\nThe version of the packages:\\\\\\\\ntorch 2.6.0\\\\\\\\nray 2.48.0\\\\\\\\nvllm 0.8.0\\\\\\\\ntransformers 4.52.4\\\\\\\\n\\\\\\\\n\\\\\\\\nActually, I met this error under all the environments based on `verl`, but I can't solve it by searching online.\\\\\\\"},{\\\\\\\"number\\\\\\\":11,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u66f4\\\\\\\\u65b0\\\\\\\\u5fae\\\\\\\\u4fe1\\\\\\\\u7fa4\\\\\\\\u4e8c\\\\\\\\u7ef4\\\\\\\\u7801\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u60a8\\\\\\\\u597d\\\\\\\\uff0c\\\\\\\\u5fae\\\\\\\\u4fe1\\\\\\\\u7fa4\\\\\\\\u4e8c\\\\\\\\u7ef4\\\\\\\\u7801\\\\\\\\u8fc7\\\\\\\\u671f\\\\\\\\u4e86\\\\\\\\uff0c\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u66f4\\\\\\\\u65b0\\\\\\\\u4e00\\\\\\\\u4e0b\\\\\\\\u5417\\\\\\\\uff0c\\\\\\\\u8c22\\\\\\\\u8c22\\\\\\\\uff01\\\\\\\"},{\\\\\\\"number\\\\\\\":10,\\\\\\\"title\\\\\\\":\\\\\\\"PG loss does not converge\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I attempted to train the Qwen 2.5 7B math model on the Math12K dataset, but the results show severe divergence. In my experiments, I disabled the KL divergence terms since current research suggests they have negligible impact (all other parameters remained unchanged). Has anyone encountered a similar issue?\\\\\\\\n\\\\\\\\n<img width=\\\\\\\\\\\\\\\"1099\\\\\\\\\\\\\\\" height=\\\\\\\\\\\\\\\"324\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/ff7f55b3-908a-4f1d-8ddc-bf5294ccdea6\\\\\\\\\\\\\\\" />\\\\\\\\n\\\\\\\\n<img width=\\\\\\\\\\\\\\\"1111\\\\\\\\\\\\\\\" height=\\\\\\\\\\\\\\\"329\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/109c2c10-266a-4239-a218-89c02d2d7b7c\\\\\\\\\\\\\\\" />\\\\\\\"},{\\\\\\\"number\\\\\\\":9,\\\\\\\"title\\\\\\\":\\\\\\\"Training Memory Leak at various steps\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"While training with a larger batch size, I encounter consistently a OOM issue when the second my train-batch is started, while with lower batch sizes the OOM error happens at various steps between step 3 and 7. What could be the reason for this?\\\\\\\\n\\\\\\\\nI using the config.yaml as basis with the following adjustments on 4 H100s for the lower batch size:\\\\\\\\n\\\\\\\\n#!/bin/bash\\\\\\\\n\\\\\\\\nset -x\\\\\\\\n\\\\\\\\nexport PYTHONUNBUFFERED=1\\\\\\\\n\\\\\\\\nMODEL_PATH=Qwen/Qwen2.5-VL-7B-Instruct\\\\\\\\n\\\\\\\\npython3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n    config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.n_gpus_per_node=4 \\\\\\\\\\\\\\\\\\\\\\\\n    data.rollout_batch_size=128 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.n=20 \\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\n\\\\\\\"},{\\\\\\\"number\\\\\\\":8,\\\\\\\"title\\\\\\\":\\\\\\\"Does GRPO training of VLM support using a Hugging Face model as the rollout-inference model?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Does it support using Hugging Face to load the VLM model for inference during the rollout phase of GRPO training, instead of using the vLLM engine?\\\\\\\\nIs it convenient to make this modification?\\\\\\\"},{\\\\\\\"number\\\\\\\":7,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u8bf7\\\\\\\\u95ee\\\\\\\\u4e0b\\\\\\\\u5956\\\\\\\\u52b1\\\\\\\\u51fd\\\\\\\\u6570\\\\\\\\u80fd\\\\\\\\u4f20\\\\\\\\u5165\\\\\\\\u4e00\\\\\\\\u4e9b\\\\\\\\u5916\\\\\\\\u90e8\\\\\\\\u4fe1\\\\\\\\u606f\\\\\\\\u5417\\\\\\\\uff1f\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5c31\\\\\\\\u7c7b\\\\\\\\u4f3c\\\\\\\\u4e8everl\\\\\\\\u4e2d\\\\\\\\u7684extra_info\\\\\\\"},{\\\\\\\"number\\\\\\\":6,\\\\\\\"title\\\\\\\":\\\\\\\"memory need for training Qwen-2.5VL-instruct-3B?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"as suggested, i use BF16 and training Qwen-2.5VL-instruct-3B on a 48GB L20. To enable bf16 training, I added `torch_dtype: bf16` and `strategy: adamw_bf16` to the `examples/config.yaml` file. However, when I subsequently ran the training command `bash examples/qwen2_5_vl_3b_geo3k_grpo.sh` for the 3B model, I encountered an out-of-memory (OOM) error. I'm using a single L20 GPU with 48GB VRAM. According to the official GitHub project documentation, training the 3B model (BF) should require only 40GB of memory. I'm unclear why this OOM error occurred.\\\\\\\\n\\\\\\\\n<img width=\\\\\\\\\\\\\\\"909\\\\\\\\\\\\\\\" height=\\\\\\\\\\\\\\\"488\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/2665b497-e084-44fb-a7ca-c8ed666d8be6\\\\\\\\\\\\\\\" />\\\\\\\"},{\\\\\\\"number\\\\\\\":5,\\\\\\\"title\\\\\\\":\\\\\\\"Why use qwen2-vl for qwen2.5-vl model?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi! \\\\\\\\nI noticed that your framework supports the training of QWEN2.5-VL, but in dictionary verl/models/transformers, why do you use qwen2-vl for modeling qwen2.5-vl, wouldn't this lead to incompatibility with the 2.5 model function?\\\\\\\"},{\\\\\\\"number\\\\\\\":4,\\\\\\\"title\\\\\\\":\\\\\\\"How to add reward model to compute reward signal?\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I want to load an additional reward model (such as CLIP or a smaller-scale model) to calculate rewards for the response. How should I proceed?\\\\\\\"},{\\\\\\\"number\\\\\\\":3,\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u76ee\\\\\\\\u524d\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u652f\\\\\\\\u6301\\\\\\\\u89c6\\\\\\\\u9891\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u5417\\\\\\\\uff0c\\\\\\\\u6837\\\\\\\\u4f8b\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u53ea\\\\\\\\u770b\\\\\\\\u5230\\\\\\\\u56fe\\\\\\\\u7247\\\\\\\\u6ca1\\\\\\\\u770b\\\\\\\\u5230\\\\\\\\u89c6\\\\\\\\u9891\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u9898\\\\\\\"},{\\\\\\\"number\\\\\\\":2,\\\\\\\"title\\\\\\\":\\\\\\\"Verl version\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Which version of verl is used in the EasyR1 project?\\\\\\\"},{\\\\\\\"number\\\\\\\":1,\\\\\\\"title\\\\\\\":\\\\\\\"Support GSPO\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"GSPO seems to be very effective. \\\\\\\\nWhen will GSPO be supported?\\\\\\\"}],\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Based on my analysis of all the GitHub issues, I found several issues that contain the target keywords:\n\n**Issues containing \"OOM\" (case insensitive):**\n- Issue #9: \"Training Memory Leak at various steps\" - contains \"OOM issue\" and \"OOM error\" \n- Issue #30: \"large rollout.n encounter OOM in training.\" - contains \"OOM error\"\n- Issue #6: \"memory need for training Qwen-2.5VL-instruct-3B?\" - contains \"out-of-memory (OOM) error\"\n\n**Issues containing \"memory\" (case insensitive):**  \n- Issue #9: \"Training Memory Leak at various steps\" - contains \"Training Memory Leak\"\n- Issue #6: \"memory need for training Qwen-2.5VL-instruct-3B?\" - contains \"memory\" in title and \"out-of-memory\" in body\n- Issue #36: \"The training time becomes slower and slower\" - contains \"GPU memory is occupied\"\n- Issue #32: \"训练之前的推理过程卡住\" - contains \"显存占用低\" (low GPU memory usage)\n\n**Issues containing \"batch\" (case insensitive):**\n- Issue #30: \"large rollout.n encounter OOM in training.\" - contains \"batch size\" \n- Issue #9: \"Training Memory Leak at various steps\" - contains \"batch size\" and \"train-batch\"\n- Issue #39: \"GRPO训练速度问题\" - contains \"micro_batch_size_per_device_for_update\" and \"micro_batch_size_per_device_for_experience\"\n\n**Issues containing \"显存\" (GPU memory in Chinese):**\n- Issue #46: \"[功能请求]为实验一开始的推理阶段增加提示\" - contains \"显存错误\" (GPU memory error) \n- Issue #32: \"训练之前的推理过程卡住\" - contains \"显存占用低\" (low GPU memory usage)\n\nNow I'll create the ANALYSIS_RESULTS.json file with all the findings:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"create_or_update_file_contents\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"path\\\": \\\"ANALYSIS_RESULTS.json\\\"}\", \"body_schema\": \"{\\\"message\\\": \\\"Add analysis results for configuration changes investigation\\\", \\\"content\\\": \\\"ewogICJ0YXJnZXRfY29tbWl0X3NoYSI6ICIwOTg5MzE1MzA2MDZkMjJmODY3ZmQxMjFiMWRjYjMyMjVhNDM2NjFmIiwKICAiY29tbWl0X2F1dGhvciI6ICJoaXlvdWdhIiwKICAiY29tbWl0X2RhdGUiOiAiMjAyNS0wOC0wOCIsCiAgInBhcmFtZXRlcl9jaGFuZ2VzIjogewogICAgIm1pY3JvX2JhdGNoX3NpemVfcGVyX2RldmljZV9mb3JfdXBkYXRlIjogewogICAgICAiYmVmb3JlIjogNCwKICAgICAgImFmdGVyIjogMSwKICAgICAgImxpbmVfbnVtYmVyIjogMzcKICAgIH0sCiAgICAibWljcm9fYmF0Y2hfc2l6ZV9wZXJfZGV2aWNlX2Zvcl9leHBlcmllbmNlIjogewogICAgICAiYmVmb3JlIjogMTYsCiAgICAgICJhZnRlciI6IDIsCiAgICAgICJsaW5lX251bWJlciI6IDM4CiAgICB9CiAgfSwKICAicmVsYXRlZF9pc3N1ZV9udW1iZXJfbGlzdCI6IFs5LCA2LCAzMCwgMzYsIDMyLCAzOSwgNDZdCn0=\\\"}\"}",
    "call_id": "toolu_01DD4JpzLyt2sUTPnis11Anw",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01DD4JpzLyt2sUTPnis11Anw",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"create_or_update_file_contents\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"content\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"ANALYSIS_RESULTS.json\\\\\\\",\\\\\\\"path\\\\\\\":\\\\\\\"ANALYSIS_RESULTS.json\\\\\\\",\\\\\\\"sha\\\\\\\":\\\\\\\"7c29832d37936703b6aa731149030abd61b66b38\\\\\\\",\\\\\\\"size\\\\\\\":455,\\\\\\\"url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/ANALYSIS_RESULTS.json?ref=main\\\\\\\",\\\\\\\"html_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/main/ANALYSIS_RESULTS.json\\\\\\\",\\\\\\\"git_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/git/blobs/7c29832d37936703b6aa731149030abd61b66b38\\\\\\\",\\\\\\\"download_url\\\\\\\":\\\\\\\"https://raw.githubusercontent.com/mcpmark-eval-zl/EasyR1/main/ANALYSIS_RESULTS.json?token=BW6O66QAVJZDIGW5SUSLCYLIZWXZU\\\\\\\",\\\\\\\"type\\\\\\\":\\\\\\\"file\\\\\\\",\\\\\\\"_links\\\\\\\":{\\\\\\\"self\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/ANALYSIS_RESULTS.json?ref=main\\\\\\\",\\\\\\\"git\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/git/blobs/7c29832d37936703b6aa731149030abd61b66b38\\\\\\\",\\\\\\\"html\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/main/ANALYSIS_RESULTS.json\\\\\\\"}},\\\\\\\"commit\\\\\\\":{\\\\\\\"sha\\\\\\\":\\\\\\\"f1208babe57c6926d92667db4562dab2e3318e05\\\\\\\",\\\\\\\"node_id\\\\\\\":\\\\\\\"C_kwDOPzPN7NoAKGYxMjA4YmFiZTU3YzY5MjZkOTI2NjdkYjQ1NjJkYWIyZTMzMThlMDU\\\\\\\",\\\\\\\"url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/git/commits/f1208babe57c6926d92667db4562dab2e3318e05\\\\\\\",\\\\\\\"html_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/commit/f1208babe57c6926d92667db4562dab2e3318e05\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"cinjeffreyy-arch\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"cinjeffreyy@gmail.com\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-09-19T19:30:38Z\\\\\\\"},\\\\\\\"committer\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"cinjeffreyy-arch\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"cinjeffreyy@gmail.com\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-09-19T19:30:38Z\\\\\\\"},\\\\\\\"tree\\\\\\\":{\\\\\\\"sha\\\\\\\":\\\\\\\"b8ec671ee4c67a9ebba1e78496951364dfa30f28\\\\\\\",\\\\\\\"url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/git/trees/b8ec671ee4c67a9ebba1e78496951364dfa30f28\\\\\\\"},\\\\\\\"message\\\\\\\":\\\\\\\"Add analysis results for configuration changes investigation\\\\\\\",\\\\\\\"parents\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"855923ac4b41c961d5cb0db590048e15c47d40ea\\\\\\\",\\\\\\\"url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/git/commits/855923ac4b41c961d5cb0db590048e15c47d40ea\\\\\\\",\\\\\\\"html_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/commit/855923ac4b41c961d5cb0db590048e15c47d40ea\\\\\\\"}],\\\\\\\"verification\\\\\\\":{\\\\\\\"verified\\\\\\\":false,\\\\\\\"reason\\\\\\\":\\\\\\\"unsigned\\\\\\\",\\\\\\\"signature\\\\\\\":null,\\\\\\\"payload\\\\\\\":null,\\\\\\\"verified_at\\\\\\\":null}}},\\\\\\\"status_code\\\\\\\":201}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "## Task Completed Successfully! \n\nI have successfully performed a deep investigation into the recent configuration changes in your EasyR1 repository and completed all the required tasks:\n\n### Summary of Findings:\n\n**1. Target Commit Identified:**\n- **Commit SHA:** `098931530606d22f867fd121b1dcb3225a43661f`\n- **Author:** `hiyouga` (Yaowei Zheng)\n- **Date:** `2025-08-08`\n- **Message:** `[misc] fix data proto (#458)`\n\n**2. Parameter Changes Found:**\n- `micro_batch_size_per_device_for_update`: Changed from `4` to `1` (line 37)\n- `micro_batch_size_per_device_for_experience`: Changed from `16` to `2` (line 38)\n\n**3. Related Issues Found:**\nI searched through all GitHub issues and found **7 issues** containing the target keywords:\n- **Issue #9:** \"Training Memory Leak at various steps\" - contains OOM and memory issues\n- **Issue #6:** \"memory need for training Qwen-2.5VL-instruct-3B?\" - contains OOM memory errors\n- **Issue #30:** \"large rollout.n encounter OOM in training\" - contains OOM and batch size issues\n- **Issue #36:** \"The training time becomes slower and slower\" - contains GPU memory usage issues\n- **Issue #32:** \"训练之前的推理过程卡住\" - contains GPU memory (显存) issues\n- **Issue #39:** \"GRPO训练速度问题\" - contains batch size parameter discussions\n- **Issue #46:** \"[功能请求]为实验一开始的推理阶段增加提示\" - contains GPU memory (显存) errors\n\n**4. Results File Created:**\nI have created the `ANALYSIS_RESULTS.json` file in your repository root with all the investigation results, including:\n- The exact 40-character commit SHA\n- Commit author and date information\n- Before/after values for both parameters with line numbers\n- Complete list of all related issue numbers\n\nThe investigation reveals that the micro batch size parameters were reduced significantly (from 4→1 and 16→2), which likely aimed to address the memory-related issues documented in the related GitHub issues.",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  }
]