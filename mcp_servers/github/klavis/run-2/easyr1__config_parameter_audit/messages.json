[
  {
    "content": "Please execute the following task in my repository mcpmark-eval-zl/EasyR1:\n\nI need you to perform a deep investigation into recent configuration changes in our EasyR1 repository that may be causing training instability issues.\n\n## Task Requirements\n\n### 1. Deep Commit Analysis\nFind the exact commit SHA where the `micro_batch_size_per_device_for_update` parameter was changed from `4` to `1` in the `examples/config.yaml` file. Use GitHub API to:\n- Examine recent commits that modified `examples/config.yaml` \n- Get the specific commit diff showing this parameter change\n- Identify the commit author and timestamp\n\n### 2. Related Parameter Investigation  \nIn the same commit you found above, identify what value the `micro_batch_size_per_device_for_experience` parameter was changed to. Document:\n- The before value for this parameter\n- The after value for this parameter  \n- The specific line numbers in the diff where these changes occurred\n\n### 3. Issue Search and Verification\nSearch through all GitHub issues (both open and closed) to find issues that contain specific keywords. Identify all issue numbers where the issue title or body text contains any of these exact terms:\n- \"OOM\" (case insensitive)\n- \"memory\" (case insensitive) \n- \"batch\" (case insensitive)\n- \"显存\" (GPU memory in Chinese)\n\nYou must find and list ALL issues that contain any of these keywords in their titles or bodies, regardless of whether you think they're related to the parameter changes.\n\n### 4. File Creation and Results\nCreate a file named exactly `ANALYSIS_RESULTS.json` in the repository root with this exact structure:\n\n```json\n{\n  \"target_commit_sha\": \"full-40-character-commit-sha\",\n  \"commit_author\": \"author-username\", \n  \"commit_date\": \"YYYY-MM-DD\",\n  \"parameter_changes\": {\n    \"micro_batch_size_per_device_for_update\": {\n      \"before\": 4,\n      \"after\": 1,\n      \"line_number\": 123\n    },\n    \"micro_batch_size_per_device_for_experience\": {\n      \"before\": 16,\n      \"after\": 2, \n      \"line_number\": 124\n    }\n  },\n  \"related_issue_number_list\": [9, 46]\n}\n```\n\n### 5. Verification Requirements\n- The commit SHA must be exactly 40 hexadecimal characters\n- The parameter values must match the actual repository changes  \n- The issue number must reference a real issue in the repository\n- All data must be obtained through GitHub API analysis, not guesswork\n\nNote: Based on your understanding, solve the task all at once by yourself, don't ask for my opinions on anything.",
    "role": "user"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "I'll help you perform a deep investigation into the configuration changes in your EasyR1 repository. Let me start by exploring the GitHub API capabilities and then systematically analyze the commits, parameter changes, and related issues.",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"user_query\": \"GitHub repository commits files issues analysis\", \"server_names\": [\"GitHub\"]}",
    "call_id": "toolu_01Fe5seVcMqTbB9Z4davLxME",
    "name": "discover_server_categories_or_actions",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01Fe5seVcMqTbB9Z4davLxME",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"servers\\\\\\\":{\\\\\\\"GitHub\\\\\\\":{\\\\\\\"detail_level\\\\\\\":\\\\\\\"categories_only\\\\\\\",\\\\\\\"details\\\\\\\":[\\\\\\\"GITHUB_META\\\\\\\",\\\\\\\"GITHUB_SECURITY-ADVISORIES\\\\\\\",\\\\\\\"GITHUB_APPS\\\\\\\",\\\\\\\"GITHUB_CLASSROOM\\\\\\\",\\\\\\\"GITHUB_CODES-OF-CONDUCT\\\\\\\",\\\\\\\"GITHUB_CREDENTIALS\\\\\\\",\\\\\\\"GITHUB_EMOJIS\\\\\\\",\\\\\\\"GITHUB_CODE-SECURITY\\\\\\\",\\\\\\\"GITHUB_DEPENDABOT\\\\\\\",\\\\\\\"GITHUB_SECRET-SCANNING\\\\\\\",\\\\\\\"GITHUB_ACTIVITY\\\\\\\",\\\\\\\"GITHUB_GISTS\\\\\\\",\\\\\\\"GITHUB_GITIGNORE\\\\\\\",\\\\\\\"GITHUB_ISSUES\\\\\\\",\\\\\\\"GITHUB_LICENSES\\\\\\\",\\\\\\\"GITHUB_MARKDOWN\\\\\\\",\\\\\\\"GITHUB_ORGS\\\\\\\",\\\\\\\"GITHUB_BILLING\\\\\\\",\\\\\\\"GITHUB_ACTIONS\\\\\\\",\\\\\\\"GITHUB_OIDC\\\\\\\",\\\\\\\"GITHUB_CAMPAIGNS\\\\\\\",\\\\\\\"GITHUB_CODE-SCANNING\\\\\\\",\\\\\\\"GITHUB_CODESPACES\\\\\\\",\\\\\\\"GITHUB_COPILOT\\\\\\\",\\\\\\\"GITHUB_PACKAGES\\\\\\\",\\\\\\\"GITHUB_INTERACTIONS\\\\\\\",\\\\\\\"GITHUB_MIGRATIONS\\\\\\\",\\\\\\\"GITHUB_PRIVATE-REGISTRIES\\\\\\\",\\\\\\\"GITHUB_PROJECTS-CLASSIC\\\\\\\",\\\\\\\"GITHUB_REPOS\\\\\\\",\\\\\\\"GITHUB_HOSTED-COMPUTE\\\\\\\",\\\\\\\"GITHUB_TEAMS\\\\\\\",\\\\\\\"GITHUB_REACTIONS\\\\\\\",\\\\\\\"GITHUB_RATE-LIMIT\\\\\\\",\\\\\\\"GITHUB_CHECKS\\\\\\\",\\\\\\\"GITHUB_DEPENDENCY-GRAPH\\\\\\\",\\\\\\\"GITHUB_GIT\\\\\\\",\\\\\\\"GITHUB_PULLS\\\\\\\",\\\\\\\"GITHUB_SEARCH\\\\\\\",\\\\\\\"GITHUB_USERS\\\\\\\"]}}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Let me get the actions for the relevant categories - repositories, issues, and search:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"category_names\": [\"GITHUB_REPOS\", \"GITHUB_ISSUES\", \"GITHUB_SEARCH\"]}",
    "call_id": "toolu_01XxXsDCmuWaP5JZBdS5RLaf",
    "name": "get_category_actions",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01XxXsDCmuWaP5JZBdS5RLaf",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"category_actions\\\\\\\":{\\\\\\\"GITHUB_REPOS\\\\\\\":[\\\\\\\"list_for_org\\\\\\\",\\\\\\\"create_in_org\\\\\\\",\\\\\\\"get_org_rulesets\\\\\\\",\\\\\\\"create_org_ruleset\\\\\\\",\\\\\\\"get_org_rule_suites\\\\\\\",\\\\\\\"get_org_rule_suite\\\\\\\",\\\\\\\"get_org_ruleset\\\\\\\",\\\\\\\"update_org_ruleset\\\\\\\",\\\\\\\"delete_org_ruleset\\\\\\\",\\\\\\\"get\\\\\\\",\\\\\\\"update\\\\\\\",\\\\\\\"delete\\\\\\\",\\\\\\\"list_activities\\\\\\\",\\\\\\\"create_attestation\\\\\\\",\\\\\\\"list_attestations\\\\\\\",\\\\\\\"list_autolinks\\\\\\\",\\\\\\\"create_autolink\\\\\\\",\\\\\\\"get_autolink\\\\\\\",\\\\\\\"delete_autolink\\\\\\\",\\\\\\\"check_automated_security_fixes\\\\\\\",\\\\\\\"enable_automated_security_fixes\\\\\\\",\\\\\\\"disable_automated_security_fixes\\\\\\\",\\\\\\\"list_branches\\\\\\\",\\\\\\\"get_branch\\\\\\\",\\\\\\\"get_branch_protection\\\\\\\",\\\\\\\"update_branch_protection\\\\\\\",\\\\\\\"delete_branch_protection\\\\\\\",\\\\\\\"get_admin_branch_protection\\\\\\\",\\\\\\\"set_admin_branch_protection\\\\\\\",\\\\\\\"delete_admin_branch_protection\\\\\\\",\\\\\\\"get_pull_request_review_protection\\\\\\\",\\\\\\\"update_pull_request_review_protection\\\\\\\",\\\\\\\"delete_pull_request_review_protection\\\\\\\",\\\\\\\"get_commit_signature_protection\\\\\\\",\\\\\\\"create_commit_signature_protection\\\\\\\",\\\\\\\"delete_commit_signature_protection\\\\\\\",\\\\\\\"get_status_checks_protection\\\\\\\",\\\\\\\"update_status_check_protection\\\\\\\",\\\\\\\"remove_status_check_protection\\\\\\\",\\\\\\\"get_all_status_check_contexts\\\\\\\",\\\\\\\"add_status_check_contexts\\\\\\\",\\\\\\\"set_status_check_contexts\\\\\\\",\\\\\\\"remove_status_check_contexts\\\\\\\",\\\\\\\"get_access_restrictions\\\\\\\",\\\\\\\"delete_access_restrictions\\\\\\\",\\\\\\\"get_apps_with_access_to_protected_branch\\\\\\\",\\\\\\\"add_app_access_restrictions\\\\\\\",\\\\\\\"set_app_access_restrictions\\\\\\\",\\\\\\\"remove_app_access_restrictions\\\\\\\",\\\\\\\"get_teams_with_access_to_protected_branch\\\\\\\",\\\\\\\"add_team_access_restrictions\\\\\\\",\\\\\\\"set_team_access_restrictions\\\\\\\",\\\\\\\"remove_team_access_restrictions\\\\\\\",\\\\\\\"get_users_with_access_to_protected_branch\\\\\\\",\\\\\\\"add_user_access_restrictions\\\\\\\",\\\\\\\"set_user_access_restrictions\\\\\\\",\\\\\\\"remove_user_access_restrictions\\\\\\\",\\\\\\\"rename_branch\\\\\\\",\\\\\\\"codeowners_errors\\\\\\\",\\\\\\\"list_collaborators\\\\\\\",\\\\\\\"check_collaborator\\\\\\\",\\\\\\\"add_collaborator\\\\\\\",\\\\\\\"remove_collaborator\\\\\\\",\\\\\\\"get_collaborator_permission_level\\\\\\\",\\\\\\\"list_commit_comments_for_repo\\\\\\\",\\\\\\\"get_commit_comment\\\\\\\",\\\\\\\"update_commit_comment\\\\\\\",\\\\\\\"delete_commit_comment\\\\\\\",\\\\\\\"list_commits\\\\\\\",\\\\\\\"list_branches_for_head_commit\\\\\\\",\\\\\\\"list_comments_for_commit\\\\\\\",\\\\\\\"create_commit_comment\\\\\\\",\\\\\\\"list_pull_requests_associated_with_commit\\\\\\\",\\\\\\\"get_commit\\\\\\\",\\\\\\\"get_combined_status_for_ref\\\\\\\",\\\\\\\"list_commit_statuses_for_ref\\\\\\\",\\\\\\\"get_community_profile_metrics\\\\\\\",\\\\\\\"compare_commits\\\\\\\",\\\\\\\"get_content\\\\\\\",\\\\\\\"create_or_update_file_contents\\\\\\\",\\\\\\\"delete_file\\\\\\\",\\\\\\\"list_contributors\\\\\\\",\\\\\\\"list_deployments\\\\\\\",\\\\\\\"create_deployment\\\\\\\",\\\\\\\"get_deployment\\\\\\\",\\\\\\\"delete_deployment\\\\\\\",\\\\\\\"list_deployment_statuses\\\\\\\",\\\\\\\"create_deployment_status\\\\\\\",\\\\\\\"get_deployment_status\\\\\\\",\\\\\\\"create_dispatch_event\\\\\\\",\\\\\\\"get_all_environments\\\\\\\",\\\\\\\"get_environment\\\\\\\",\\\\\\\"create_or_update_environment\\\\\\\",\\\\\\\"delete_an_environment\\\\\\\",\\\\\\\"list_deployment_branch_policies\\\\\\\",\\\\\\\"create_deployment_branch_policy\\\\\\\",\\\\\\\"get_deployment_branch_policy\\\\\\\",\\\\\\\"update_deployment_branch_policy\\\\\\\",\\\\\\\"delete_deployment_branch_policy\\\\\\\",\\\\\\\"get_all_deployment_protection_rules\\\\\\\",\\\\\\\"create_deployment_protection_rule\\\\\\\",\\\\\\\"list_custom_deployment_rule_integrations\\\\\\\",\\\\\\\"get_custom_deployment_protection_rule\\\\\\\",\\\\\\\"disable_deployment_protection_rule\\\\\\\",\\\\\\\"list_forks\\\\\\\",\\\\\\\"create_fork\\\\\\\",\\\\\\\"list_webhooks\\\\\\\",\\\\\\\"create_webhook\\\\\\\",\\\\\\\"get_webhook\\\\\\\",\\\\\\\"update_webhook\\\\\\\",\\\\\\\"delete_webhook\\\\\\\",\\\\\\\"get_webhook_config_for_repo\\\\\\\",\\\\\\\"update_webhook_config_for_repo\\\\\\\",\\\\\\\"list_webhook_deliveries\\\\\\\",\\\\\\\"get_webhook_delivery\\\\\\\",\\\\\\\"redeliver_webhook_delivery\\\\\\\",\\\\\\\"ping_webhook\\\\\\\",\\\\\\\"test_push_webhook\\\\\\\",\\\\\\\"list_invitations\\\\\\\",\\\\\\\"update_invitation\\\\\\\",\\\\\\\"delete_invitation\\\\\\\",\\\\\\\"list_deploy_keys\\\\\\\",\\\\\\\"create_deploy_key\\\\\\\",\\\\\\\"get_deploy_key\\\\\\\",\\\\\\\"delete_deploy_key\\\\\\\",\\\\\\\"list_languages\\\\\\\",\\\\\\\"merge_upstream\\\\\\\",\\\\\\\"merge\\\\\\\",\\\\\\\"get_pages\\\\\\\",\\\\\\\"create_pages_site\\\\\\\",\\\\\\\"update_information_about_pages_site\\\\\\\",\\\\\\\"delete_pages_site\\\\\\\",\\\\\\\"list_pages_builds\\\\\\\",\\\\\\\"request_pages_build\\\\\\\",\\\\\\\"get_latest_pages_build\\\\\\\",\\\\\\\"get_pages_build\\\\\\\",\\\\\\\"create_pages_deployment\\\\\\\",\\\\\\\"get_pages_deployment\\\\\\\",\\\\\\\"cancel_pages_deployment\\\\\\\",\\\\\\\"get_pages_health_check\\\\\\\",\\\\\\\"check_private_vulnerability_reporting\\\\\\\",\\\\\\\"enable_private_vulnerability_reporting\\\\\\\",\\\\\\\"disable_private_vulnerability_reporting\\\\\\\",\\\\\\\"get_custom_properties_values\\\\\\\",\\\\\\\"create_or_update_custom_properties_values\\\\\\\",\\\\\\\"get_readme\\\\\\\",\\\\\\\"get_readme_in_directory\\\\\\\",\\\\\\\"list_releases\\\\\\\",\\\\\\\"create_release\\\\\\\",\\\\\\\"get_release_asset\\\\\\\",\\\\\\\"update_release_asset\\\\\\\",\\\\\\\"delete_release_asset\\\\\\\",\\\\\\\"generate_release_notes\\\\\\\",\\\\\\\"get_latest_release\\\\\\\",\\\\\\\"get_release_by_tag\\\\\\\",\\\\\\\"get_release\\\\\\\",\\\\\\\"update_release\\\\\\\",\\\\\\\"delete_release\\\\\\\",\\\\\\\"list_release_assets\\\\\\\",\\\\\\\"upload_release_asset\\\\\\\",\\\\\\\"get_branch_rules\\\\\\\",\\\\\\\"get_repo_rulesets\\\\\\\",\\\\\\\"create_repo_ruleset\\\\\\\",\\\\\\\"get_repo_rule_suites\\\\\\\",\\\\\\\"get_repo_rule_suite\\\\\\\",\\\\\\\"get_repo_ruleset\\\\\\\",\\\\\\\"update_repo_ruleset\\\\\\\",\\\\\\\"delete_repo_ruleset\\\\\\\",\\\\\\\"get_repo_ruleset_history\\\\\\\",\\\\\\\"get_repo_ruleset_version\\\\\\\",\\\\\\\"get_code_frequency_stats\\\\\\\",\\\\\\\"get_commit_activity_stats\\\\\\\",\\\\\\\"get_contributors_stats\\\\\\\",\\\\\\\"get_participation_stats\\\\\\\",\\\\\\\"get_punch_card_stats\\\\\\\",\\\\\\\"create_commit_status\\\\\\\",\\\\\\\"list_tags\\\\\\\",\\\\\\\"list_tag_protection\\\\\\\",\\\\\\\"create_tag_protection\\\\\\\",\\\\\\\"delete_tag_protection\\\\\\\",\\\\\\\"download_tarball_archive\\\\\\\",\\\\\\\"list_teams\\\\\\\",\\\\\\\"get_all_topics\\\\\\\",\\\\\\\"replace_all_topics\\\\\\\",\\\\\\\"get_clones\\\\\\\",\\\\\\\"get_top_paths\\\\\\\",\\\\\\\"get_top_referrers\\\\\\\",\\\\\\\"get_views\\\\\\\",\\\\\\\"transfer\\\\\\\",\\\\\\\"check_vulnerability_alerts\\\\\\\",\\\\\\\"enable_vulnerability_alerts\\\\\\\",\\\\\\\"disable_vulnerability_alerts\\\\\\\",\\\\\\\"download_zipball_archive\\\\\\\",\\\\\\\"create_using_template\\\\\\\",\\\\\\\"list_public\\\\\\\",\\\\\\\"list_for_authenticated_user\\\\\\\",\\\\\\\"create_for_authenticated_user\\\\\\\",\\\\\\\"list_invitations_for_authenticated_user\\\\\\\",\\\\\\\"accept_invitation_for_authenticated_user\\\\\\\",\\\\\\\"decline_invitation_for_authenticated_user\\\\\\\",\\\\\\\"list_for_user\\\\\\\"],\\\\\\\"GITHUB_ISSUES\\\\\\\":[\\\\\\\"list\\\\\\\",\\\\\\\"list_for_org\\\\\\\",\\\\\\\"list_assignees\\\\\\\",\\\\\\\"check_user_can_be_assigned\\\\\\\",\\\\\\\"list_for_repo\\\\\\\",\\\\\\\"create\\\\\\\",\\\\\\\"list_comments_for_repo\\\\\\\",\\\\\\\"get_comment\\\\\\\",\\\\\\\"update_comment\\\\\\\",\\\\\\\"delete_comment\\\\\\\",\\\\\\\"list_events_for_repo\\\\\\\",\\\\\\\"get_event\\\\\\\",\\\\\\\"get\\\\\\\",\\\\\\\"update\\\\\\\",\\\\\\\"add_assignees\\\\\\\",\\\\\\\"remove_assignees\\\\\\\",\\\\\\\"check_user_can_be_assigned_to_issue\\\\\\\",\\\\\\\"list_comments\\\\\\\",\\\\\\\"create_comment\\\\\\\",\\\\\\\"list_dependencies_blocked_by\\\\\\\",\\\\\\\"add_blocked_by_dependency\\\\\\\",\\\\\\\"remove_dependency_blocked_by\\\\\\\",\\\\\\\"list_dependencies_blocking\\\\\\\",\\\\\\\"list_events\\\\\\\",\\\\\\\"list_labels_on_issue\\\\\\\",\\\\\\\"add_labels\\\\\\\",\\\\\\\"set_labels\\\\\\\",\\\\\\\"remove_all_labels\\\\\\\",\\\\\\\"remove_label\\\\\\\",\\\\\\\"lock\\\\\\\",\\\\\\\"unlock\\\\\\\",\\\\\\\"get_parent\\\\\\\",\\\\\\\"remove_sub_issue\\\\\\\",\\\\\\\"list_sub_issues\\\\\\\",\\\\\\\"add_sub_issue\\\\\\\",\\\\\\\"reprioritize_sub_issue\\\\\\\",\\\\\\\"list_events_for_timeline\\\\\\\",\\\\\\\"list_labels_for_repo\\\\\\\",\\\\\\\"create_label\\\\\\\",\\\\\\\"get_label\\\\\\\",\\\\\\\"update_label\\\\\\\",\\\\\\\"delete_label\\\\\\\",\\\\\\\"list_milestones\\\\\\\",\\\\\\\"create_milestone\\\\\\\",\\\\\\\"get_milestone\\\\\\\",\\\\\\\"update_milestone\\\\\\\",\\\\\\\"delete_milestone\\\\\\\",\\\\\\\"list_labels_for_milestone\\\\\\\",\\\\\\\"list_for_authenticated_user\\\\\\\"],\\\\\\\"GITHUB_SEARCH\\\\\\\":[\\\\\\\"code\\\\\\\",\\\\\\\"commits\\\\\\\",\\\\\\\"issues_and_pull_requests\\\\\\\",\\\\\\\"labels\\\\\\\",\\\\\\\"repos\\\\\\\",\\\\\\\"topics\\\\\\\",\\\\\\\"users\\\\\\\"]}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Now let me get the details for the specific actions I need:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"category_name\": \"GITHUB_REPOS\", \"action_name\": \"list_commits\"}",
    "call_id": "toolu_01393DbiQRU7RF4XfvV6SQXR",
    "name": "get_action_details",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01393DbiQRU7RF4XfvV6SQXR",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"name\\\\\\\":\\\\\\\"list_commits\\\\\\\",\\\\\\\"method\\\\\\\":\\\\\\\"GET\\\\\\\",\\\\\\\"path\\\\\\\":\\\\\\\"/repos/{owner}/{repo}/commits\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"**Signature verification object**\\\\\\\\n\\\\\\\\nThe response will include a `verification` object that describes the result of verifying the commit's signature. The following fields are included in the `verification` object:\\\\\\\\n\\\\\\\\n| Name | Type | Description |\\\\\\\\n| ---- | ---- | ----------- |\\\\\\\\n| `verified` | `boolean` | Indicates whether GitHub considers the signature in this commit to be verified. |\\\\\\\\n| `reason` | `string` | The reason for verified value. Possible values and their meanings are enumerated in table below. |\\\\\\\\n| `signature` | `string` | The signature that was extracted from the commit. |\\\\\\\\n| `payload` | `string` | The value that was signed. |\\\\\\\\n| `verified_at` | `string` | The date the signature was verified by GitHub. |\\\\\\\\n\\\\\\\\nThese are the possible values for `reason` in the `verification` object:\\\\\\\\n\\\\\\\\n| Value | Description |\\\\\\\\n| ----- | ----------- |\\\\\\\\n| `expired_key` | The key that made the signature is expired. |\\\\\\\\n| `not_signing_key` | The \\\\\\\\\\\\\\\"signing\\\\\\\\\\\\\\\" flag is not among the usage flags in the GPG key that made the signature. |\\\\\\\\n| `gpgverify_error` | There was an error communicating with the signature verification service. |\\\\\\\\n| `gpgverify_unavailable` | The signature verification service is currently unavailable. |\\\\\\\\n| `unsigned` | The object does not include a signature. |\\\\\\\\n| `unknown_signature_type` | A non-PGP signature was found in the commit. |\\\\\\\\n| `no_user` | No user was associated with the `committer` email address in the commit. |\\\\\\\\n| `unverified_email` | The `committer` email address in the commit was associated with a user, but the email address is not verified on their account. |\\\\\\\\n| `bad_email` | The `committer` email address in the commit is not included in the identities of the PGP key that made the signature. |\\\\\\\\n| `unknown_key` | The key that made the signature has not been registered with any user's account. |\\\\\\\\n| `malformed_signature` | There was an error parsing the signature. |\\\\\\\\n| `invalid` | The signature could not be cryptographically verified using the key whose key-id was found in the signature. |\\\\\\\\n| `valid` | None of the above errors applied, so the signature is considered to be verified. |\\\\\\\",\\\\\\\"tags\\\\\\\":[\\\\\\\"repos\\\\\\\"],\\\\\\\"path_params\\\\\\\":{\\\\\\\"owner\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The account owner of the repository. The name is not case sensitive.\\\\\\\"},\\\\\\\"repo\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The name of the repository without the `.git` extension. The name is not case sensitive.\\\\\\\"}},\\\\\\\"query_params\\\\\\\":{\\\\\\\"sha\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"SHA or branch to start listing commits from. Default: the repository\\\\\\\\u2019s default branch (usually `main`).\\\\\\\"},\\\\\\\"path\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"Only commits containing this file path will be returned.\\\\\\\"},\\\\\\\"author\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"GitHub username or email address to use to filter by commit author.\\\\\\\"},\\\\\\\"committer\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"GitHub username or email address to use to filter by commit committer.\\\\\\\"},\\\\\\\"since\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":\\\\\\\"date-time\\\\\\\",\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"Only show results that were last updated after the given time. This is a timestamp in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) format: `YYYY-MM-DDTHH:MM:SSZ`. Due to limitations of Git, timestamps must be between 1970-01-01 and 2099-12-31 (inclusive) or unexpected results may be returned.\\\\\\\"},\\\\\\\"until\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":\\\\\\\"date-time\\\\\\\",\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"Only commits before this date will be returned. This is a timestamp in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) format: `YYYY-MM-DDTHH:MM:SSZ`. Due to limitations of Git, timestamps must be between 1970-01-01 and 2099-12-31 (inclusive) or unexpected results may be returned.\\\\\\\"},\\\\\\\"per_page\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The number of results per page (max 100). For more information, see \\\\\\\\\\\\\\\"[Using pagination in the REST API](https://docs.github.com/rest/using-the-rest-api/using-pagination-in-the-rest-api).\\\\\\\\\\\\\\\"\\\\\\\"},\\\\\\\"page\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The page number of the results to fetch. For more information, see \\\\\\\\\\\\\\\"[Using pagination in the REST API](https://docs.github.com/rest/using-the-rest-api/using-pagination-in-the-rest-api).\\\\\\\\\\\\\\\"\\\\\\\"}},\\\\\\\"body_schema\\\\\\\":{},\\\\\\\"response_schema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"items\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Commit\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"sha\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"node_id\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"html_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"comments_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"commit\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"author\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"null\\\\\\\",\\\\\\\"object\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"committer\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"null\\\\\\\",\\\\\\\"object\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"parents\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"stats\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"files\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"_truncated\\\\\\\":true}},\\\\\\\"required\\\\\\\":[\\\\\\\"url\\\\\\\",\\\\\\\"sha\\\\\\\",\\\\\\\"node_id\\\\\\\",\\\\\\\"html_url\\\\\\\",\\\\\\\"comments_url\\\\\\\",\\\\\\\"commit\\\\\\\",\\\\\\\"author\\\\\\\",\\\\\\\"committer\\\\\\\",\\\\\\\"parents\\\\\\\"]}}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "arguments": "{\"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\"}",
    "call_id": "toolu_018vDLgVwJ37oj1df2HGvUsk",
    "name": "get_action_details",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_018vDLgVwJ37oj1df2HGvUsk",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"method\\\\\\\":\\\\\\\"GET\\\\\\\",\\\\\\\"path\\\\\\\":\\\\\\\"/repos/{owner}/{repo}/commits/{ref}\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Returns the contents of a single commit reference. You must have `read` access for the repository to use this endpoint.\\\\\\\\n\\\\\\\\n> [!NOTE]\\\\\\\\n> If there are more than 300 files in the commit diff and the default JSON media type is requested, the response will include pagination link headers for the remaining files, up to a limit of 3000 files. Each page contains the static commit information, and the only changes are to the file listing.\\\\\\\\n\\\\\\\\nThis endpoint supports the following custom media types. For more information, see \\\\\\\\\\\\\\\"[Media types](https://docs.github.com/rest/using-the-rest-api/getting-started-with-the-rest-api#media-types).\\\\\\\\\\\\\\\" Pagination query parameters are not supported for these media types.\\\\\\\\n\\\\\\\\n- **`application/vnd.github.diff`**: Returns the diff of the commit. Larger diffs may time out and return a 5xx status code.\\\\\\\\n- **`application/vnd.github.patch`**: Returns the patch of the commit. Diffs with binary data will have no `patch` property. Larger diffs may time out and return a 5xx status code.\\\\\\\\n- **`application/vnd.github.sha`**: Returns the commit's SHA-1 hash. You can use this endpoint to check if a remote reference's SHA-1 hash is the same as your local reference's SHA-1 hash by providing the local SHA-1 reference as the ETag.\\\\\\\\n\\\\\\\\n**Signature verification object**\\\\\\\\n\\\\\\\\nThe response will include a `verification` object that describes the result of verifying the commit's signature. The following fields are included in the `verification` object:\\\\\\\\n\\\\\\\\n| Name | Type | Description |\\\\\\\\n| ---- | ---- | ----------- |\\\\\\\\n| `verified` | `boolean` | Indicates whether GitHub considers the signature in this commit to be verified. |\\\\\\\\n| `reason` | `string` | The reason for verified value. Possible values and their meanings are enumerated in table below. |\\\\\\\\n| `signature` | `string` | The signature that was extracted from the commit. |\\\\\\\\n| `payload` | `string` | The value that was signed. |\\\\\\\\n| `verified_at` | `string` | The date the signature was verified by GitHub. |\\\\\\\\n\\\\\\\\nThese are the possible values for `reason` in the `verification` object:\\\\\\\\n\\\\\\\\n| Value | Description |\\\\\\\\n| ----- | ----------- |\\\\\\\\n| `expired_key` | The key that made the signature is expired. |\\\\\\\\n| `not_signing_key` | The \\\\\\\\\\\\\\\"signing\\\\\\\\\\\\\\\" flag is not among the usage flags in the GPG key that made the signature. |\\\\\\\\n| `gpgverify_error` | There was an error communicating with the signature verification service. |\\\\\\\\n| `gpgverify_unavailable` | The signature verification service is currently unavailable. |\\\\\\\\n| `unsigned` | The object does not include a signature. |\\\\\\\\n| `unknown_signature_type` | A non-PGP signature was found in the commit. |\\\\\\\\n| `no_user` | No user was associated with the `committer` email address in the commit. |\\\\\\\\n| `unverified_email` | The `committer` email address in the commit was associated with a user, but the email address is not verified on their account. |\\\\\\\\n| `bad_email` | The `committer` email address in the commit is not included in the identities of the PGP key that made the signature. |\\\\\\\\n| `unknown_key` | The key that made the signature has not been registered with any user's account. |\\\\\\\\n| `malformed_signature` | There was an error parsing the signature. |\\\\\\\\n| `invalid` | The signature could not be cryptographically verified using the key whose key-id was found in the signature. |\\\\\\\\n| `valid` | None of the above errors applied, so the signature is considered to be verified. |\\\\\\\",\\\\\\\"tags\\\\\\\":[\\\\\\\"repos\\\\\\\"],\\\\\\\"path_params\\\\\\\":{\\\\\\\"owner\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The account owner of the repository. The name is not case sensitive.\\\\\\\"},\\\\\\\"repo\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The name of the repository without the `.git` extension. The name is not case sensitive.\\\\\\\"},\\\\\\\"ref\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The commit reference. Can be a commit SHA, branch name (`heads/BRANCH_NAME`), or tag name (`tags/TAG_NAME`). For more information, see \\\\\\\\\\\\\\\"[Git References](https://git-scm.com/book/en/v2/Git-Internals-Git-References)\\\\\\\\\\\\\\\" in the Git documentation.\\\\\\\"}},\\\\\\\"query_params\\\\\\\":{\\\\\\\"page\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The page number of the results to fetch. For more information, see \\\\\\\\\\\\\\\"[Using pagination in the REST API](https://docs.github.com/rest/using-the-rest-api/using-pagination-in-the-rest-api).\\\\\\\\\\\\\\\"\\\\\\\"},\\\\\\\"per_page\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The number of results per page (max 100). For more information, see \\\\\\\\\\\\\\\"[Using pagination in the REST API](https://docs.github.com/rest/using-the-rest-api/using-pagination-in-the-rest-api).\\\\\\\\\\\\\\\"\\\\\\\"}},\\\\\\\"body_schema\\\\\\\":{},\\\\\\\"response_schema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Commit\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"sha\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"node_id\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"html_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"comments_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"commit\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"author\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"null\\\\\\\",\\\\\\\"object\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"committer\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"null\\\\\\\",\\\\\\\"object\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"parents\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"stats\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"files\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"_truncated\\\\\\\":true}},\\\\\\\"required\\\\\\\":[\\\\\\\"url\\\\\\\",\\\\\\\"sha\\\\\\\",\\\\\\\"node_id\\\\\\\",\\\\\\\"html_url\\\\\\\",\\\\\\\"comments_url\\\\\\\",\\\\\\\"commit\\\\\\\",\\\\\\\"author\\\\\\\",\\\\\\\"committer\\\\\\\",\\\\\\\"parents\\\\\\\"]}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "arguments": "{\"category_name\": \"GITHUB_ISSUES\", \"action_name\": \"list_for_repo\"}",
    "call_id": "toolu_014WC1cttgNpboPi1kEYVfro",
    "name": "get_action_details",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_014WC1cttgNpboPi1kEYVfro",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"name\\\\\\\":\\\\\\\"list_for_repo\\\\\\\",\\\\\\\"method\\\\\\\":\\\\\\\"GET\\\\\\\",\\\\\\\"path\\\\\\\":\\\\\\\"/repos/{owner}/{repo}/issues\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"List issues in a repository. Only open issues will be listed.\\\\\\\\n\\\\\\\\n> [!NOTE]\\\\\\\\n> GitHub's REST API considers every pull request an issue, but not every issue is a pull request. For this reason, \\\\\\\\\\\\\\\"Issues\\\\\\\\\\\\\\\" endpoints may return both issues and pull requests in the response. You can identify pull requests by the `pull_request` key. Be aware that the `id` of a pull request returned from \\\\\\\\\\\\\\\"Issues\\\\\\\\\\\\\\\" endpoints will be an _issue id_. To find out the pull request id, use the \\\\\\\\\\\\\\\"[List pull requests](https://docs.github.com/rest/pulls/pulls#list-pull-requests)\\\\\\\\\\\\\\\" endpoint.\\\\\\\\n\\\\\\\\nThis endpoint supports the following custom media types. For more information, see \\\\\\\\\\\\\\\"[Media types](https://docs.github.com/rest/using-the-rest-api/getting-started-with-the-rest-api#media-types).\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n- **`application/vnd.github.raw+json`**: Returns the raw markdown body. Response will include `body`. This is the default if you do not pass any specific media type.\\\\\\\\n- **`application/vnd.github.text+json`**: Returns a text only representation of the markdown body. Response will include `body_text`.\\\\\\\\n- **`application/vnd.github.html+json`**: Returns HTML rendered from the body's markdown. Response will include `body_html`.\\\\\\\\n- **`application/vnd.github.full+json`**: Returns raw, text, and HTML representations. Response will include `body`, `body_text`, and `body_html`.\\\\\\\",\\\\\\\"tags\\\\\\\":[\\\\\\\"issues\\\\\\\"],\\\\\\\"path_params\\\\\\\":{\\\\\\\"owner\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The account owner of the repository. The name is not case sensitive.\\\\\\\"},\\\\\\\"repo\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The name of the repository without the `.git` extension. The name is not case sensitive.\\\\\\\"}},\\\\\\\"query_params\\\\\\\":{\\\\\\\"milestone\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"If an `integer` is passed, it should refer to a milestone by its `number` field. If the string `*` is passed, issues with any milestone are accepted. If the string `none` is passed, issues without milestones are returned.\\\\\\\"},\\\\\\\"state\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"Indicates the state of the issues to return.\\\\\\\"},\\\\\\\"assignee\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"Can be the name of a user. Pass in `none` for issues with no assigned user, and `*` for issues assigned to any user.\\\\\\\"},\\\\\\\"type\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"Can be the name of an issue type. If the string `*` is passed, issues with any type are accepted. If the string `none` is passed, issues without type are returned.\\\\\\\"},\\\\\\\"creator\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The user that created the issue.\\\\\\\"},\\\\\\\"mentioned\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"A user that's mentioned in the issue.\\\\\\\"},\\\\\\\"labels\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"A list of comma separated label names. Example: `bug,ui,@high`\\\\\\\"},\\\\\\\"sort\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"What to sort results by.\\\\\\\"},\\\\\\\"direction\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The direction to sort the results by.\\\\\\\"},\\\\\\\"since\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":\\\\\\\"date-time\\\\\\\",\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"Only show results that were last updated after the given time. This is a timestamp in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) format: `YYYY-MM-DDTHH:MM:SSZ`.\\\\\\\"},\\\\\\\"per_page\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The number of results per page (max 100). For more information, see \\\\\\\\\\\\\\\"[Using pagination in the REST API](https://docs.github.com/rest/using-the-rest-api/using-pagination-in-the-rest-api).\\\\\\\\\\\\\\\"\\\\\\\"},\\\\\\\"page\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":false,\\\\\\\"description\\\\\\\":\\\\\\\"The page number of the results to fetch. For more information, see \\\\\\\\\\\\\\\"[Using pagination in the REST API](https://docs.github.com/rest/using-the-rest-api/using-pagination-in-the-rest-api).\\\\\\\\\\\\\\\"\\\\\\\"}},\\\\\\\"body_schema\\\\\\\":{},\\\\\\\"response_schema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"items\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Issues are a great way to keep track of tasks, enhancements, and bugs for your projects.\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"id\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"node_id\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"URL for the issue\\\\\\\"},\\\\\\\"repository_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"labels_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"comments_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"events_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"html_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"number\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"Number uniquely identifying the issue within its repository\\\\\\\"},\\\\\\\"state\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"State of the issue; either 'open' or 'closed'\\\\\\\"},\\\\\\\"state_reason\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"string\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The reason for the current state\\\\\\\"},\\\\\\\"title\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"Title of the issue\\\\\\\"},\\\\\\\"body\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"string\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"Contents of the issue\\\\\\\"},\\\\\\\"user\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"labels\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"Labels to associate with this issue; pass one or more label names to replace the set of labels on this issue; send an empty array to clear all labels from the issue; note that the labels are silently dropped for users without push access to the repository\\\\\\\"},\\\\\\\"assignee\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"assignees\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"array\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"milestone\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"locked\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"boolean\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"active_lock_reason\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"string\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"comments\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"integer\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"pull_request\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"closed_at\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"string\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"created_at\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"updated_at\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"draft\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"boolean\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"closed_by\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"body_html\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"body_text\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"timeline_url\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"type\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"object\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The type of issue.\\\\\\\"},\\\\\\\"repository\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"A repository on GitHub.\\\\\\\"},\\\\\\\"performed_via_github_app\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"author_association\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"How the author is associated with the repository.\\\\\\\"},\\\\\\\"reactions\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"sub_issues_summary\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"parent_issue_url\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"string\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"URL to get the parent issue of this issue, if it is a sub-issue\\\\\\\"},\\\\\\\"issue_dependencies_summary\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"issue_field_values\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"array\\\\\\\",\\\\\\\"_truncated\\\\\\\":true}},\\\\\\\"required\\\\\\\":[\\\\\\\"assignee\\\\\\\",\\\\\\\"closed_at\\\\\\\",\\\\\\\"comments\\\\\\\",\\\\\\\"comments_url\\\\\\\",\\\\\\\"events_url\\\\\\\",\\\\\\\"html_url\\\\\\\",\\\\\\\"id\\\\\\\",\\\\\\\"node_id\\\\\\\",\\\\\\\"labels\\\\\\\",\\\\\\\"labels_url\\\\\\\",\\\\\\\"milestone\\\\\\\",\\\\\\\"number\\\\\\\",\\\\\\\"repository_url\\\\\\\",\\\\\\\"state\\\\\\\",\\\\\\\"locked\\\\\\\",\\\\\\\"title\\\\\\\",\\\\\\\"url\\\\\\\",\\\\\\\"user\\\\\\\",\\\\\\\"author_association\\\\\\\",\\\\\\\"created_at\\\\\\\",\\\\\\\"updated_at\\\\\\\"]}}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "arguments": "{\"category_name\": \"GITHUB_REPOS\", \"action_name\": \"create_or_update_file_contents\"}",
    "call_id": "toolu_01AzBWMqLuUN3Hz18EJswqNd",
    "name": "get_action_details",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01AzBWMqLuUN3Hz18EJswqNd",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"name\\\\\\\":\\\\\\\"create_or_update_file_contents\\\\\\\",\\\\\\\"method\\\\\\\":\\\\\\\"PUT\\\\\\\",\\\\\\\"path\\\\\\\":\\\\\\\"/repos/{owner}/{repo}/contents/{path}\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Creates a new file or replaces an existing file in a repository.\\\\\\\\n\\\\\\\\n> [!NOTE]\\\\\\\\n> If you use this endpoint and the \\\\\\\\\\\\\\\"[Delete a file](https://docs.github.com/rest/repos/contents/#delete-a-file)\\\\\\\\\\\\\\\" endpoint in parallel, the concurrent requests will conflict and you will receive errors. You must use these endpoints serially instead.\\\\\\\\n\\\\\\\\nOAuth app tokens and personal access tokens (classic) need the `repo` scope to use this endpoint. The `workflow` scope is also required in order to modify files in the `.github/workflows` directory.\\\\\\\",\\\\\\\"tags\\\\\\\":[\\\\\\\"repos\\\\\\\"],\\\\\\\"path_params\\\\\\\":{\\\\\\\"owner\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The account owner of the repository. The name is not case sensitive.\\\\\\\"},\\\\\\\"repo\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The name of the repository without the `.git` extension. The name is not case sensitive.\\\\\\\"},\\\\\\\"path\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"format\\\\\\\":null,\\\\\\\"required\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"path parameter\\\\\\\"}},\\\\\\\"query_params\\\\\\\":{},\\\\\\\"body_schema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"message\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"The commit message.\\\\\\\"},\\\\\\\"content\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"The new file content, using Base64 encoding.\\\\\\\"},\\\\\\\"sha\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"**Required if you are updating a file**. The blob SHA of the file being replaced.\\\\\\\"},\\\\\\\"branch\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"The branch name. Default: the repository\\\\\\\\u2019s default branch.\\\\\\\"},\\\\\\\"committer\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"The person that committed the file. Default: the authenticated user.\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"name\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The name of the author or committer of the commit. You'll receive a `422` status code if `name` is omitted.\\\\\\\"},\\\\\\\"email\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The email of the author or committer of the commit. You'll receive a `422` status code if `email` is omitted.\\\\\\\"},\\\\\\\"date\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true}},\\\\\\\"required\\\\\\\":[\\\\\\\"name\\\\\\\",\\\\\\\"email\\\\\\\"]},\\\\\\\"author\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"The author of the file. Default: The `committer` or the authenticated user if you omit `committer`.\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"name\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The name of the author or committer of the commit. You'll receive a `422` status code if `name` is omitted.\\\\\\\"},\\\\\\\"email\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true,\\\\\\\"description\\\\\\\":\\\\\\\"The email of the author or committer of the commit. You'll receive a `422` status code if `email` is omitted.\\\\\\\"},\\\\\\\"date\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"_truncated\\\\\\\":true}},\\\\\\\"required\\\\\\\":[\\\\\\\"name\\\\\\\",\\\\\\\"email\\\\\\\"]}},\\\\\\\"required\\\\\\\":[\\\\\\\"message\\\\\\\",\\\\\\\"content\\\\\\\"]},\\\\\\\"response_schema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"File Commit\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"content\\\\\\\":{\\\\\\\"type\\\\\\\":[\\\\\\\"object\\\\\\\",\\\\\\\"null\\\\\\\"],\\\\\\\"_truncated\\\\\\\":true},\\\\\\\"commit\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"_truncated\\\\\\\":true}},\\\\\\\"required\\\\\\\":[\\\\\\\"content\\\\\\\",\\\\\\\"commit\\\\\\\"]}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Now let me start the investigation by first listing the commits that modified the `examples/config.yaml` file:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"list_commits\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\"}\", \"query_params\": \"{\\\"path\\\": \\\"examples/config.yaml\\\", \\\"per_page\\\": 50}\", \"include_output_fields\": [\"sha\", \"commit.author.name\", \"commit.author.date\", \"commit.message\", \"author.login\"]}",
    "call_id": "toolu_017Y1gdkEwEkuSEjV82ZEz9i",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_017Y1gdkEwEkuSEjV82ZEz9i",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"list_commits\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"098931530606d22f867fd121b1dcb3225a43661f\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[misc] fix data proto (#458)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-08-08T12:05:58Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"87e862a1f8f66cfd653a8d1b66ef9fa0368a4a50\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support auto resume (#425)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-15T05:48:35Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"52ba694ab5185a967d64801681961501468e6ed8\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[examples] fix config (#420)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-14T13:20:26Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[worker] add dynamic batching (#417)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-14T10:41:21Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"c19396ea16edc383cb25fedb3da58ae08e08046e\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[example] update config (#394)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-24T09:54:16Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"bd3b4eb275858421f12cac8d5d800ef353b0c8d0\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] fix video data (#386)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-23T12:24:55Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"0304d2debd27d7cbd60b0e10d63505b6a056851f\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[release] 0.3.1 (#370)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-19T06:55:58Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"2fc149222a9d6f01800c2b9fe660d46864e33b8c\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[examples] add qwen3_14b_dapo17k_dapo (#369)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-19T00:28:31Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Leijun Zhou\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"Saigyouji-Yuyuko1000\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"866b7cfd59bc700ccd3e25c0863d87a9b89c2343\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[worker] update reward manager (#360)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-17T17:27:53Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"e303179c31e582e0a7be2da9179ca9967848823f\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] add online filtering (#358)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-17T15:23:17Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Leijun Zhou\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"Saigyouji-Yuyuko1000\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"ad02af917a275550f4bfff95f9e1ce78be1fa8f1\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] repeat rollout and prepare filter (#346)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-14T18:55:19Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"572db6c119f63d195b96fbb52608f7e2dc36eb15\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] save best checkpoint (#343)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-13T20:12:53Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"6870bf93bffe68a7c9600f0d0258207484fe6189\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[worker] colocate actor and ref model (#342)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-13T19:14:28Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"98ebdcfb2d1e26bd71800b771dd9692f073e9373\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] fix position ids for qwen2vl mrope & add test (#339)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-06-13T13:59:59Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"647c30e94c63d3f8b8f0d65d00b0f7669595c41d\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[reward] support batch reward (#271)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-27T15:50:24Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"c024735f22cd0d202a3d9e2c263180bd861c65e9\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] support custom chat template (#270)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-27T15:11:48Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"8d28d058146058142e5e17e68616e43fed2a7469\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support async reward (#252)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-22T17:21:13Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"cba6cdfc03911bc69740ad352d9646e787e2acb0\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] add multi image dataset (#217)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-14T15:23:00Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"7895ba58fe2466aa18e539a44aaa79c9b7e7de64\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] enable user to filter overlong examples in RLHFDataset (#210)\\\\\\\\n\\\\\\\\nCo-authored-by: hoshi-hiyouga <hiyouga@buaa.edu.cn>\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-14T11:05:23Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Qunhong Zeng\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"0x404\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"0c6b7fb0e9e1a22d3d8bdc650cca1ea251203297\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[data] update data configs (#214)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-14T10:55:45Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[core] separate score fn & vllm logit bias (#204)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-10T17:15:19Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[misc] minor update (#188)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-03T20:26:53Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[misc] support adamw bf16 (#171)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-27T15:44:01Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"cada92630e53290423da3ee38b37274ab9254fa6\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[tracking] add tensorboard (#170)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-27T14:34:11Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support 32b (#164)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-26T17:13:32Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support save limit & fix oom issue (#158)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-25T12:29:59Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support channel-wise reward (#155)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-24T12:43:43Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"1e675810f9c6de29a78ec09980d237a84a33747c\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[trainer] support val sampling (#133)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-17T19:04:15Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"69b7029480d7fbeaa9f93d148545dbfbeb375836\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[core] remove entropy loss (#132)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-17T17:46:12Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}},{\\\\\\\"sha\\\\\\\":\\\\\\\"ac9bd29d8f18c0c2f979f2c4ac8773e675a7d35d\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"message\\\\\\\":\\\\\\\"[readme] add multinode instructions (#118)\\\\\\\",\\\\\\\"author\\\\\\\":{\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-14T08:41:10Z\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\"}},\\\\\\\"author\\\\\\\":{\\\\\\\"login\\\\\\\":\\\\\\\"hiyouga\\\\\\\"}}],\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Let me examine recent commits more closely, starting with the most recent ones. I'll look at the individual commits to see which one contains the parameter changes:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"ref\\\": \\\"f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\"}\", \"include_output_fields\": [\"sha\", \"commit.author\", \"commit.committer\", \"files\"]}",
    "call_id": "toolu_017Ut6L9V8xFmtXWzBjWmLRq",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_017Ut6L9V8xFmtXWzBjWmLRq",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"files\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"866715e76ae96f8d23059ca1e752fdd7e8c41a23\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\".gitignore\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":3,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":3,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/.gitignore\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/.gitignore\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/.gitignore?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -170,6 +170,9 @@ cython_debug/\\\\\\\\n # PyPI configuration file\\\\\\\\n .pypirc\\\\\\\\n \\\\\\\\n+# pytorch\\\\\\\\n+*.pt\\\\\\\\n+\\\\\\\\n # outputs\\\\\\\\n outputs/\\\\\\\\n checkpoints/\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"c65bf2f8cbc11ba271be213a300e9fe1c3287600\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/config.yaml\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fconfig.yaml?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -38,6 +38,7 @@ worker:\\\\\\\\n     micro_batch_size_per_device_for_experience: 16  # equivalent to verl's rollout.log_prob_micro_batch_size_per_gpu\\\\\\\\n     max_grad_norm: 1.0\\\\\\\\n     padding_free: true\\\\\\\\n+    dynamic_batching: true\\\\\\\\n     ulysses_size: 1\\\\\\\\n     model:\\\\\\\\n       model_path: Qwen/Qwen2.5-7B-Instruct\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"e40090bf7a5e6a127200d5529a8ee0a8f62215d8\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen3_14b_dapo17k_dapo.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":3,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/examples%2Fqwen3_14b_dapo17k_dapo.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/examples%2Fqwen3_14b_dapo17k_dapo.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen3_14b_dapo17k_dapo.sh?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -16,7 +16,8 @@ python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     data.rollout_batch_size=512 \\\\\\\\\\\\\\\\\\\\\\\\n     data.mini_rollout_batch_size=256 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.micro_batch_size_per_device_for_update=1 \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.actor.micro_batch_size_per_device_for_experience=8 \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.actor.micro_batch_size_per_device_for_experience=4 \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.actor.ulysses_size=8 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.fsdp.torch_dtype=bf16 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.optim.strategy=adamw_bf16 \\\\\\\\\\\\\\\\\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"5f79c7b47864d779369b9198f6953130ab2f1501\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"tests/test_dataset.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":7,\\\\\\\"changes\\\\\\\":9,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/tests%2Ftest_dataset.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/tests%2Ftest_dataset.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/tests%2Ftest_dataset.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -35,24 +35,19 @@ def test_image_dataset():\\\\\\\\n     )\\\\\\\\n     token_ids = [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655]\\\\\\\\n     assert set(dataset[0].keys()) == {\\\\\\\\n-        \\\\\\\\\\\\\\\"problem\\\\\\\\\\\\\\\",\\\\\\\\n-        \\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\",\\\\\\\\n         \\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\",\\\\\\\\n         \\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\",\\\\\\\\n         \\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\",\\\\\\\\n         \\\\\\\\\\\\\\\"raw_prompt_ids\\\\\\\\\\\\\\\",\\\\\\\\n+        \\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\",\\\\\\\\n         \\\\\\\\\\\\\\\"multi_modal_data\\\\\\\\\\\\\\\",\\\\\\\\n     }\\\\\\\\n-    assert dataset[0][\\\\\\\\\\\\\\\"problem\\\\\\\\\\\\\\\"] == (\\\\\\\\n-        \\\\\\\\\\\\\\\"<image>Chords $\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\overline{A C}$ and $\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\overline{D F}$ are equidistant from the center. \\\\\\\\\\\\\\\"\\\\\\\\n-        \\\\\\\\\\\\\\\"If the radius of $\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\odot G$ is 26 find $A C$\\\\\\\\\\\\\\\"\\\\\\\\n-    )\\\\\\\\n-    assert dataset[0][\\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\"] == \\\\\\\\\\\\\\\"48\\\\\\\\\\\\\\\"\\\\\\\\n     assert torch.all(dataset[0][\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"] == torch.tensor(token_ids))\\\\\\\\n     assert torch.all(dataset[0][\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"] == torch.ones(16))\\\\\\\\n     assert torch.all(dataset[0][\\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\"] == torch.arange(16).unsqueeze(0).expand(3, -1))\\\\\\\\n     assert list(dataset[0][\\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\"].size()) == [3, 16]  # avoid fake positive caused by broadcasting\\\\\\\\n     assert dataset[0][\\\\\\\\\\\\\\\"raw_prompt_ids\\\\\\\\\\\\\\\"] == token_ids\\\\\\\\n+    assert dataset[0][\\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\"] == \\\\\\\\\\\\\\\"48\\\\\\\\\\\\\\\"\\\\\\\\n     assert isinstance(dataset[0][\\\\\\\\\\\\\\\"multi_modal_data\\\\\\\\\\\\\\\"][\\\\\\\\\\\\\\\"images\\\\\\\\\\\\\\\"][0], Image)\\\\\\\\n \\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"3c94ee215ed3671841f980d8841b3d959bf2e145\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"tests/test_dynamic_batch.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"added\\\\\\\",\\\\\\\"additions\\\\\\\":78,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":78,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/tests%2Ftest_dynamic_batch.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/tests%2Ftest_dynamic_batch.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/tests%2Ftest_dynamic_batch.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -0,0 +1,78 @@\\\\\\\\n+# Copyright 2024 Bytedance Ltd. and/or its affiliates\\\\\\\\n+#\\\\\\\\n+# Licensed under the Apache License, Version 2.0 (the \\\\\\\\\\\\\\\"License\\\\\\\\\\\\\\\");\\\\\\\\n+# you may not use this file except in compliance with the License.\\\\\\\\n+# You may obtain a copy of the License at\\\\\\\\n+#\\\\\\\\n+#     http://www.apache.org/licenses/LICENSE-2.0\\\\\\\\n+#\\\\\\\\n+# Unless required by applicable law or agreed to in writing, software\\\\\\\\n+# distributed under the License is distributed on an \\\\\\\\\\\\\\\"AS IS\\\\\\\\\\\\\\\" BASIS,\\\\\\\\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\\\\\\\n+# See the License for the specific language governing permissions and\\\\\\\\n+# limitations under the License.\\\\\\\\n+\\\\\\\\n+import numpy as np\\\\\\\\n+import torch\\\\\\\\n+\\\\\\\\n+from verl.protocol import DataProto\\\\\\\\n+from verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def _create_random_mask(\\\\\\\\n+    input_ids: torch.Tensor,\\\\\\\\n+    max_ratio_of_valid_token: float,\\\\\\\\n+    max_ratio_of_left_padding: float,\\\\\\\\n+    min_ratio_of_valid_token: float = 0,\\\\\\\\n+) -> torch.Tensor:\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Create a random mask given input_ids. Support left padding and right padding.\\\\\\\\n+\\\\\\\\n+    Process:\\\\\\\\n+    - Sample valid token length\\\\\\\\n+    - Sample left_padding length\\\\\\\\n+    - Generate padding\\\\\\\\n+\\\\\\\\n+    Args:\\\\\\\\n+        input_ids:\\\\\\\\n+            shape (batch_size, seq_len)\\\\\\\\n+\\\\\\\\n+    Returns:\\\\\\\\n+        mask:\\\\\\\\n+            shape (batch_size, seq_len)\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    assert max_ratio_of_valid_token > 0 and max_ratio_of_valid_token <= 1.0\\\\\\\\n+    assert max_ratio_of_left_padding >= 0 and max_ratio_of_left_padding < 1.0\\\\\\\\n+    assert min_ratio_of_valid_token <= max_ratio_of_valid_token\\\\\\\\n+\\\\\\\\n+    batch_size, sequence_length = input_ids.shape\\\\\\\\n+    max_num_valid_tokens = int(sequence_length * max_ratio_of_valid_token)\\\\\\\\n+    min_num_valid_tokens = max(1, int(sequence_length * min_ratio_of_valid_token))\\\\\\\\n+    max_left_padding = int(sequence_length * max_ratio_of_left_padding)\\\\\\\\n+    assert max_num_valid_tokens + max_left_padding <= sequence_length\\\\\\\\n+    assert max_num_valid_tokens > 0 and max_ratio_of_valid_token <= sequence_length\\\\\\\\n+    mask = torch.ones_like(input_ids, dtype=torch.int64)\\\\\\\\n+    # TODO: we can make this faster\\\\\\\\n+    for i in range(batch_size):\\\\\\\\n+        num_left_padding = np.random.randint(low=0, high=max_left_padding + 1, dtype=np.int64)\\\\\\\\n+        num_valid = np.random.randint(low=min_num_valid_tokens, high=max_num_valid_tokens + 1, dtype=np.int64)\\\\\\\\n+\\\\\\\\n+        for index in range(num_left_padding):\\\\\\\\n+            mask[i, index] = 0\\\\\\\\n+\\\\\\\\n+        for index in range(num_left_padding + num_valid, sequence_length):\\\\\\\\n+            mask[i, index] = 0\\\\\\\\n+\\\\\\\\n+    return mask\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def test_dynamic_batch():\\\\\\\\n+    input_ids = torch.randint(low=0, high=10, size=(20, 100))\\\\\\\\n+    attention_mask = _create_random_mask(\\\\\\\\n+        input_ids=input_ids, max_ratio_of_left_padding=0.1, max_ratio_of_valid_token=0.9, min_ratio_of_valid_token=0.5\\\\\\\\n+    )\\\\\\\\n+    data = {\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\": input_ids, \\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\": attention_mask}\\\\\\\\n+    dataproto = DataProto.from_single_dict(data)\\\\\\\\n+    micro_batches, micro_bsz_idx_lst = prepare_dynamic_batch(dataproto, max_token_len=300)\\\\\\\\n+    input_ids = torch.cat([micro_batch.batch[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"] for micro_batch in micro_batches], dim=0)\\\\\\\\n+    input_ids = restore_dynamic_batch(input_ids, micro_bsz_idx_lst)\\\\\\\\n+    torch.testing.assert_close(input_ids, dataproto.batch[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"])\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"3a00c6d222682d663dcd5969cb2668f60aa4f3ff\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/protocol.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":9,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":12,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fprotocol.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fprotocol.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fprotocol.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -196,9 +196,12 @@ def __getitem__(\\\\\\\\n         if isinstance(item, (list, np.ndarray, torch.Tensor)):\\\\\\\\n             return self.index_select(item)\\\\\\\\n \\\\\\\\n-        tensor_data = self.batch[item] if self.batch is not None else None\\\\\\\\n-        non_tensor_data = {key: value[item] for key, value in self.non_tensor_batch.items()}\\\\\\\\n-        return DataProtoItem(batch=tensor_data, non_tensor_batch=non_tensor_data, meta_info=self.meta_info)\\\\\\\\n+        if isinstance(item, (int, np.integer)):\\\\\\\\n+            tensor_data = self.batch[item] if self.batch is not None else None\\\\\\\\n+            non_tensor_data = {key: value[item] for key, value in self.non_tensor_batch.items()}\\\\\\\\n+            return DataProtoItem(batch=tensor_data, non_tensor_batch=non_tensor_data, meta_info=self.meta_info)\\\\\\\\n+\\\\\\\\n+        raise TypeError(f\\\\\\\\\\\\\\\"Indexing with {type(item)} is not supported.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n     def __getstate__(self) -> Tuple[bytes, Dict[str, NDArray], Dict[str, Any]]:\\\\\\\\n         buffer = io.BytesIO()\\\\\\\\n@@ -573,6 +576,9 @@ def split(self, split_size: int) -> List[\\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\"]:\\\\\\\\n         Returns:\\\\\\\\n             List[DataProto]: a list of DataProto after splitting\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+        assert len(self) % split_size == 0, (\\\\\\\\n+            f\\\\\\\\\\\\\\\"only support equal split. Got size of DataProto {len(self)} and split {split_size}.\\\\\\\\\\\\\\\"\\\\\\\\n+        )\\\\\\\\n         chunks = len(self) // split_size\\\\\\\\n         return self.chunk(chunks)\\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"447e7629e07cf684ace1d0c300fed7e60e638b4d\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/core_algos.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":9,\\\\\\\"deletions\\\\\\\":4,\\\\\\\"changes\\\\\\\":13,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Ftrainer%2Fcore_algos.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Ftrainer%2Fcore_algos.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fcore_algos.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -343,7 +343,7 @@ def compute_policy_loss(\\\\\\\\n     clip_ratio_high: float,\\\\\\\\n     clip_ratio_dual: float,\\\\\\\\n     loss_avg_mode: Literal[\\\\\\\\\\\\\\\"token\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"seq\\\\\\\\\\\\\\\"],\\\\\\\\n-) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\\\\\\\\n+) -> Tuple[torch.Tensor, Dict[str, float]]:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Compute the clipped policy objective and related metrics for PPO.\\\\\\\\n \\\\\\\\n     Adapted from https://github.com/huggingface/trl/blob/v0.15.0/trl/trainer/ppo_trainer.py#L568\\\\\\\\n@@ -417,7 +417,7 @@ def compute_value_loss(\\\\\\\\n     response_mask: torch.Tensor,\\\\\\\\n     cliprange_value: float,\\\\\\\\n     loss_avg_mode: Literal[\\\\\\\\\\\\\\\"token\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"seq\\\\\\\\\\\\\\\"],\\\\\\\\n-) -> Tuple[torch.Tensor, float]:\\\\\\\\n+) -> Tuple[torch.Tensor, Dict[str, float]]:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Compute the value loss.\\\\\\\\n \\\\\\\\n     Adapted from https://github.com/huggingface/trl/blob/v0.15.0/trl/trainer/ppo_trainer.py#L556\\\\\\\\n@@ -442,15 +442,20 @@ def compute_value_loss(\\\\\\\\n             value function loss\\\\\\\\n         vf_clipfrac: a float\\\\\\\\n             The ratio of vf being clipped\\\\\\\\n+        vpred_mean: a float\\\\\\\\n+            The mean of predicted values\\\\\\\\n \\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     vpredclipped = torch.clamp(vpreds, values - cliprange_value, values + cliprange_value)\\\\\\\\n     vf_loss1 = torch.square(vpreds - returns)\\\\\\\\n     vf_loss2 = torch.square(vpredclipped - returns)\\\\\\\\n     clipped_vf_losses = torch.max(vf_loss1, vf_loss2)  # clip if vf_loss1 < vf_loss2\\\\\\\\n     vf_loss = 0.5 * average_loss(clipped_vf_losses, response_mask, mode=loss_avg_mode)\\\\\\\\n-    vf_clipfrac = VF.masked_mean((vf_loss1 < vf_loss2).float(), response_mask).detach().item()\\\\\\\\n-    return vf_loss, vf_clipfrac\\\\\\\\n+    metrics = {\\\\\\\\n+        \\\\\\\\\\\\\\\"vf_clipfrac\\\\\\\\\\\\\\\": VF.masked_mean((vf_loss1 < vf_loss2).float(), response_mask).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"vpred_mean\\\\\\\\\\\\\\\": VF.masked_mean(vpreds, response_mask).detach().item(),\\\\\\\\n+    }\\\\\\\\n+    return vf_loss, metrics\\\\\\\\n \\\\\\\\n \\\\\\\\n def compute_kl(\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"75d73a3e9fb7021b2e18d31c41ac4ce1e0b2b6cd\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/metrics.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":28,\\\\\\\"deletions\\\\\\\":20,\\\\\\\"changes\\\\\\\":48,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Ftrainer%2Fmetrics.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Ftrainer%2Fmetrics.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fmetrics.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -24,6 +24,32 @@ def reduce_metrics(metrics: Dict[str, List[Any]]) -> Dict[str, Any]:\\\\\\\\n     return {key: np.mean(value) for key, value in metrics.items()}\\\\\\\\n \\\\\\\\n \\\\\\\\n+def compute_length_metrics(batch: DataProto) -> Dict[str, Any]:\\\\\\\\n+    max_response_length = batch.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+\\\\\\\\n+    prompt_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, :-max_response_length].bool()\\\\\\\\n+    response_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -max_response_length:].bool()\\\\\\\\n+\\\\\\\\n+    max_prompt_length = prompt_mask.size(-1)\\\\\\\\n+    prompt_length = prompt_mask.sum(-1).float()\\\\\\\\n+    response_length = response_mask.sum(-1).float()\\\\\\\\n+\\\\\\\\n+    return {\\\\\\\\n+        # response length\\\\\\\\n+        \\\\\\\\\\\\\\\"response_length/mean\\\\\\\\\\\\\\\": torch.mean(response_length).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"response_length/max\\\\\\\\\\\\\\\": torch.max(response_length).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"response_length/min\\\\\\\\\\\\\\\": torch.min(response_length).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"response_length/clip_ratio\\\\\\\\\\\\\\\": torch.mean(torch.eq(response_length, max_response_length).float())\\\\\\\\n+        .detach()\\\\\\\\n+        .item(),\\\\\\\\n+        # prompt length\\\\\\\\n+        \\\\\\\\\\\\\\\"prompt_length/mean\\\\\\\\\\\\\\\": torch.mean(prompt_length).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"prompt_length/max\\\\\\\\\\\\\\\": torch.max(prompt_length).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"prompt_length/min\\\\\\\\\\\\\\\": torch.min(prompt_length).detach().item(),\\\\\\\\n+        \\\\\\\\\\\\\\\"prompt_length/clip_ratio\\\\\\\\\\\\\\\": torch.mean(torch.eq(prompt_length, max_prompt_length).float()).detach().item(),\\\\\\\\n+    }\\\\\\\\n+\\\\\\\\n+\\\\\\\\n def compute_data_metrics(batch: DataProto, use_critic: bool = False) -> Dict[str, Any]:\\\\\\\\n     sequence_score = batch.batch[\\\\\\\\\\\\\\\"token_level_scores\\\\\\\\\\\\\\\"].sum(-1)\\\\\\\\n     sequence_reward = batch.batch[\\\\\\\\\\\\\\\"token_level_rewards\\\\\\\\\\\\\\\"].sum(-1)\\\\\\\\n@@ -32,14 +58,8 @@ def compute_data_metrics(batch: DataProto, use_critic: bool = False) -> Dict[str\\\\\\\\n     returns = batch.batch[\\\\\\\\\\\\\\\"returns\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n     max_response_length = batch.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n-\\\\\\\\n-    prompt_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, :-max_response_length].bool()\\\\\\\\n     response_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -max_response_length:].bool()\\\\\\\\n \\\\\\\\n-    max_prompt_length = prompt_mask.size(-1)\\\\\\\\n-    prompt_length = prompt_mask.sum(-1).float()\\\\\\\\n-    response_length = response_mask.sum(-1).float()\\\\\\\\n-\\\\\\\\n     valid_adv = torch.masked_select(advantages, response_mask)\\\\\\\\n     valid_returns = torch.masked_select(returns, response_mask)\\\\\\\\n \\\\\\\\n@@ -49,7 +69,7 @@ def compute_data_metrics(batch: DataProto, use_critic: bool = False) -> Dict[str\\\\\\\\n         return_diff_var = torch.var(valid_returns - valid_values)\\\\\\\\n         return_var = torch.var(valid_returns)\\\\\\\\n \\\\\\\\n-    metrics = {\\\\\\\\n+    return {\\\\\\\\n         # score\\\\\\\\n         \\\\\\\\\\\\\\\"critic/score/mean\\\\\\\\\\\\\\\": torch.mean(sequence_score).detach().item(),\\\\\\\\n         \\\\\\\\\\\\\\\"critic/score/max\\\\\\\\\\\\\\\": torch.max(sequence_score).detach().item(),\\\\\\\\n@@ -78,20 +98,8 @@ def compute_data_metrics(batch: DataProto, use_critic: bool = False) -> Dict[str\\\\\\\\n             if use_critic\\\\\\\\n             else {}\\\\\\\\n         ),\\\\\\\\n-        # response length\\\\\\\\n-        \\\\\\\\\\\\\\\"response_length/mean\\\\\\\\\\\\\\\": torch.mean(response_length).detach().item(),\\\\\\\\n-        \\\\\\\\\\\\\\\"response_length/max\\\\\\\\\\\\\\\": torch.max(response_length).detach().item(),\\\\\\\\n-        \\\\\\\\\\\\\\\"response_length/min\\\\\\\\\\\\\\\": torch.min(response_length).detach().item(),\\\\\\\\n-        \\\\\\\\\\\\\\\"response_length/clip_ratio\\\\\\\\\\\\\\\": torch.mean(torch.eq(response_length, max_response_length).float())\\\\\\\\n-        .detach()\\\\\\\\n-        .item(),\\\\\\\\n-        # prompt length\\\\\\\\n-        \\\\\\\\\\\\\\\"prompt_length/mean\\\\\\\\\\\\\\\": torch.mean(prompt_length).detach().item(),\\\\\\\\n-        \\\\\\\\\\\\\\\"prompt_length/max\\\\\\\\\\\\\\\": torch.max(prompt_length).detach().item(),\\\\\\\\n-        \\\\\\\\\\\\\\\"prompt_length/min\\\\\\\\\\\\\\\": torch.min(prompt_length).detach().item(),\\\\\\\\n-        \\\\\\\\\\\\\\\"prompt_length/clip_ratio\\\\\\\\\\\\\\\": torch.mean(torch.eq(prompt_length, max_prompt_length).float()).detach().item(),\\\\\\\\n+        **compute_length_metrics(batch),\\\\\\\\n     }\\\\\\\\n-    return metrics\\\\\\\\n \\\\\\\\n \\\\\\\\n def compute_timing_metrics(batch: DataProto, timing_raw: Dict[str, float]) -> Dict[str, Any]:\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"817ced5d41d8db5d34db02cc03e994ec58b1c1eb\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/ray_trainer.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":13,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":15,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fray_trainer.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -46,7 +46,13 @@\\\\\\\\n from . import core_algos\\\\\\\\n from .config import PPOConfig\\\\\\\\n from .core_algos import AdvantageEstimator, FixedKLController, KLController, compute_kl, get_kl_controller\\\\\\\\n-from .metrics import compute_data_metrics, compute_throughout_metrics, compute_timing_metrics, reduce_metrics\\\\\\\\n+from .metrics import (\\\\\\\\n+    compute_data_metrics,\\\\\\\\n+    compute_length_metrics,\\\\\\\\n+    compute_throughout_metrics,\\\\\\\\n+    compute_timing_metrics,\\\\\\\\n+    reduce_metrics,\\\\\\\\n+)\\\\\\\\n \\\\\\\\n \\\\\\\\n class Role(IntEnum):\\\\\\\\n@@ -380,6 +386,7 @@ def _validate(self) -> Dict[str, Any]:\\\\\\\\n         # Lists to collect samples for the table\\\\\\\\n         sample_inputs, sample_outputs, sample_labels, sample_scores = [], [], [], []\\\\\\\\n         reward_metrics_lst = defaultdict(list)\\\\\\\\n+        length_metrics_lst = defaultdict(list)\\\\\\\\n         print(\\\\\\\\\\\\\\\"Start validation...\\\\\\\\\\\\\\\")\\\\\\\\n         self.actor_rollout_ref_wg.prepare_rollout_engine()\\\\\\\\n         for batch_dict in self.val_dataloader:\\\\\\\\n@@ -420,12 +427,16 @@ def _validate(self) -> Dict[str, Any]:\\\\\\\\n             for key, value in reward_metrics.items():\\\\\\\\n                 reward_metrics_lst[key].extend(value)\\\\\\\\n \\\\\\\\n+            for key, value in compute_length_metrics(test_batch).items():\\\\\\\\n+                length_metrics_lst[key].append(value)\\\\\\\\n+\\\\\\\\n         self.actor_rollout_ref_wg.release_rollout_engine()\\\\\\\\n         self._maybe_log_val_generations(sample_inputs, sample_outputs, sample_labels, sample_scores)\\\\\\\\n         self.val_reward_score = torch.cat(reward_tensor_lst, dim=0).sum(-1).mean().item()\\\\\\\\n         val_reward_metrics = {f\\\\\\\\\\\\\\\"val/{key}_reward\\\\\\\\\\\\\\\": value for key, value in reduce_metrics(reward_metrics_lst).items()}\\\\\\\\n+        val_length_metrics = {f\\\\\\\\\\\\\\\"val_{key}\\\\\\\\\\\\\\\": value for key, value in reduce_metrics(length_metrics_lst).items()}\\\\\\\\n         print(\\\\\\\\\\\\\\\"Finish validation.\\\\\\\\\\\\\\\")\\\\\\\\n-        return {\\\\\\\\\\\\\\\"val/reward_score\\\\\\\\\\\\\\\": self.val_reward_score, **val_reward_metrics}\\\\\\\\n+        return {\\\\\\\\\\\\\\\"val/reward_score\\\\\\\\\\\\\\\": self.val_reward_score, **val_reward_metrics, **val_length_metrics}\\\\\\\\n \\\\\\\\n     def _balance_batch(self, batch: DataProto, metrics: Dict[str, Any], logging_prefix: str = \\\\\\\\\\\\\\\"global_seqlen\\\\\\\\\\\\\\\") -> None:\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Reorder the data on single controller such that each dp rank gets similar total tokens\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"c853b53eaffbd8533e3d3cc5968883031ddadc7b\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/dataset.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Futils%2Fdataset.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Futils%2Fdataset.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fdataset.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -218,6 +218,7 @@ def __len__(self):\\\\\\\\n     def __getitem__(self, index):\\\\\\\\n         example: dict = self.dataset[index]\\\\\\\\n         messages = self._build_messages(example)\\\\\\\\n+        example.pop(self.prompt_key, None)\\\\\\\\n \\\\\\\\n         if self.image_key in example:\\\\\\\\n             prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"b7aeac131a1159430874a8c3f1389ad29db8f9d3\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/seqlen_balancing.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":34,\\\\\\\"deletions\\\\\\\":16,\\\\\\\"changes\\\\\\\":50,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Futils%2Fseqlen_balancing.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Futils%2Fseqlen_balancing.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fseqlen_balancing.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -14,12 +14,15 @@\\\\\\\\n \\\\\\\\n import copy\\\\\\\\n import heapq\\\\\\\\n+from itertools import chain\\\\\\\\n from typing import List, Tuple\\\\\\\\n \\\\\\\\n import torch\\\\\\\\n from tensordict import TensorDict\\\\\\\\n from torch import distributed as dist\\\\\\\\n \\\\\\\\n+from ..protocol import DataProto\\\\\\\\n+\\\\\\\\n \\\\\\\\n class Set:\\\\\\\\n     def __init__(self) -> None:\\\\\\\\n@@ -148,8 +151,9 @@ def greedy_partition(seqlen_list: List[int], k_partitions: int, equal_size: bool\\\\\\\\n \\\\\\\\n \\\\\\\\n def get_seqlen_balanced_partitions(seqlen_list: List[int], k_partitions: int, equal_size: bool):\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"get order of seq lengths to make partitions balanced, this is\\\\\\\\n-        used in balacing sum of seqlength across dp ranks and microbatches\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Get order of seq lengths to make partitions balanced, this is\\\\\\\\n+    used in balacing sum of seqlength across dp ranks and microbatches.\\\\\\\\n+\\\\\\\\n     Parameters:\\\\\\\\n         seqlen_list (List[int]):\\\\\\\\n             seq lengths of each items\\\\\\\\n@@ -159,6 +163,7 @@ def get_seqlen_balanced_partitions(seqlen_list: List[int], k_partitions: int, eq\\\\\\\\n             if True, number of items in each partitions must be equal.\\\\\\\\n             if False, only consider balancing the sum, each partition can have\\\\\\\\n             variable number of items\\\\\\\\n+\\\\\\\\n     Returns:\\\\\\\\n         partitions (List[List[int]]):\\\\\\\\n             return k_partitions list containing the index of items.\\\\\\\\n@@ -228,29 +233,22 @@ def rearrange_micro_batches(batch: TensorDict, max_token_len, dp_group=None):\\\\\\\\n     assert max_token_len >= max_seq_len, (\\\\\\\\n         f\\\\\\\\\\\\\\\"max_token_len must be greater than the sequence length. Got {max_token_len=} and {max_seq_len=}\\\\\\\\\\\\\\\"\\\\\\\\n     )\\\\\\\\n-\\\\\\\\n-    seq_len_effective: torch.Tensor = batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"].sum(dim=1)\\\\\\\\n-    total_seqlen = seq_len_effective.sum().item()\\\\\\\\n+    effective_seqlen = torch.sum(batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"], dim=-1)\\\\\\\\n+    total_seqlen = effective_seqlen.sum().item()\\\\\\\\n     num_micro_batches = ceildiv(total_seqlen, max_token_len)\\\\\\\\n     if dist.is_initialized():\\\\\\\\n         num_micro_batches = torch.tensor([num_micro_batches], device=\\\\\\\\\\\\\\\"cuda\\\\\\\\\\\\\\\")\\\\\\\\n         dist.all_reduce(num_micro_batches, op=dist.ReduceOp.MAX, group=dp_group)\\\\\\\\n         num_micro_batches = num_micro_batches.cpu().item()\\\\\\\\n \\\\\\\\n-    seq_len_effective = seq_len_effective.tolist()\\\\\\\\n-    assert num_micro_batches <= len(seq_len_effective)\\\\\\\\n-\\\\\\\\n-    micro_bsz_idx = get_seqlen_balanced_partitions(seq_len_effective, num_micro_batches, equal_size=False)\\\\\\\\n+    effective_seqlen = effective_seqlen.tolist()\\\\\\\\n+    assert num_micro_batches <= len(effective_seqlen)\\\\\\\\n+    micro_bsz_idx = get_seqlen_balanced_partitions(effective_seqlen, num_micro_batches, equal_size=False)\\\\\\\\n \\\\\\\\n     micro_batches = []\\\\\\\\n-\\\\\\\\n     for partition in micro_bsz_idx:\\\\\\\\n-        curr_micro_batch = []\\\\\\\\n-        for idx in partition:\\\\\\\\n-            curr_micro_batch.append(batch[idx : idx + 1])\\\\\\\\n-        curr_micro_batch = torch.cat(curr_micro_batch)\\\\\\\\n-\\\\\\\\n-        micro_batches.append(curr_micro_batch)\\\\\\\\n+        curr_micro_batch = [batch[idx] for idx in partition]\\\\\\\\n+        micro_batches.append(torch.stack(curr_micro_batch))\\\\\\\\n \\\\\\\\n     return micro_batches, micro_bsz_idx\\\\\\\\n \\\\\\\\n@@ -262,3 +260,23 @@ def get_reverse_idx(idx_map):\\\\\\\\n         reverse_idx_map[idx] = i\\\\\\\\n \\\\\\\\n     return reverse_idx_map\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def prepare_dynamic_batch(data: DataProto, max_token_len: int) -> tuple[list[DataProto], list[list[int]]]:\\\\\\\\n+    batch, batch_idx_list = rearrange_micro_batches(data.batch, max_token_len=max_token_len)\\\\\\\\n+    micro_batches = []\\\\\\\\n+    for i, batch_idx in enumerate(batch_idx_list):\\\\\\\\n+        tensors = dict(batch[i])\\\\\\\\n+        non_tensors = {}\\\\\\\\n+        for key in data.non_tensor_batch.keys():\\\\\\\\n+            non_tensors[key] = [data.non_tensor_batch[key][idx] for idx in batch_idx]\\\\\\\\n+\\\\\\\\n+        micro_batches.append(DataProto.from_dict(tensors, non_tensors))\\\\\\\\n+\\\\\\\\n+    return micro_batches, batch_idx_list\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def restore_dynamic_batch(data: torch.Tensor, batch_idx_list: List[List[int]]) -> torch.Tensor:\\\\\\\\n+    indices = list(chain.from_iterable(batch_idx_list))\\\\\\\\n+    revert_indices = torch.tensor(get_reverse_idx(indices), dtype=torch.long)\\\\\\\\n+    return data[revert_indices]\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"96c27e9c9ac095d0ad88da39c3ed8ae4343888b3\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/ulysses.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":50,\\\\\\\"deletions\\\\\\\":21,\\\\\\\"changes\\\\\\\":71,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Futils%2Fulysses.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Futils%2Fulysses.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fulysses.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -14,7 +14,7 @@\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n Utilities for DeepSpeed Ulysses Sequence Parallelism.\\\\\\\\n DeepSpeed Ulysses Paper: https://arxiv.org/abs/2309.14509\\\\\\\\n-Inspired from: https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/sequence/layer.py\\\\\\\\n+Inspired from: https://github.com/deepspeedai/DeepSpeed/blob/master/deepspeed/sequence/layer.py\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n from typing import Any, Optional, Tuple\\\\\\\\n@@ -181,10 +181,7 @@ def forward(\\\\\\\\n \\\\\\\\n     @staticmethod\\\\\\\\n     def backward(ctx: Any, *grad_output: Tensor) -> Tuple[None, Tensor, None, None]:\\\\\\\\n-        if ctx.async_op:\\\\\\\\n-            input_t = torch.cat(grad_output[1:], dim=ctx.gather_dim).contiguous()\\\\\\\\n-        else:\\\\\\\\n-            input_t = grad_output[0]\\\\\\\\n+        input_t = torch.cat(grad_output[1:], dim=ctx.gather_dim).contiguous() if ctx.async_op else grad_output[0]\\\\\\\\n         return (\\\\\\\\n             None,\\\\\\\\n             all_to_all_tensor(input_t, ctx.gather_dim, ctx.scatter_dim, ctx.group, False),\\\\\\\\n@@ -246,8 +243,22 @@ def gather_outputs_and_unpad(\\\\\\\\n     grad_scaler: bool = True,\\\\\\\\n     group: Optional[dist.ProcessGroup] = None,\\\\\\\\n ):\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    Gather a tensor across a process group and optionally unpad its padded elements.\\\\\\\\n+\\\\\\\\n+    Args:\\\\\\\\n+        x (Tensor): Input tensor to gather.\\\\\\\\n+        gather_dim (int): Dimension along which to gather across ranks.\\\\\\\\n+        unpad_dim (int, optional): Dimension from which to remove padding. If None, no unpadding.\\\\\\\\n+        padding_size (int): Number of padding elements to remove on `unpad_dim`. Defaults to 0.\\\\\\\\n+        grad_scaler (bool): Whether to apply gradient scaling during gather. Defaults to True.\\\\\\\\n+        group (ProcessGroup, optional): Process group for gathering. If None, uses\\\\\\\\n+            `get_ulysses_sequence_parallel_group()`. If still None, returns `x` unchanged.\\\\\\\\n+\\\\\\\\n+    Returns:\\\\\\\\n+        Tensor: The gathered tensor, with padding removed if requested.\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     group = get_ulysses_sequence_parallel_group() if group is None else group\\\\\\\\n-    # sp_size = get_ulysses_sequence_parallel_world_size()\\\\\\\\n     if group is None:\\\\\\\\n         return x\\\\\\\\n     x = Gather.apply(group, x, gather_dim, grad_scaler)\\\\\\\\n@@ -259,15 +270,35 @@ def gather_outputs_and_unpad(\\\\\\\\n     return x\\\\\\\\n \\\\\\\\n \\\\\\\\n+def ulysses_pad(input_ids_rmpad: torch.Tensor, position_ids_rmpad: Optional[torch.Tensor] = None, sp_size: int = 1):\\\\\\\\n+    if position_ids_rmpad is not None:\\\\\\\\n+        assert position_ids_rmpad.size(-2) == 1\\\\\\\\n+        assert input_ids_rmpad.size(-1) == position_ids_rmpad.size(-1)\\\\\\\\n+\\\\\\\\n+    if sp_size <= 1:\\\\\\\\n+        return input_ids_rmpad, position_ids_rmpad, 0\\\\\\\\n+\\\\\\\\n+    _, total_seq_len = input_ids_rmpad.shape\\\\\\\\n+    pad_size = (sp_size - total_seq_len % sp_size) % sp_size\\\\\\\\n+    if pad_size > 0:\\\\\\\\n+        input_ids_rmpad = torch.nn.functional.pad(input_ids_rmpad, (0, pad_size), value=0)\\\\\\\\n+        if position_ids_rmpad is not None:\\\\\\\\n+            pad_pos_ids = torch.arange(pad_size, device=position_ids_rmpad.device).unsqueeze(0)\\\\\\\\n+            if position_ids_rmpad.dim() == 3:\\\\\\\\n+                pad_pos_ids = pad_pos_ids.unsqueeze(0).repeat(3, 1, 1)\\\\\\\\n+            position_ids_rmpad = torch.cat((position_ids_rmpad, pad_pos_ids), dim=-1)\\\\\\\\n+\\\\\\\\n+    return input_ids_rmpad, position_ids_rmpad, pad_size\\\\\\\\n+\\\\\\\\n+\\\\\\\\n def ulysses_pad_and_slice_inputs(\\\\\\\\n     input_ids_rmpad: torch.Tensor, position_ids_rmpad: Optional[torch.Tensor] = None, sp_size: int = 1\\\\\\\\n ):\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     Pad and slice input_ids to be divisible by sp_size\\\\\\\\n     Pad position_ids to be divisible by sp_size.\\\\\\\\n \\\\\\\\n-    Note both input_ids_rmpad and position_ids_rmpad will be padded,\\\\\\\\n-    but only input_ids will be sliced.\\\\\\\\n+    Note both input_ids_rmpad and position_ids_rmpad will be padded and sliced.\\\\\\\\n \\\\\\\\n     The is the utility of pre-forward for ulysses sequence parallelism\\\\\\\\n \\\\\\\\n@@ -281,18 +312,16 @@ def ulysses_pad_and_slice_inputs(\\\\\\\\n         torch.Tensor: padded and sliced position_ids\\\\\\\\n         int: pad size\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    input_ids_rmpad, position_ids_rmpad, pad_size = ulysses_pad(input_ids_rmpad, position_ids_rmpad, sp_size)\\\\\\\\n+    input_ids_rmpad = slice_input_tensor(input_ids_rmpad, dim=-1, padding=False)\\\\\\\\n     if position_ids_rmpad is not None:\\\\\\\\n-        assert position_ids_rmpad.size(0) == 1\\\\\\\\n-        assert input_ids_rmpad.size(1) == position_ids_rmpad.size(1)\\\\\\\\n-    if sp_size <= 1:\\\\\\\\n-        return input_ids_rmpad, position_ids_rmpad, 0\\\\\\\\n-    _, total_seq_len = input_ids_rmpad.shape\\\\\\\\n-    pad_size = (sp_size - total_seq_len % sp_size) % sp_size\\\\\\\\n-    if pad_size > 0:\\\\\\\\n-        input_ids_rmpad = torch.nn.functional.pad(input_ids_rmpad, (0, pad_size), value=0)\\\\\\\\n-        if position_ids_rmpad is not None:\\\\\\\\n-            pad_pos_ids = torch.arange(pad_size, device=position_ids_rmpad.device).unsqueeze(0)\\\\\\\\n-            position_ids_rmpad = torch.cat((position_ids_rmpad, pad_pos_ids), dim=-1)\\\\\\\\n-    # we don't need to slice position ids\\\\\\\\n-    input_ids_rmpad = slice_input_tensor(input_ids_rmpad, dim=1, padding=False)\\\\\\\\n+        position_ids_rmpad = slice_input_tensor(position_ids_rmpad, dim=-1, padding=False)\\\\\\\\n+\\\\\\\\n     return input_ids_rmpad, position_ids_rmpad, pad_size\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def validate_ulysses_config(num_heads, ulysses_sequence_size):\\\\\\\\n+    if ulysses_sequence_size > 1:\\\\\\\\n+        assert num_heads % ulysses_sequence_size == 0, (\\\\\\\\n+            f\\\\\\\\\\\\\\\"num_heads ({num_heads}) must be divisible by ulysses sequence size({ulysses_sequence_size})\\\\\\\\\\\\\\\"\\\\\\\\n+        )\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"4a8fcba26e7627463fe33de449eea8116cef5afc\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/actor/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":4,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Factor%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Factor%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Factor%2Fconfig.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -96,9 +96,12 @@ class ActorConfig:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"number of ppo epochs for each rollout batch\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     padding_free: bool = True\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"use padding-free training\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    dynamic_batching: bool = True\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"enable dynamic batching\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     ulysses_size: int = 1\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"ulysses sequence parallel size\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     use_torch_compile: bool = True\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"enable torch compile\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     model: ModelConfig = field(default_factory=ModelConfig)\\\\\\\\n     optim: OptimConfig = field(default_factory=OptimConfig)\\\\\\\\n     fsdp: FSDPConfig = field(default_factory=FSDPConfig)\\\\\\\\n@@ -119,5 +122,6 @@ class RefConfig:\\\\\\\\n     # below are auto keys\\\\\\\\n     micro_batch_size_per_device_for_experience: int = field(default=-1, init=False)\\\\\\\\n     padding_free: bool = field(default=False, init=False)\\\\\\\\n+    dynamic_batching: bool = field(default=False, init=False)\\\\\\\\n     ulysses_size: int = field(default=1, init=False)\\\\\\\\n     use_torch_compile: bool = field(default=True, init=False)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"edeffd50de3825644cd94e2d137f775a1886d34c\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/actor/dp_actor.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":30,\\\\\\\"deletions\\\\\\\":16,\\\\\\\"changes\\\\\\\":46,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Factor%2Fdp_actor.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Factor%2Fdp_actor.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Factor%2Fdp_actor.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -29,6 +29,7 @@\\\\\\\\n from ...trainer.core_algos import average_loss, compute_kl, compute_policy_loss\\\\\\\\n from ...utils import torch_functional as VF\\\\\\\\n from ...utils.py_functional import append_to_dict\\\\\\\\n+from ...utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\\\\\\\\n from ...utils.ulysses import gather_outputs_and_unpad, ulysses_pad_and_slice_inputs\\\\\\\\n from .base import BasePPOActor\\\\\\\\n from .config import ActorConfig\\\\\\\\n@@ -89,9 +90,7 @@ def _forward_micro_batch(self, micro_batch: Dict[str, torch.Tensor], temperature\\\\\\\\n                     multi_modal_inputs[key] = None\\\\\\\\n \\\\\\\\n         if self.config.padding_free:\\\\\\\\n-            input_ids_rmpad, indices, *_ = unpad_input(\\\\\\\\n-                input_ids.unsqueeze(-1), attention_mask\\\\\\\\n-            )  # input_ids_rmpad (total_nnz, ...)\\\\\\\\n+            input_ids_rmpad, indices, *_ = unpad_input(input_ids.unsqueeze(-1), attention_mask)  # (total_nnz, 1)\\\\\\\\n             input_ids_rmpad = input_ids_rmpad.transpose(0, 1)  # (1, total_nnz)\\\\\\\\n \\\\\\\\n             # unpad the position_ids to align the rotary\\\\\\\\n@@ -197,9 +196,13 @@ def compute_log_prob(self, data: DataProto) -> torch.Tensor:\\\\\\\\n         select_keys = [\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\"]\\\\\\\\n         non_tensor_select_keys = [\\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n-        micro_batches = data.select(select_keys, non_tensor_select_keys).split(\\\\\\\\n-            self.config.micro_batch_size_per_device_for_experience\\\\\\\\n-        )\\\\\\\\n+        data = data.select(select_keys, non_tensor_select_keys)\\\\\\\\n+        if self.config.dynamic_batching:\\\\\\\\n+            max_token_len = self.config.micro_batch_size_per_device_for_experience * data.batch[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+            micro_batches, batch_idx_list = prepare_dynamic_batch(data, max_token_len=max_token_len)\\\\\\\\n+        else:\\\\\\\\n+            micro_batches = data.split(self.config.micro_batch_size_per_device_for_experience)\\\\\\\\n+\\\\\\\\n         log_probs_lst = []\\\\\\\\n         if self.rank == 0:\\\\\\\\n             micro_batches = tqdm(micro_batches, desc=\\\\\\\\\\\\\\\"Compute log probs\\\\\\\\\\\\\\\", position=1)\\\\\\\\n@@ -210,6 +213,10 @@ def compute_log_prob(self, data: DataProto) -> torch.Tensor:\\\\\\\\n             log_probs_lst.append(log_probs)\\\\\\\\n \\\\\\\\n         log_probs = torch.concat(log_probs_lst, dim=0)\\\\\\\\n+\\\\\\\\n+        if self.config.dynamic_batching:\\\\\\\\n+            log_probs = restore_dynamic_batch(log_probs, batch_idx_list)\\\\\\\\n+\\\\\\\\n         return log_probs\\\\\\\\n \\\\\\\\n     def update_policy(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n@@ -230,19 +237,24 @@ def update_policy(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n                 mini_batches = tqdm(mini_batches, desc=\\\\\\\\\\\\\\\"Train mini-batches\\\\\\\\\\\\\\\", position=1)\\\\\\\\n \\\\\\\\n             for mini_batch in mini_batches:\\\\\\\\n-                gradient_accumulation = (\\\\\\\\n-                    self.config.global_batch_size_per_device // self.config.micro_batch_size_per_device_for_update\\\\\\\\n-                )\\\\\\\\n-                micro_batches = mini_batch.split(self.config.micro_batch_size_per_device_for_update)\\\\\\\\n+                response_length = mini_batch.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+                response_mask = mini_batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -response_length:]\\\\\\\\n+                total_response_tokens = torch.sum(response_mask)\\\\\\\\n+\\\\\\\\n+                if self.config.dynamic_batching:\\\\\\\\n+                    max_input_len = mini_batch.batch[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+                    max_token_len = self.config.micro_batch_size_per_device_for_update * max_input_len\\\\\\\\n+                    micro_batches, _ = prepare_dynamic_batch(mini_batch, max_token_len=max_token_len)\\\\\\\\n+                else:\\\\\\\\n+                    micro_batches = mini_batch.split(self.config.micro_batch_size_per_device_for_update)\\\\\\\\n+\\\\\\\\n                 if self.rank == 0:\\\\\\\\n                     micro_batches = tqdm(micro_batches, desc=\\\\\\\\\\\\\\\"Update policy\\\\\\\\\\\\\\\", position=2)\\\\\\\\n \\\\\\\\n                 for micro_batch in micro_batches:\\\\\\\\n                     model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}\\\\\\\\n-                    responses = model_inputs[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-                    response_length = responses.size(1)\\\\\\\\n-                    attention_mask = model_inputs[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n-                    response_mask = attention_mask[:, -response_length:]\\\\\\\\n+                    response_length = model_inputs[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+                    response_mask = model_inputs[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -response_length:]\\\\\\\\n                     old_log_probs = model_inputs[\\\\\\\\\\\\\\\"old_log_probs\\\\\\\\\\\\\\\"]\\\\\\\\n                     advantages = model_inputs[\\\\\\\\\\\\\\\"advantages\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n@@ -268,11 +280,13 @@ def update_policy(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n                             kl_penalty=self.config.kl_penalty,\\\\\\\\n                         )\\\\\\\\n                         kl_loss = average_loss(kld, response_mask, mode=self.config.loss_avg_mode)\\\\\\\\n-                        pg_loss = pg_loss + kl_loss * self.config.kl_coef\\\\\\\\n+                        loss = pg_loss + kl_loss * self.config.kl_coef\\\\\\\\n                         metrics[\\\\\\\\\\\\\\\"actor/kl_loss\\\\\\\\\\\\\\\"] = kl_loss.detach().item()\\\\\\\\n                         metrics[\\\\\\\\\\\\\\\"actor/kl_coef\\\\\\\\\\\\\\\"] = self.config.kl_coef\\\\\\\\n+                    else:\\\\\\\\n+                        loss = pg_loss\\\\\\\\n \\\\\\\\n-                    loss = pg_loss / gradient_accumulation\\\\\\\\n+                    loss = loss * torch.sum(response_mask) / total_response_tokens\\\\\\\\n                     loss.backward()\\\\\\\\n \\\\\\\\n                     batch_metrics = {\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"10381150c0519cfad00172972a01bbf637f1ff5f\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Fconfig.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -48,5 +48,6 @@ class WorkerConfig:\\\\\\\\n     def post_init(self):\\\\\\\\n         self.ref.micro_batch_size_per_device_for_experience = self.actor.micro_batch_size_per_device_for_experience\\\\\\\\n         self.ref.padding_free = self.actor.padding_free\\\\\\\\n+        self.ref.dynamic_batching = self.actor.dynamic_batching\\\\\\\\n         self.ref.ulysses_size = self.actor.ulysses_size\\\\\\\\n         self.ref.use_torch_compile = self.actor.use_torch_compile\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"07390a4721a045b366c8203e883ac1a901a94109\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/critic/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Fcritic%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Fcritic%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Fcritic%2Fconfig.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -39,6 +39,8 @@ class CriticConfig:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"number of ppo epochs for each rollout batch\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     padding_free: bool = False\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"use padding-free training\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    dynamic_batching: bool = True\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"enable dynamic batching\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     ulysses_size: int = 1\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"ulysses sequence parallel size\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     model: ModelConfig = field(default_factory=ModelConfig)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"13693bf7cd8f43e9da73f3859bbdf8042a1e0342\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/critic/dp_critic.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":31,\\\\\\\"deletions\\\\\\\":20,\\\\\\\"changes\\\\\\\":51,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Fcritic%2Fdp_critic.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Fcritic%2Fdp_critic.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Fcritic%2Fdp_critic.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -26,8 +26,8 @@\\\\\\\\n \\\\\\\\n from ...protocol import DataProto\\\\\\\\n from ...trainer.core_algos import compute_value_loss\\\\\\\\n-from ...utils import torch_functional as VF\\\\\\\\n from ...utils.py_functional import append_to_dict\\\\\\\\n+from ...utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\\\\\\\\n from ...utils.ulysses import gather_outputs_and_unpad, ulysses_pad_and_slice_inputs\\\\\\\\n from .base import BasePPOCritic\\\\\\\\n from .config import CriticConfig\\\\\\\\n@@ -149,9 +149,13 @@ def compute_values(self, data: DataProto) -> torch.Tensor:\\\\\\\\n         select_keys = [\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\"]\\\\\\\\n         non_tensor_select_keys = [\\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n-        micro_batches = data.select(select_keys, non_tensor_select_keys).split(\\\\\\\\n-            self.config.micro_batch_size_per_device_for_experience\\\\\\\\n-        )\\\\\\\\n+        data = data.select(select_keys, non_tensor_select_keys)\\\\\\\\n+        if self.config.dynamic_batching:\\\\\\\\n+            max_token_len = self.config.micro_batch_size_per_device_for_experience * data.batch[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+            micro_batches, batch_idx_list = prepare_dynamic_batch(data, max_token_len=max_token_len)\\\\\\\\n+        else:\\\\\\\\n+            micro_batches = data.split(self.config.micro_batch_size_per_device_for_experience)\\\\\\\\n+\\\\\\\\n         values_lst = []\\\\\\\\n         if self.rank == 0:\\\\\\\\n             micro_batches = tqdm(micro_batches, desc=\\\\\\\\\\\\\\\"Compute values\\\\\\\\\\\\\\\", position=1)\\\\\\\\n@@ -162,10 +166,12 @@ def compute_values(self, data: DataProto) -> torch.Tensor:\\\\\\\\n             values_lst.append(values)\\\\\\\\n \\\\\\\\n         values = torch.concat(values_lst, dim=0)\\\\\\\\n-        responses = data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-        attention_mask = data.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n-        response_length = responses.size(1)\\\\\\\\n-        values = values * attention_mask[:, -response_length:]\\\\\\\\n+\\\\\\\\n+        if self.config.dynamic_batching:\\\\\\\\n+            values = restore_dynamic_batch(values, batch_idx_list)\\\\\\\\n+\\\\\\\\n+        response_length = data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(1)\\\\\\\\n+        values = values * data.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -response_length:]  # only action tokens have values\\\\\\\\n         return values\\\\\\\\n \\\\\\\\n     def update_critic(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n@@ -184,38 +190,43 @@ def update_critic(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n                 mini_batches = tqdm(mini_batches, desc=\\\\\\\\\\\\\\\"Train mini-batches\\\\\\\\\\\\\\\", position=1)\\\\\\\\n \\\\\\\\n             for mini_batch in mini_batches:\\\\\\\\n-                gradient_accumulation = (\\\\\\\\n-                    self.config.global_batch_size_per_device // self.config.micro_batch_size_per_device_for_update\\\\\\\\n-                )\\\\\\\\n-                micro_batches = mini_batch.split(self.config.micro_batch_size_per_device_for_update)\\\\\\\\n+                response_length = mini_batch.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+                response_mask = mini_batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -response_length:]\\\\\\\\n+                total_response_tokens = torch.sum(response_mask)\\\\\\\\n+\\\\\\\\n+                if self.config.dynamic_batching:\\\\\\\\n+                    max_input_len = mini_batch.batch[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+                    max_token_len = self.config.micro_batch_size_per_device_for_update * max_input_len\\\\\\\\n+                    micro_batches, _ = prepare_dynamic_batch(mini_batch, max_token_len=max_token_len)\\\\\\\\n+                else:\\\\\\\\n+                    micro_batches = mini_batch.split(self.config.micro_batch_size_per_device_for_update)\\\\\\\\n+\\\\\\\\n                 if self.rank == 0:\\\\\\\\n                     micro_batches = tqdm(micro_batches, desc=\\\\\\\\\\\\\\\"Update critic\\\\\\\\\\\\\\\", position=2)\\\\\\\\n \\\\\\\\n                 for micro_batch in micro_batches:\\\\\\\\n                     model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}\\\\\\\\n-                    responses = model_inputs[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-                    response_length = responses.size(1)\\\\\\\\n-                    attention_mask = model_inputs[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n-                    response_mask = attention_mask[:, -response_length:]\\\\\\\\n+                    response_length = model_inputs[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n+                    response_mask = model_inputs[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -response_length:]\\\\\\\\n                     values = model_inputs[\\\\\\\\\\\\\\\"values\\\\\\\\\\\\\\\"]\\\\\\\\n                     returns = model_inputs[\\\\\\\\\\\\\\\"returns\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n                     vpreds = self._forward_micro_batch(model_inputs)\\\\\\\\n-                    vf_loss, vf_clipfrac = compute_value_loss(\\\\\\\\n+                    vf_loss, vf_metrics = compute_value_loss(\\\\\\\\n                         vpreds=vpreds,\\\\\\\\n                         returns=returns,\\\\\\\\n                         values=values,\\\\\\\\n                         response_mask=response_mask,\\\\\\\\n                         cliprange_value=self.config.cliprange_value,\\\\\\\\n                         loss_avg_mode=self.config.loss_avg_mode,\\\\\\\\n                     )\\\\\\\\n-                    loss = vf_loss / gradient_accumulation\\\\\\\\n+                    loss = vf_loss * torch.sum(response_mask) / total_response_tokens\\\\\\\\n                     loss.backward()\\\\\\\\n \\\\\\\\n                     batch_metrics = {\\\\\\\\n                         \\\\\\\\\\\\\\\"critic/vf_loss\\\\\\\\\\\\\\\": vf_loss.detach().item(),\\\\\\\\n-                        \\\\\\\\\\\\\\\"critic/vf_clipfrac\\\\\\\\\\\\\\\": vf_clipfrac.detach().item(),\\\\\\\\n-                        \\\\\\\\\\\\\\\"critic/vpred_mean\\\\\\\\\\\\\\\": VF.masked_mean(vpreds, response_mask).detach().item(),\\\\\\\\n+                        \\\\\\\\\\\\\\\"critic/vf_clipfrac\\\\\\\\\\\\\\\": vf_metrics[\\\\\\\\\\\\\\\"vf_clipfrac\\\\\\\\\\\\\\\"],\\\\\\\\n+                        \\\\\\\\\\\\\\\"critic/vpred_mean\\\\\\\\\\\\\\\": vf_metrics[\\\\\\\\\\\\\\\"vpred_mean\\\\\\\\\\\\\\\"],\\\\\\\\n                     }\\\\\\\\n                     append_to_dict(metrics, batch_metrics)\\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"06035f16908c958220571314fae4c582eb651118\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/fsdp_workers.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":3,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/f4264e7aabc898fc5e56396a1723cc3a0ff813d4/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Ffsdp_workers.py?ref=f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -518,16 +518,17 @@ def update_actor(self, data: DataProto):\\\\\\\\n             ) / (1024**3)\\\\\\\\n             metrics[\\\\\\\\\\\\\\\"perf/cpu_memory_used_gb\\\\\\\\\\\\\\\"] = psutil.virtual_memory().used / (1024**3)\\\\\\\\n \\\\\\\\n-            self.lr_scheduler.step()\\\\\\\\n             lr = self.lr_scheduler.get_last_lr()[0]\\\\\\\\n             metrics[\\\\\\\\\\\\\\\"actor/lr\\\\\\\\\\\\\\\"] = lr\\\\\\\\n+            self.lr_scheduler.step()\\\\\\\\n \\\\\\\\n             # Metrics should be in non_tensor_batch instead of meta_info, as DataProto not concat meta_info.\\\\\\\\n             output = DataProto(\\\\\\\\n                 non_tensor_batch={\\\\\\\\n                     key: np.array([value] if np.isscalar(value) else value) for key, value in metrics.items()\\\\\\\\n                 }\\\\\\\\n             )\\\\\\\\n+            output = self.ulysses_sharding_manager.postprocess_data(data=output)\\\\\\\\n \\\\\\\\n         if self._use_param_offload:\\\\\\\\n             offload_fsdp_model(self.fsdp_module)\\\\\\\"}],\\\\\\\"sha\\\\\\\":\\\\\\\"f4264e7aabc898fc5e56396a1723cc3a0ff813d4\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"committer\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"noreply@github.com\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-14T10:41:21Z\\\\\\\"},\\\\\\\"author\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"hiyouga@buaa.edu.cn\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-14T10:41:21Z\\\\\\\"}}},\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "This commit seems to be about dynamic batching, but I can see from the patch that it only added a line to `examples/config.yaml`. Let me look at a different commit to find the parameter changes we're looking for. Let me check the commit before this one:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"ref\\\": \\\"52ba694ab5185a967d64801681961501468e6ed8\\\"}\", \"include_output_fields\": [\"sha\", \"commit.author\", \"commit.committer\", \"files\"]}",
    "call_id": "toolu_017uEaKkkTYskpstj5jDEUqr",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_017uEaKkkTYskpstj5jDEUqr",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"files\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"82d255d364b3d656de17a0fe2b29d152cb9a968e\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/config.yaml\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/52ba694ab5185a967d64801681961501468e6ed8/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/52ba694ab5185a967d64801681961501468e6ed8/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fconfig.yaml?ref=52ba694ab5185a967d64801681961501468e6ed8\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -61,15 +61,15 @@ worker:\\\\\\\\n   rollout:\\\\\\\\n     n: 5\\\\\\\\n     temperature: 1.0\\\\\\\\n-    top_p: 0.99\\\\\\\\n+    top_p: 1.0\\\\\\\\n     limit_images: 0\\\\\\\\n     gpu_memory_utilization: 0.6\\\\\\\\n     enforce_eager: false\\\\\\\\n     enable_chunked_prefill: false\\\\\\\\n     tensor_parallel_size: 2\\\\\\\\n     disable_tqdm: false\\\\\\\\n     val_override_config:\\\\\\\\n-      temperature: 0.5\\\\\\\\n+      temperature: 1.0\\\\\\\\n       n: 1\\\\\\\\n \\\\\\\\n   ref:\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"b05fc5d1daad225e3f8b80e5d5e724dbc952ae2f\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen3_14b_dapo17k_dapo.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/52ba694ab5185a967d64801681961501468e6ed8/examples%2Fqwen3_14b_dapo17k_dapo.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/52ba694ab5185a967d64801681961501468e6ed8/examples%2Fqwen3_14b_dapo17k_dapo.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen3_14b_dapo17k_dapo.sh?ref=52ba694ab5185a967d64801681961501468e6ed8\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -29,6 +29,7 @@ python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.max_num_batched_tokens=22528 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.val_override_config='{\\\\\\\\\\\\\\\"n\\\\\\\\\\\\\\\":16,\\\\\\\\\\\\\\\"temperature\\\\\\\\\\\\\\\":1.0,\\\\\\\\\\\\\\\"top_p\\\\\\\\\\\\\\\":0.7}' \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.gpu_memory_utilization=0.8 \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.rollout.tensor_parallel_size=4 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.reward.reward_function=./examples/reward_function/dapo.py:compute_score \\\\\\\\\\\\\\\\\\\\\\\\n     worker.reward.reward_function_kwargs='{\\\\\\\\\\\\\\\"max_response_length\\\\\\\\\\\\\\\":20480,\\\\\\\\\\\\\\\"overlong_buffer_length\\\\\\\\\\\\\\\":4096,\\\\\\\\\\\\\\\"overlong_penalty_factor\\\\\\\\\\\\\\\":1.0}' \\\\\\\\\\\\\\\\\\\\\\\\n     algorithm.disable_kl=True \\\\\\\\\\\\\\\\\\\\\\\"}],\\\\\\\"sha\\\\\\\":\\\\\\\"52ba694ab5185a967d64801681961501468e6ed8\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"committer\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"noreply@github.com\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-14T13:20:26Z\\\\\\\"},\\\\\\\"author\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"Yaowei Zheng\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"hiyouga@buaa.edu.cn\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-07-14T13:20:26Z\\\\\\\"}}},\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "This commit changed temperature and top_p values, not the micro_batch_size parameters. Let me check an earlier commit that might have the changes I'm looking for:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"ref\\\": \\\"db81d107dfc70dc05fd10d955b90289e56e4df17\\\"}\", \"include_output_fields\": [\"sha\", \"commit.author\", \"commit.committer\", \"files\"]}",
    "call_id": "toolu_012b92xXLGamitkL9ubEJ8ob",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_012b92xXLGamitkL9ubEJ8ob",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"files\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"38bce35f1b84633756e6971a7da70fbd3568854e\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"Dockerfile\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":10,\\\\\\\"deletions\\\\\\\":4,\\\\\\\"changes\\\\\\\":14,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/Dockerfile\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/Dockerfile\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/Dockerfile?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -12,6 +12,7 @@ ENV HF_HUB_ENABLE_HF_TRANSFER=\\\\\\\\\\\\\\\"1\\\\\\\\\\\\\\\"\\\\\\\\n # Define installation arguments\\\\\\\\n ARG APT_SOURCE=https://mirrors.tuna.tsinghua.edu.cn/ubuntu/\\\\\\\\n ARG PIP_INDEX=https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\\\\\\\\n+ARG VLLM_COMMIT=227578480d71fc94ef46ca77fb69496412158d68\\\\\\\\n \\\\\\\\n # Set apt source\\\\\\\\n RUN cp /etc/apt/sources.list /etc/apt/sources.list.bak && \\\\\\\\\\\\\\\\\\\\\\\\n@@ -42,15 +43,20 @@ RUN pip uninstall -y torch torchvision torchaudio \\\\\\\\\\\\\\\\\\\\\\\\n     pytorch-quantization pytorch-triton torch-tensorrt \\\\\\\\\\\\\\\\\\\\\\\\n     xgboost transformer_engine flash_attn apex megatron-core\\\\\\\\n \\\\\\\\n-# Install torch-2.6.0 + vllm-0.8.1\\\\\\\\n-RUN pip install --no-cache-dir vllm==0.8.1 torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 tensordict torchdata \\\\\\\\\\\\\\\\\\\\\\\\n+# Install vllm-0.7.4-nightly\\\\\\\\n+RUN pip install --no-cache-dir vllm --pre --extra-index-url \\\\\\\\\\\\\\\"https://wheels.vllm.ai/${VLLM_COMMIT}\\\\\\\\\\\\\\\" && \\\\\\\\\\\\\\\\\\\\\\\\n+    git clone -b verl_v1 https://github.com/hiyouga/vllm.git && \\\\\\\\\\\\\\\\\\\\\\\\n+    cp -r vllm/vllm/ /usr/local/lib/python3.10/dist-packages/\\\\\\\\n+\\\\\\\\n+# Install torch-2.5.1\\\\\\\\n+RUN pip install --no-cache-dir torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 tensordict torchdata \\\\\\\\\\\\\\\\\\\\\\\\n     transformers>=4.49.0 accelerate datasets peft hf-transfer \\\\\\\\\\\\\\\\\\\\\\\\n     ray codetiming hydra-core pandas pyarrow>=15.0.0 pylatexenc qwen-vl-utils wandb liger-kernel mathruler \\\\\\\\\\\\\\\\\\\\\\\\n     pytest yapf py-spy pyext pre-commit ruff\\\\\\\\n \\\\\\\\n # Install flash_attn-2.7.4.post1\\\\\\\\n-RUN wget -nv https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl && \\\\\\\\\\\\\\\\\\\\\\\\n-    pip install --no-cache-dir flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\\\\\\\\n+RUN wget -nv https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl && \\\\\\\\\\\\\\\\\\\\\\\\n+    pip install --no-cache-dir flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\\\\\\\\n \\\\\\\\n # Fix cv2\\\\\\\\n RUN pip uninstall -y pynvml nvidia-ml-py && \\\\\\\\\\\\\\\\\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"a962a5510bdcce9f38087b1334128ca98b5731e8\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"Dockerfile_nightly\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"added\\\\\\\",\\\\\\\"additions\\\\\\\":58,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":58,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/Dockerfile_nightly\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/Dockerfile_nightly\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/Dockerfile_nightly?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -0,0 +1,58 @@\\\\\\\\n+# Start from the NVIDIA official image (ubuntu-22.04 + python-3.10)\\\\\\\\n+# https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-24-08.html\\\\\\\\n+FROM nvcr.io/nvidia/pytorch:24.08-py3\\\\\\\\n+\\\\\\\\n+# Define environments\\\\\\\\n+ENV MAX_JOBS=32\\\\\\\\n+ENV VLLM_WORKER_MULTIPROC_METHOD=spawn\\\\\\\\n+ENV DEBIAN_FRONTEND=noninteractive\\\\\\\\n+ENV NODE_OPTIONS=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+ENV HF_HUB_ENABLE_HF_TRANSFER=\\\\\\\\\\\\\\\"1\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+# Define installation arguments\\\\\\\\n+ARG APT_SOURCE=https://mirrors.tuna.tsinghua.edu.cn/ubuntu/\\\\\\\\n+ARG PIP_INDEX=https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\\\\\\\\n+\\\\\\\\n+# Set apt source\\\\\\\\n+RUN cp /etc/apt/sources.list /etc/apt/sources.list.bak && \\\\\\\\\\\\\\\\\\\\\\\\n+    { \\\\\\\\\\\\\\\\\\\\\\\\n+    echo \\\\\\\\\\\\\\\"deb ${APT_SOURCE} jammy main restricted universe multiverse\\\\\\\\\\\\\\\"; \\\\\\\\\\\\\\\\\\\\\\\\n+    echo \\\\\\\\\\\\\\\"deb ${APT_SOURCE} jammy-updates main restricted universe multiverse\\\\\\\\\\\\\\\"; \\\\\\\\\\\\\\\\\\\\\\\\n+    echo \\\\\\\\\\\\\\\"deb ${APT_SOURCE} jammy-backports main restricted universe multiverse\\\\\\\\\\\\\\\"; \\\\\\\\\\\\\\\\\\\\\\\\n+    echo \\\\\\\\\\\\\\\"deb ${APT_SOURCE} jammy-security main restricted universe multiverse\\\\\\\\\\\\\\\"; \\\\\\\\\\\\\\\\\\\\\\\\n+    } > /etc/apt/sources.list\\\\\\\\n+\\\\\\\\n+# Install systemctl\\\\\\\\n+RUN apt-get update && \\\\\\\\\\\\\\\\\\\\\\\\n+    apt-get install -y -o Dpkg::Options::=\\\\\\\\\\\\\\\"--force-confdef\\\\\\\\\\\\\\\" systemd && \\\\\\\\\\\\\\\\\\\\\\\\n+    apt-get clean\\\\\\\\n+\\\\\\\\n+# Install tini\\\\\\\\n+RUN apt-get update && \\\\\\\\\\\\\\\\\\\\\\\\n+    apt-get install -y tini && \\\\\\\\\\\\\\\\\\\\\\\\n+    apt-get clean\\\\\\\\n+\\\\\\\\n+# Change pip source\\\\\\\\n+RUN pip config set global.index-url \\\\\\\\\\\\\\\"${PIP_INDEX}\\\\\\\\\\\\\\\" && \\\\\\\\\\\\\\\\\\\\\\\\n+    pip config set global.extra-index-url \\\\\\\\\\\\\\\"${PIP_INDEX}\\\\\\\\\\\\\\\" && \\\\\\\\\\\\\\\\\\\\\\\\n+    python -m pip install --upgrade pip\\\\\\\\n+\\\\\\\\n+# Uninstall nv-pytorch fork\\\\\\\\n+RUN pip uninstall -y torch torchvision torchaudio \\\\\\\\\\\\\\\\\\\\\\\\n+    pytorch-quantization pytorch-triton torch-tensorrt \\\\\\\\\\\\\\\\\\\\\\\\n+    xgboost transformer_engine flash_attn apex megatron-core\\\\\\\\n+\\\\\\\\n+# Install torch-2.6.0 + vllm-0.8.1\\\\\\\\n+RUN pip install --no-cache-dir vllm==0.8.1 torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 tensordict torchdata \\\\\\\\\\\\\\\\\\\\\\\\n+    transformers>=4.49.0 accelerate datasets peft hf-transfer \\\\\\\\\\\\\\\\\\\\\\\\n+    ray codetiming hydra-core pandas pyarrow>=15.0.0 pylatexenc qwen-vl-utils wandb liger-kernel mathruler \\\\\\\\\\\\\\\\\\\\\\\\n+    pytest yapf py-spy pyext pre-commit ruff\\\\\\\\n+\\\\\\\\n+# Install flash_attn-2.7.4.post1\\\\\\\\n+RUN wget -nv https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl && \\\\\\\\\\\\\\\\\\\\\\\\n+    pip install --no-cache-dir flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\\\\\\\\n+\\\\\\\\n+# Fix cv2\\\\\\\\n+RUN pip uninstall -y pynvml nvidia-ml-py && \\\\\\\\\\\\\\\\\\\\\\\\n+    pip install --no-cache-dir nvidia-ml-py>=12.560.30 opencv-python-headless==4.8.0.74 fastapi==0.115.6 && \\\\\\\\\\\\\\\\\\\\\\\\n+    pip install --no-cache-dir --upgrade optree>=0.13.0\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"0ffd85944f3cb928bce00f1be3d9f37ee58b349e\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"README.md\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":4,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":5,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/README.md\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/README.md\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/README.md?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -35,13 +35,16 @@ EasyR1 is efficient and scalable due to the design of **[HybirdEngine](https://a\\\\\\\\n - Python 3.9+\\\\\\\\n - transformers>=4.49.0\\\\\\\\n - flash-attn>=2.4.3\\\\\\\\n-- vllm>=0.7.3 (0.8.0 is recommended)\\\\\\\\n+- vllm>=0.7.3\\\\\\\\n \\\\\\\\n We provide a [Dockerfile](./Dockerfile) to easily build environments.\\\\\\\\n \\\\\\\\n We recommend using the [pre-built docker image](https://hub.docker.com/r/hiyouga/verl) in EasyR1.\\\\\\\\n \\\\\\\\n ```bash\\\\\\\\n+# stable\\\\\\\\n+docker pull hiyouga/verl:ngc-th2.5.1-cu120-vllm0.7.4-hotfix\\\\\\\\n+# nightly\\\\\\\\n docker pull hiyouga/verl:ngc-th2.6.0-cu120-vllm0.8.0\\\\\\\\n ```\\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"620bf80b38c3bcdc8c2a916934400be9c4a30fc0\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/baselines/qwen2_5_vl_3b_clevr.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":0,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":3,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/examples%2Fbaselines%2Fqwen2_5_vl_3b_clevr.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/examples%2Fbaselines%2Fqwen2_5_vl_3b_clevr.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fbaselines%2Fqwen2_5_vl_3b_clevr.sh?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -1,8 +1,5 @@\\\\\\\\n set -x\\\\\\\\n \\\\\\\\n-export VLLM_ATTENTION_BACKEND=XFORMERS\\\\\\\\n-export VLLM_USE_V1=0\\\\\\\\n-\\\\\\\\n MODEL_PATH=Qwen/Qwen2.5-VL-3B-Instruct  # replace it with your local file path\\\\\\\\n \\\\\\\\n SYSTEM_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"7123b377a418c230b91aa59ee4b23c0e4bf6aca8\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/baselines/qwen2_5_vl_3b_geoqa8k.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":0,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":3,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/examples%2Fbaselines%2Fqwen2_5_vl_3b_geoqa8k.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/examples%2Fbaselines%2Fqwen2_5_vl_3b_geoqa8k.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fbaselines%2Fqwen2_5_vl_3b_geoqa8k.sh?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -1,8 +1,5 @@\\\\\\\\n set -x\\\\\\\\n \\\\\\\\n-export VLLM_ATTENTION_BACKEND=XFORMERS\\\\\\\\n-export VLLM_USE_V1=0\\\\\\\\n-\\\\\\\\n MODEL_PATH=Qwen/Qwen2.5-VL-3B-Instruct  # replace it with your local file path\\\\\\\\n \\\\\\\\n SYSTEM_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"7bcc0c50738b4e8251b9ae9d130ec03e7678eb8e\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/config.yaml\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":4,\\\\\\\"deletions\\\\\\\":5,\\\\\\\"changes\\\\\\\":9,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fconfig.yaml?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -46,7 +46,7 @@ worker:\\\\\\\\n   rollout:\\\\\\\\n     temperature: 1.0\\\\\\\\n     n: 5\\\\\\\\n-    gpu_memory_utilization: 0.5\\\\\\\\n+    gpu_memory_utilization: 0.6\\\\\\\\n     enforce_eager: false\\\\\\\\n     enable_chunked_prefill: false\\\\\\\\n     tensor_parallel_size: 2\\\\\\\\n@@ -74,12 +74,11 @@ trainer:\\\\\\\\n   experiment_name: qwen2_5_7b_math_grpo\\\\\\\\n   n_gpus_per_node: 8\\\\\\\\n   nnodes: 1\\\\\\\\n-  val_freq: 5\\\\\\\\n+  val_freq: 5  # -1 to disable\\\\\\\\n   val_before_train: true\\\\\\\\n   val_only: false\\\\\\\\n   val_generations_to_log: 1\\\\\\\\n-  save_freq: 5\\\\\\\\n-  remove_previous_ckpt: false\\\\\\\\n-  remove_ckpt_after_load: false\\\\\\\\n+  save_freq: 5  # -1 to disable\\\\\\\\n+  save_limit: 3  # -1 to disable\\\\\\\\n   save_checkpoint_path: null\\\\\\\\n   load_checkpoint_path: null\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"7a1e653f66e29df5342dfb1b4271353b3ca1e14e\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"added\\\\\\\",\\\\\\\"additions\\\\\\\":19,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":19,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -0,0 +1,19 @@\\\\\\\\n+set -x\\\\\\\\n+\\\\\\\\n+MODEL_PATH=Qwen/Qwen2.5-VL-32B-Instruct  # replace it with your local file path\\\\\\\\n+\\\\\\\\n+SYSTEM_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal monologue and then provide the final answer.\\\\\\\\n+ The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in \\\\\\\\\\\\\\\\boxed{}.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n+    config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n+    data.train_files=hiyouga/geometry3k@train \\\\\\\\\\\\\\\\\\\\\\\\n+    data.val_files=hiyouga/geometry3k@test \\\\\\\\\\\\\\\\\\\\\\\\n+    data.system_prompt=\\\\\\\\\\\\\\\"${SYSTEM_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.actor.micro_batch_size_per_device_for_update=1 \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.actor.micro_batch_size_per_device_for_experience=8 \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.rollout.tensor_parallel_size=8 \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.rollout.enable_chunked_prefill=false \\\\\\\\\\\\\\\\\\\\\\\\n+    trainer.experiment_name=qwen2_5_vl_32b_geo_grpo \\\\\\\\\\\\\\\\\\\\\\\\n+    trainer.n_gpus_per_node=8\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"39d8e4b122da9a7b0d43e937fa691126648c0531\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_vl_7b_geo3k_reinforce.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/examples%2Fqwen2_5_vl_7b_geo3k_reinforce.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/examples%2Fqwen2_5_vl_7b_geo3k_reinforce.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_vl_7b_geo3k_reinforce.sh?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -7,11 +7,11 @@ SYSTEM_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal mono\\\\\\\\n \\\\\\\\n python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n-    algorithm.adv_estimator=reinforce_plus_plus \\\\\\\\\\\\\\\\\\\\\\\\n     data.train_files=hiyouga/geometry3k@train \\\\\\\\\\\\\\\\\\\\\\\\n     data.val_files=hiyouga/geometry3k@test \\\\\\\\\\\\\\\\\\\\\\\\n     data.system_prompt=\\\\\\\\\\\\\\\"${SYSTEM_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.enable_chunked_prefill=false \\\\\\\\\\\\\\\\\\\\\\\\n+    algorithm.adv_estimator=reinforce_plus_plus \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.experiment_name=qwen2_5_vl_7b_geo_reinforce_pp \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.n_gpus_per_node=8\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"5ecd476d7f67238374e85ec06586a5989e20e536\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_vl_7b_geo3k_swanlab.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/examples%2Fqwen2_5_vl_7b_geo3k_swanlab.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/examples%2Fqwen2_5_vl_7b_geo3k_swanlab.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_vl_7b_geo3k_swanlab.sh?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -6,7 +6,7 @@ SYSTEM_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal mono\\\\\\\\n  The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in \\\\\\\\\\\\\\\\boxed{}.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n-    config=examples/grpo_example.yaml \\\\\\\\\\\\\\\\\\\\\\\\n+    config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n     data.train_files=hiyouga/geometry3k@train \\\\\\\\\\\\\\\\\\\\\\\\n     data.val_files=hiyouga/geometry3k@test \\\\\\\\\\\\\\\\\\\\\\\\n     data.system_prompt=\\\\\\\\\\\\\\\"${SYSTEM_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"228067a714f201b65119a065ce9e97b825a20097\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":5,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/verl%2Ftrainer%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/verl%2Ftrainer%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fconfig.py?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -74,10 +74,9 @@ class TrainerConfig:\\\\\\\\n     val_freq: int = -1\\\\\\\\n     val_before_train: bool = True\\\\\\\\n     val_only: bool = False\\\\\\\\n-    val_generations_to_log: int = 1\\\\\\\\n+    val_generations_to_log: int = 0\\\\\\\\n     save_freq: int = -1\\\\\\\\n-    remove_previous_ckpt: bool = False\\\\\\\\n-    remove_ckpt_after_load: bool = False\\\\\\\\n+    save_limit: int = -1\\\\\\\\n     save_checkpoint_path: Optional[str] = None\\\\\\\\n     load_checkpoint_path: Optional[str] = None\\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"7bcbf76a619dafcc5d28f90bc2d3e1c8e83633e8\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/ray_trainer.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":16,\\\\\\\"deletions\\\\\\\":25,\\\\\\\"changes\\\\\\\":41,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fray_trainer.py?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -40,6 +40,7 @@\\\\\\\\n from ..single_controller.ray import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup\\\\\\\\n from ..single_controller.ray.base import create_colocated_worker_cls\\\\\\\\n from ..utils import torch_functional as VF\\\\\\\\n+from ..utils.checkpoint import CHECKPOINT_TRACKER, remove_obsolete_ckpt\\\\\\\\n from ..utils.dataset import RLHFDataset, collate_fn\\\\\\\\n from ..utils.seqlen_balancing import get_seqlen_balanced_partitions, log_seqlen_unbalance\\\\\\\\n from ..utils.tracking import Tracking, ValGenerationsLogger\\\\\\\\n@@ -365,7 +366,7 @@ def _create_dataloader(self) -> None:\\\\\\\\n \\\\\\\\n     def _maybe_log_val_generations(self, inputs: List[str], outputs: List[str], scores: List[float]) -> None:\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Log a table of validation samples\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        if self.config.trainer.val_generations_to_log == 0:\\\\\\\\n+        if self.config.trainer.val_generations_to_log <= 0:\\\\\\\\n             return\\\\\\\\n \\\\\\\\n         # Create tuples of (input, output, score) and sort by input text\\\\\\\\n@@ -475,7 +476,7 @@ def init_workers(self) -> None:\\\\\\\\n         # NOTE: if you want to use a different resource pool for each role, which can support different parallel size,\\\\\\\\n         # you should not use `create_colocated_worker_cls`. Instead, directly pass different resource pool to different worker groups.\\\\\\\\n         # See https://github.com/volcengine/verl/blob/master/examples/ray/tutorial.ipynb for more information.\\\\\\\\n-        all_wg = {}\\\\\\\\n+        all_wg: Dict[str, FSDPWorker] = {}\\\\\\\\n         self.wg_dicts = []\\\\\\\\n         for resource_pool, class_dict in self.resource_pool_to_cls.items():\\\\\\\\n             worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)\\\\\\\\n@@ -486,45 +487,39 @@ def init_workers(self) -> None:\\\\\\\\n             self.wg_dicts.append(wg_dict)\\\\\\\\n \\\\\\\\n         if self.use_critic:\\\\\\\\n-            self.critic_wg: FSDPWorker = all_wg[\\\\\\\\\\\\\\\"critic\\\\\\\\\\\\\\\"]\\\\\\\\n+            self.critic_wg = all_wg[\\\\\\\\\\\\\\\"critic\\\\\\\\\\\\\\\"]\\\\\\\\n             self.critic_wg.init_model()\\\\\\\\n \\\\\\\\n         if self.use_reference_policy:\\\\\\\\n-            self.ref_policy_wg: FSDPWorker = all_wg[\\\\\\\\\\\\\\\"ref\\\\\\\\\\\\\\\"]\\\\\\\\n+            self.ref_policy_wg = all_wg[\\\\\\\\\\\\\\\"ref\\\\\\\\\\\\\\\"]\\\\\\\\n             self.ref_policy_wg.init_model()\\\\\\\\n \\\\\\\\n         if self.use_reward_model:\\\\\\\\n-            self.rm_wg: FSDPWorker = all_wg[\\\\\\\\\\\\\\\"rm\\\\\\\\\\\\\\\"]\\\\\\\\n+            self.rm_wg = all_wg[\\\\\\\\\\\\\\\"rm\\\\\\\\\\\\\\\"]\\\\\\\\n             self.rm_wg.init_model()\\\\\\\\n \\\\\\\\n         # we should create rollout at the end so that vllm can have a better estimation of kv cache memory\\\\\\\\n-        self.actor_rollout_wg: FSDPWorker = all_wg[\\\\\\\\\\\\\\\"actor_rollout\\\\\\\\\\\\\\\"]\\\\\\\\n+        self.actor_rollout_wg = all_wg[\\\\\\\\\\\\\\\"actor_rollout\\\\\\\\\\\\\\\"]\\\\\\\\n         self.actor_rollout_wg.init_model()\\\\\\\\n \\\\\\\\n     def _save_checkpoint(self) -> None:\\\\\\\\n-        # path: {save_checkpoint_path}/global_step_{global_step}/actor\\\\\\\\n+        # path: {save_checkpoint_path}/global_step_{global_step}/{actor,critic}\\\\\\\\n+        remove_obsolete_ckpt(\\\\\\\\n+            self.config.trainer.save_checkpoint_path, self.global_step, self.config.trainer.save_limit\\\\\\\\n+        )\\\\\\\\n         folder_path = os.path.join(self.config.trainer.save_checkpoint_path, f\\\\\\\\\\\\\\\"global_step_{self.global_step}\\\\\\\\\\\\\\\")\\\\\\\\n         actor_path = os.path.join(folder_path, \\\\\\\\\\\\\\\"actor\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-        self.actor_rollout_wg.save_checkpoint(\\\\\\\\n-            actor_path,\\\\\\\\n-            self.global_step,\\\\\\\\n-            remove_previous_ckpt=self.config.trainer.remove_previous_ckpt,\\\\\\\\n-        )\\\\\\\\n+        self.actor_rollout_wg.save_checkpoint(actor_path)\\\\\\\\n \\\\\\\\n         if self.use_critic:\\\\\\\\n             critic_path = os.path.join(folder_path, \\\\\\\\\\\\\\\"critic\\\\\\\\\\\\\\\")\\\\\\\\n-            self.critic_wg.save_checkpoint(\\\\\\\\n-                critic_path,\\\\\\\\n-                self.global_step,\\\\\\\\n-                remove_previous_ckpt=self.config.trainer.remove_previous_ckpt,\\\\\\\\n-            )\\\\\\\\n+            self.critic_wg.save_checkpoint(critic_path)\\\\\\\\n \\\\\\\\n         dataloader_path = os.path.join(folder_path, \\\\\\\\\\\\\\\"dataloader.pt\\\\\\\\\\\\\\\")\\\\\\\\n         dataloader_state_dict = self.train_dataloader.state_dict()\\\\\\\\n         torch.save(dataloader_state_dict, dataloader_path)\\\\\\\\n \\\\\\\\n-        last_global_step_path = os.path.join(self.config.trainer.save_checkpoint_path, \\\\\\\\\\\\\\\"latest_global_step.txt\\\\\\\\\\\\\\\")\\\\\\\\n+        last_global_step_path = os.path.join(self.config.trainer.save_checkpoint_path, CHECKPOINT_TRACKER)\\\\\\\\n         with open(last_global_step_path, \\\\\\\\\\\\\\\"w\\\\\\\\\\\\\\\") as f:\\\\\\\\n             f.write(str(self.global_step))\\\\\\\\n \\\\\\\\n@@ -538,14 +533,10 @@ def _load_checkpoint(self) -> None:\\\\\\\\n         print(f\\\\\\\\\\\\\\\"Load from checkpoint: {self.config.trainer.load_checkpoint_path}.\\\\\\\\\\\\\\\")\\\\\\\\n         self.global_step = int(self.config.trainer.load_checkpoint_path.strip(os.path.sep).split(\\\\\\\\\\\\\\\"global_step_\\\\\\\\\\\\\\\")[-1])\\\\\\\\n         actor_path = os.path.join(self.config.trainer.load_checkpoint_path, \\\\\\\\\\\\\\\"actor\\\\\\\\\\\\\\\")\\\\\\\\n-        self.actor_rollout_wg.load_checkpoint(\\\\\\\\n-            actor_path, remove_ckpt_after_load=self.config.trainer.remove_ckpt_after_load\\\\\\\\n-        )\\\\\\\\n+        self.actor_rollout_wg.load_checkpoint(actor_path)\\\\\\\\n         if self.use_critic:\\\\\\\\n             critic_path = os.path.join(self.config.trainer.load_checkpoint_path, \\\\\\\\\\\\\\\"critic\\\\\\\\\\\\\\\")\\\\\\\\n-            self.critic_wg.load_checkpoint(\\\\\\\\n-                critic_path, remove_ckpt_after_load=self.config.trainer.remove_ckpt_after_load\\\\\\\\n-            )\\\\\\\\n+            self.critic_wg.load_checkpoint(critic_path)\\\\\\\\n \\\\\\\\n         dataloader_path = os.path.join(self.config.trainer.load_checkpoint_path, \\\\\\\\\\\\\\\"dataloader.pt\\\\\\\\\\\\\\\")\\\\\\\\n         if os.path.exists(dataloader_path):\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"de1a2fcb2b6b796760785a5de9f485eaaa251403\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/checkpoint/__init__.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":5,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":5,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/verl%2Futils%2Fcheckpoint%2F__init__.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/verl%2Futils%2Fcheckpoint%2F__init__.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fcheckpoint%2F__init__.py?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -11,3 +11,8 @@\\\\\\\\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\\\\\\\n # See the License for the specific language governing permissions and\\\\\\\\n # limitations under the License.\\\\\\\\n+\\\\\\\\n+from .checkpoint_manager import CHECKPOINT_TRACKER, remove_obsolete_ckpt\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+__all__ = [\\\\\\\\\\\\\\\"CHECKPOINT_TRACKER\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"remove_obsolete_ckpt\\\\\\\\\\\\\\\"]\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"749b60cb53c749a03348da4f626ee461741f845e\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/checkpoint/checkpoint_manager.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":33,\\\\\\\"deletions\\\\\\\":21,\\\\\\\"changes\\\\\\\":54,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/verl%2Futils%2Fcheckpoint%2Fcheckpoint_manager.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/verl%2Futils%2Fcheckpoint%2Fcheckpoint_manager.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fcheckpoint%2Fcheckpoint_manager.py?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -14,10 +14,11 @@\\\\\\\\n \\\\\\\\n import os\\\\\\\\n import random\\\\\\\\n+import re\\\\\\\\n import shutil\\\\\\\\n import tempfile\\\\\\\\n from abc import ABC, abstractmethod\\\\\\\\n-from typing import Any, Dict, Union\\\\\\\\n+from typing import Any, Dict, Optional, Union\\\\\\\\n \\\\\\\\n import numpy as np\\\\\\\\n import torch\\\\\\\\n@@ -27,7 +28,7 @@\\\\\\\\n from transformers import PreTrainedTokenizer, ProcessorMixin\\\\\\\\n \\\\\\\\n \\\\\\\\n-CHECKPOINT_TRACKER = \\\\\\\\\\\\\\\"latest_checkpointed_iteration.txt\\\\\\\\\\\\\\\"\\\\\\\\n+CHECKPOINT_TRACKER = \\\\\\\\\\\\\\\"latest_global_step.txt\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n \\\\\\\\n class BaseCheckpointManager(ABC):\\\\\\\\n@@ -52,9 +53,6 @@ def __init__(\\\\\\\\n         lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\\\\\\\\n         processing_class: Union[PreTrainedTokenizer, ProcessorMixin],\\\\\\\\n     ):\\\\\\\\n-        self.previous_global_step = None\\\\\\\\n-        self.previous_save_local_path = None\\\\\\\\n-\\\\\\\\n         self.model = model\\\\\\\\n         self.optimizer = optimizer\\\\\\\\n         self.lr_scheduler = lr_scheduler\\\\\\\\n@@ -72,20 +70,8 @@ def load_checkpoint(self, *args, **kwargs):\\\\\\\\n     def save_checkpoint(self, *args, **kwargs):\\\\\\\\n         raise NotImplementedError\\\\\\\\n \\\\\\\\n-    def remove_previous_save_local_path(self):\\\\\\\\n-        if not self.previous_save_local_path:\\\\\\\\n-            return\\\\\\\\n-\\\\\\\\n-        abs_path = os.path.abspath(self.previous_save_local_path)\\\\\\\\n-        print(f\\\\\\\\\\\\\\\"Checkpoint manager remove previous save local path: {abs_path}\\\\\\\\\\\\\\\")\\\\\\\\n-        if not os.path.exists(abs_path):\\\\\\\\n-            return\\\\\\\\n-\\\\\\\\n-        # remove previous local_path\\\\\\\\n-        shutil.rmtree(abs_path, ignore_errors=True)\\\\\\\\n-\\\\\\\\n     @staticmethod\\\\\\\\n-    def local_mkdir(path) -> str:\\\\\\\\n+    def local_mkdir(path: str) -> str:\\\\\\\\n         if not os.path.isabs(path):\\\\\\\\n             working_dir = os.getcwd()\\\\\\\\n             path = os.path.join(working_dir, path)\\\\\\\\n@@ -114,14 +100,14 @@ def get_rng_state() -> Dict[str, Any]:\\\\\\\\n         return rng_state\\\\\\\\n \\\\\\\\n     @staticmethod\\\\\\\\n-    def load_rng_state(rng_state: Dict[str, Any]) -> None:\\\\\\\\n+    def load_rng_state(rng_state: Dict[str, Any]):\\\\\\\\n         torch.set_rng_state(rng_state[\\\\\\\\\\\\\\\"cpu\\\\\\\\\\\\\\\"])\\\\\\\\n         torch.cuda.set_rng_state(rng_state[\\\\\\\\\\\\\\\"cuda\\\\\\\\\\\\\\\"])\\\\\\\\n         np.random.set_state(rng_state[\\\\\\\\\\\\\\\"numpy\\\\\\\\\\\\\\\"])\\\\\\\\n         random.setstate(rng_state[\\\\\\\\\\\\\\\"random\\\\\\\\\\\\\\\"])\\\\\\\\n \\\\\\\\n \\\\\\\\n-def find_latest_ckpt_path(path, directory_format=\\\\\\\\\\\\\\\"global_step_{}\\\\\\\\\\\\\\\"):\\\\\\\\n+def find_latest_ckpt_path(path: Optional[str] = None, directory_format: str = \\\\\\\\\\\\\\\"global_step_{}\\\\\\\\\\\\\\\") -> Optional[str]:\\\\\\\\n     if path is None:\\\\\\\\n         return None\\\\\\\\n \\\\\\\\n@@ -132,6 +118,7 @@ def find_latest_ckpt_path(path, directory_format=\\\\\\\\\\\\\\\"global_step_{}\\\\\\\\\\\\\\\"):\\\\\\\\n \\\\\\\\n     with open(tracker_file, \\\\\\\\\\\\\\\"rb\\\\\\\\\\\\\\\") as f:\\\\\\\\n         iteration = int(f.read().decode())\\\\\\\\n+\\\\\\\\n     ckpt_path = os.path.join(path, directory_format.format(iteration))\\\\\\\\n     if not os.path.exists(ckpt_path):\\\\\\\\n         print(\\\\\\\\\\\\\\\"Checkpoint does not exist: %s\\\\\\\\\\\\\\\", ckpt_path)\\\\\\\\n@@ -141,8 +128,33 @@ def find_latest_ckpt_path(path, directory_format=\\\\\\\\\\\\\\\"global_step_{}\\\\\\\\\\\\\\\"):\\\\\\\\n     return ckpt_path\\\\\\\\n \\\\\\\\n \\\\\\\\n-def get_checkpoint_tracker_filename(root_path: str):\\\\\\\\n+def get_checkpoint_tracker_filename(root_path: str) -> str:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     Tracker file rescords the latest chckpoint during training to restart from.\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     return os.path.join(root_path, CHECKPOINT_TRACKER)\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def remove_obsolete_ckpt(path: str, global_step: int, save_limit: int = -1, directory_format: str = \\\\\\\\\\\\\\\"global_step_{}\\\\\\\\\\\\\\\"):\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    Remove the obsolete checkpoints that exceed the save_limit.\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    if save_limit <= 0:\\\\\\\\n+        return\\\\\\\\n+\\\\\\\\n+    if not os.path.exists(path):\\\\\\\\n+        return\\\\\\\\n+\\\\\\\\n+    pattern = re.escape(directory_format).replace(r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\{\\\\\\\\\\\\\\\\}\\\\\\\\\\\\\\\", r\\\\\\\\\\\\\\\"(\\\\\\\\\\\\\\\\d+)\\\\\\\\\\\\\\\")\\\\\\\\n+    ckpt_folders = []\\\\\\\\n+    for folder in os.listdir(path):\\\\\\\\n+        if match := re.match(pattern, folder):\\\\\\\\n+            step = int(match.group(1))\\\\\\\\n+            if step < global_step:\\\\\\\\n+                ckpt_folders.append((step, folder))\\\\\\\\n+\\\\\\\\n+    ckpt_folders.sort(reverse=True)\\\\\\\\n+    for _, folder in ckpt_folders[save_limit - 1 :]:\\\\\\\\n+        folder_path = os.path.join(path, folder)\\\\\\\\n+        shutil.rmtree(folder_path, ignore_errors=True)\\\\\\\\n+        print(f\\\\\\\\\\\\\\\"Removed obsolete checkpoint: {folder_path}\\\\\\\\\\\\\\\")\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"ab4086ee8d8a6869a615e84c21a630b2aea8e93b\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/checkpoint/fsdp_checkpoint_manager.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":23,\\\\\\\"deletions\\\\\\\":41,\\\\\\\"changes\\\\\\\":64,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/verl%2Futils%2Fcheckpoint%2Ffsdp_checkpoint_manager.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/verl%2Futils%2Fcheckpoint%2Ffsdp_checkpoint_manager.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fcheckpoint%2Ffsdp_checkpoint_manager.py?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -14,7 +14,7 @@\\\\\\\\n \\\\\\\\n import os\\\\\\\\n import warnings\\\\\\\\n-from typing import Union\\\\\\\\n+from typing import Optional, Union\\\\\\\\n \\\\\\\\n import torch\\\\\\\\n import torch.distributed as dist\\\\\\\\n@@ -49,30 +49,20 @@ def __init__(\\\\\\\\n     ):\\\\\\\\n         super().__init__(model, optimizer, lr_scheduler, processing_class)\\\\\\\\n \\\\\\\\n-    def load_checkpoint(self, path: str = None, remove_ckpt_after_load: bool = False):\\\\\\\\n+    def load_checkpoint(self, path: Optional[str] = None):\\\\\\\\n         if path is None:\\\\\\\\n             return\\\\\\\\n \\\\\\\\n         # every rank download its own checkpoint\\\\\\\\n-        local_model_path = os.path.join(path, f\\\\\\\\\\\\\\\"model_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n-        local_optim_path = os.path.join(path, f\\\\\\\\\\\\\\\"optim_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n-        local_extra_state_path = os.path.join(path, f\\\\\\\\\\\\\\\"extra_state_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n-        print(\\\\\\\\n-            f\\\\\\\\\\\\\\\"[rank-{self.rank}]: Loading from {local_model_path} and {local_optim_path} and {local_extra_state_path}\\\\\\\\\\\\\\\"\\\\\\\\n-        )\\\\\\\\n-        model_state_dict = torch.load(local_model_path, weights_only=False)\\\\\\\\n-        optimizer_state_dict = torch.load(local_optim_path, weights_only=False)\\\\\\\\n-        extra_state_dict = torch.load(local_extra_state_path, weights_only=False)\\\\\\\\n-\\\\\\\\n-        if remove_ckpt_after_load:\\\\\\\\n-            try:\\\\\\\\n-                os.remove(local_model_path)\\\\\\\\n-                os.remove(local_optim_path)\\\\\\\\n-                os.remove(local_extra_state_path)\\\\\\\\n-            except Exception as e:\\\\\\\\n-                print(f\\\\\\\\\\\\\\\"[rank-{self.rank}]: remove ckpt file after loading failed, exception {e} will be ignored.\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n+        model_path = os.path.join(path, f\\\\\\\\\\\\\\\"model_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n+        optim_path = os.path.join(path, f\\\\\\\\\\\\\\\"optim_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n+        extra_state_path = os.path.join(path, f\\\\\\\\\\\\\\\"extra_state_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n+        print(f\\\\\\\\\\\\\\\"[rank-{self.rank}]: Loading from {model_path} and {optim_path} and {extra_state_path}.\\\\\\\\\\\\\\\")\\\\\\\\n+        model_state_dict = torch.load(model_path, weights_only=False)\\\\\\\\n+        optimizer_state_dict = torch.load(optim_path, weights_only=False)\\\\\\\\n+        extra_state_dict = torch.load(extra_state_path, weights_only=False)\\\\\\\\n         lr_scheduler_state_dict = extra_state_dict[\\\\\\\\\\\\\\\"lr_scheduler\\\\\\\\\\\\\\\"]\\\\\\\\n+\\\\\\\\n         state_dict_config = ShardedStateDictConfig(offload_to_cpu=True)\\\\\\\\n         optim_config = ShardedOptimStateDictConfig(offload_to_cpu=True)\\\\\\\\n         with warnings.catch_warnings():\\\\\\\\n@@ -82,22 +72,15 @@ def load_checkpoint(self, path: str = None, remove_ckpt_after_load: bool = False\\\\\\\\n                 if self.optimizer is not None:\\\\\\\\n                     self.optimizer.load_state_dict(optimizer_state_dict)\\\\\\\\n \\\\\\\\n-        # recover random state\\\\\\\\n-        if \\\\\\\\\\\\\\\"rng\\\\\\\\\\\\\\\" in extra_state_dict:\\\\\\\\n-            self.load_rng_state(extra_state_dict[\\\\\\\\\\\\\\\"rng\\\\\\\\\\\\\\\"])\\\\\\\\n-\\\\\\\\n         if self.lr_scheduler is not None:\\\\\\\\n             self.lr_scheduler.load_state_dict(lr_scheduler_state_dict)\\\\\\\\n \\\\\\\\n-    def save_checkpoint(self, local_path: str, global_step: int, remove_previous_ckpt: bool = False):\\\\\\\\n-        # record the previous global step\\\\\\\\n-        self.previous_global_step = global_step\\\\\\\\n-\\\\\\\\n-        # remove previous local_path\\\\\\\\n-        if remove_previous_ckpt:\\\\\\\\n-            self.remove_previous_save_local_path()\\\\\\\\n+        # recover random state\\\\\\\\n+        if \\\\\\\\\\\\\\\"rng\\\\\\\\\\\\\\\" in extra_state_dict:\\\\\\\\n+            self.load_rng_state(extra_state_dict[\\\\\\\\\\\\\\\"rng\\\\\\\\\\\\\\\"])\\\\\\\\n \\\\\\\\n-        local_path = self.local_mkdir(local_path)\\\\\\\\n+    def save_checkpoint(self, path: str):\\\\\\\\n+        path = self.local_mkdir(path)\\\\\\\\n         dist.barrier()\\\\\\\\n \\\\\\\\n         # every rank will save its own model and optim shard\\\\\\\\n@@ -121,9 +104,9 @@ def save_checkpoint(self, local_path: str, global_step: int, remove_previous_ckp\\\\\\\\n                     \\\\\\\\\\\\\\\"lr_scheduler\\\\\\\\\\\\\\\": lr_scheduler_state_dict,\\\\\\\\n                     \\\\\\\\\\\\\\\"rng\\\\\\\\\\\\\\\": self.get_rng_state(),\\\\\\\\n                 }\\\\\\\\n-                model_path = os.path.join(local_path, f\\\\\\\\\\\\\\\"model_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n-                optim_path = os.path.join(local_path, f\\\\\\\\\\\\\\\"optim_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n-                extra_path = os.path.join(local_path, f\\\\\\\\\\\\\\\"extra_state_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n+                model_path = os.path.join(path, f\\\\\\\\\\\\\\\"model_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n+                optim_path = os.path.join(path, f\\\\\\\\\\\\\\\"optim_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n+                extra_path = os.path.join(path, f\\\\\\\\\\\\\\\"extra_state_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n                 print(f\\\\\\\\\\\\\\\"[rank-{self.rank}]: Saving model to {os.path.abspath(model_path)}.\\\\\\\\\\\\\\\")\\\\\\\\n                 print(f\\\\\\\\\\\\\\\"[rank-{self.rank}]: Saving checkpoint to {os.path.abspath(model_path)}.\\\\\\\\\\\\\\\")\\\\\\\\n@@ -138,12 +121,11 @@ def save_checkpoint(self, local_path: str, global_step: int, remove_previous_ckp\\\\\\\\n         dist.barrier()\\\\\\\\n \\\\\\\\n         if self.rank == 0:\\\\\\\\n-            hf_local_path = os.path.join(local_path, \\\\\\\\\\\\\\\"huggingface\\\\\\\\\\\\\\\")\\\\\\\\n-            os.makedirs(hf_local_path, exist_ok=True)\\\\\\\\n+            hf_path = os.path.join(path, \\\\\\\\\\\\\\\"huggingface\\\\\\\\\\\\\\\")\\\\\\\\n+            os.makedirs(hf_path, exist_ok=True)\\\\\\\\n             assert isinstance(self.model._fsdp_wrapped_module, PreTrainedModel)\\\\\\\\n-            self.model._fsdp_wrapped_module.config.save_pretrained(hf_local_path)\\\\\\\\n-            self.model._fsdp_wrapped_module.generation_config.save_pretrained(hf_local_path)\\\\\\\\n-            self.processing_class.save_pretrained(hf_local_path)\\\\\\\\n+            self.model._fsdp_wrapped_module.config.save_pretrained(hf_path)\\\\\\\\n+            self.model._fsdp_wrapped_module.generation_config.save_pretrained(hf_path)\\\\\\\\n+            self.processing_class.save_pretrained(hf_path)\\\\\\\\n \\\\\\\\n         dist.barrier()\\\\\\\\n-        self.previous_save_local_path = local_path\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"cdb769df5238629847e3092689aff2d02bc6d42a\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/fsdp_workers.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":5,\\\\\\\"deletions\\\\\\\":8,\\\\\\\"changes\\\\\\\":13,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Ffsdp_workers.py?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -315,6 +315,7 @@ def _build_model_optimizer(\\\\\\\\n                 lr=optim_config.lr,\\\\\\\\n                 betas=optim_config.betas,\\\\\\\\n                 weight_decay=optim_config.weight_decay,\\\\\\\\n+                fused=True,\\\\\\\\n             )\\\\\\\\n             num_warmup_steps = int(optim_config.lr_warmup_ratio * optim_config.training_steps)\\\\\\\\n             self.lr_scheduler = get_constant_schedule_with_warmup(\\\\\\\\n@@ -411,26 +412,22 @@ def init_model(self):\\\\\\\\n             )\\\\\\\\n \\\\\\\\n     @register(dispatch_mode=Dispatch.ONE_TO_ALL)\\\\\\\\n-    def save_checkpoint(self, path: str, global_step: int = 0, remove_previous_ckpt: bool = False):\\\\\\\\n+    def save_checkpoint(self, path: str):\\\\\\\\n         assert self._is_actor or self._is_critic\\\\\\\\n         if self._use_param_offload:\\\\\\\\n             load_fsdp_model(self.fsdp_module)\\\\\\\\n \\\\\\\\n-        self.checkpoint_manager.save_checkpoint(\\\\\\\\n-            local_path=path,\\\\\\\\n-            global_step=global_step,\\\\\\\\n-            remove_previous_ckpt=remove_previous_ckpt,\\\\\\\\n-        )\\\\\\\\n+        self.checkpoint_manager.save_checkpoint(path)\\\\\\\\n         dist.barrier()\\\\\\\\n         if self._use_param_offload:\\\\\\\\n             offload_fsdp_model(self.fsdp_module)\\\\\\\\n \\\\\\\\n     @register(dispatch_mode=Dispatch.ONE_TO_ALL)\\\\\\\\n-    def load_checkpoint(self, path: str, remove_ckpt_after_load: bool = False):\\\\\\\\n+    def load_checkpoint(self, path: str):\\\\\\\\n         if self._use_param_offload:\\\\\\\\n             load_fsdp_model(self.fsdp_module)\\\\\\\\n \\\\\\\\n-        self.checkpoint_manager.load_checkpoint(path=path, remove_ckpt_after_load=remove_ckpt_after_load)\\\\\\\\n+        self.checkpoint_manager.load_checkpoint(path)\\\\\\\\n         dist.barrier()\\\\\\\\n         if self._use_param_offload:\\\\\\\\n             offload_fsdp_model(self.fsdp_module)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"656ce9e5bb5a4fca352fe4d15f2b06b2784a43af\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/db81d107dfc70dc05fd10d955b90289e56e4df17/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/db81d107dfc70dc05fd10d955b90289e56e4df17/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py?ref=db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -81,6 +81,7 @@ def __init__(self, model_path: str, config: RolloutConfig, tokenizer: PreTrained\\\\\\\\n             enable_sleep_mode=True,\\\\\\\\n             distributed_executor_backend=\\\\\\\\\\\\\\\"external_launcher\\\\\\\\\\\\\\\",\\\\\\\\n             disable_custom_all_reduce=True,\\\\\\\\n+            disable_mm_preprocessor_cache=True,\\\\\\\\n             disable_log_stats=config.disable_log_stats,\\\\\\\\n             enable_chunked_prefill=config.enable_chunked_prefill,\\\\\\\\n             **vllm_init_kwargs,\\\\\\\"}],\\\\\\\"sha\\\\\\\":\\\\\\\"db81d107dfc70dc05fd10d955b90289e56e4df17\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"committer\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"noreply@github.com\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-25T12:29:59Z\\\\\\\"},\\\\\\\"author\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"hiyouga@buaa.edu.cn\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-25T12:29:59Z\\\\\\\"}}},\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "I can see this commit includes changes to `examples/config.yaml`. Let me examine the patch for this file to see what changed:\n\nLooking at the patch for `examples/config.yaml`, I can see it changed:\n- `gpu_memory_utilization: 0.5` to `gpu_memory_utilization: 0.6`\n- `val_freq: 5` to `val_freq: 5  # -1 to disable`\n- `save_freq: 5` to `save_freq: 5  # -1 to disable`\n- `save_limit: 3  # -1 to disable`\n\nBut this doesn't seem to contain the micro_batch_size parameter changes. Let me look at a different pattern - let me check if there are any configuration changes in the shell script files that were mentioned in the commit:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"ref\\\": \\\"6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\"}\", \"include_output_fields\": [\"sha\", \"commit.author\", \"commit.committer\", \"files\"]}",
    "call_id": "toolu_01G7qyT3N71hSZYW8y98eHgH",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01G7qyT3N71hSZYW8y98eHgH",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"files\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"9edca3eb0d0967a7539ad9a03165f028775f1e5d\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/config.yaml\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":4,\\\\\\\"deletions\\\\\\\":5,\\\\\\\"changes\\\\\\\":9,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fconfig.yaml?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -14,17 +14,16 @@ data:\\\\\\\\n \\\\\\\\n algorithm:\\\\\\\\n   adv_estimator: grpo\\\\\\\\n-  kl_coef: 0.0\\\\\\\\n+  use_kl_loss: true\\\\\\\\n+  kl_penalty: low_var_kl\\\\\\\\n+  kl_coef: 1.0e-2\\\\\\\\n \\\\\\\\n worker:\\\\\\\\n   actor:\\\\\\\\n     global_batch_size: 128\\\\\\\\n     micro_batch_size_per_device_for_update: 4\\\\\\\\n     micro_batch_size_per_device_for_experience: 16\\\\\\\\n     max_grad_norm: 1.0\\\\\\\\n-    use_kl_loss: true\\\\\\\\n-    kl_loss_coef: 1.0e-2\\\\\\\\n-    kl_loss_type: low_var_kl\\\\\\\\n     padding_free: true\\\\\\\\n     ulysses_sequence_parallel_size: 1\\\\\\\\n     model:\\\\\\\\n@@ -72,7 +71,7 @@ trainer:\\\\\\\\n   total_episodes: 15\\\\\\\\n   logger: [\\\\\\\\\\\\\\\"console\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"wandb\\\\\\\\\\\\\\\"]\\\\\\\\n   project_name: easy_r1\\\\\\\\n-  experiment_name: qwen2_5_7b_math\\\\\\\\n+  experiment_name: qwen2_5_7b_math_grpo\\\\\\\\n   n_gpus_per_node: 8\\\\\\\\n   nnodes: 1\\\\\\\\n   val_freq: 5\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"fed923885d44ca550f0d12215b683ed14a35a2a4\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_vl_3b_geo3k_grpo.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/examples%2Fqwen2_5_vl_3b_geo3k_grpo.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/examples%2Fqwen2_5_vl_3b_geo3k_grpo.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_vl_3b_geo3k_grpo.sh?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -13,5 +13,5 @@ python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.tensor_parallel_size=1 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.enable_chunked_prefill=false \\\\\\\\\\\\\\\\\\\\\\\\n-    trainer.experiment_name=qwen2_5_vl_3b_geo \\\\\\\\\\\\\\\\\\\\\\\\n+    trainer.experiment_name=qwen2_5_vl_3b_geo_grpo \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.n_gpus_per_node=2\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"17f8778922aa84da8cc66d132d2e03a03f85edb5\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_vl_7b_geo3k_swanlab.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/examples%2Fqwen2_5_vl_7b_geo3k_swanlab.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/examples%2Fqwen2_5_vl_7b_geo3k_swanlab.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_vl_7b_geo3k_swanlab.sh?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -12,6 +12,6 @@ python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     data.system_prompt=\\\\\\\\\\\\\\\"${SYSTEM_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.enable_chunked_prefill=false \\\\\\\\\\\\\\\\\\\\\\\\n-    trainer.experiment_name=qwen2_5_vl_7b_geo \\\\\\\\\\\\\\\\\\\\\\\\n+    trainer.experiment_name=qwen2_5_vl_7b_geo_grpo \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.logger=['console','swanlab'] \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.n_gpus_per_node=8\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"bbfa73edc5ed49d98ede3f11bbdeec53b354b9ce\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/models/transformers/flash_attention_utils.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":20,\\\\\\\"changes\\\\\\\":22,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fmodels%2Ftransformers%2Fflash_attention_utils.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fmodels%2Ftransformers%2Fflash_attention_utils.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fmodels%2Ftransformers%2Fflash_attention_utils.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -86,9 +86,7 @@ def _custom_flash_attention_forward(\\\\\\\\n     flash_kwargs = {\\\\\\\\\\\\\\\"window_size\\\\\\\\\\\\\\\": (sliding_window, sliding_window)} if use_sliding_windows else {}\\\\\\\\n \\\\\\\\n     if _flash_supports_deterministic:\\\\\\\\n-        if deterministic is None:\\\\\\\\n-            deterministic = _flash_deterministic_enabled\\\\\\\\n-        flash_kwargs[\\\\\\\\\\\\\\\"deterministic\\\\\\\\\\\\\\\"] = deterministic\\\\\\\\n+        flash_kwargs[\\\\\\\\\\\\\\\"deterministic\\\\\\\\\\\\\\\"] = deterministic if deterministic is not None else _flash_deterministic_enabled\\\\\\\\n \\\\\\\\n     if kwargs.get(\\\\\\\\\\\\\\\"softcap\\\\\\\\\\\\\\\") is not None:\\\\\\\\n         flash_kwargs[\\\\\\\\\\\\\\\"softcap\\\\\\\\\\\\\\\"] = kwargs.pop(\\\\\\\\\\\\\\\"softcap\\\\\\\\\\\\\\\")\\\\\\\\n@@ -114,7 +112,7 @@ def _custom_flash_attention_forward(\\\\\\\\n         batch_size = query_states.size(0)\\\\\\\\n         query_states, key_states, value_states, _, cu_seq_lens, max_seq_lens = prepare_fa2_from_position_ids(\\\\\\\\n             query_states, key_states, value_states, position_ids\\\\\\\\n-        )  # remove channel dimension\\\\\\\\n+        )\\\\\\\\n         cu_seqlens_q, cu_seqlens_k = cu_seq_lens\\\\\\\\n         max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\\\\\\\\n         attn_output = flash_attn_varlen_func(\\\\\\\\n@@ -172,21 +170,6 @@ def flash_attention_forward(\\\\\\\\n     key = key.transpose(1, 2)\\\\\\\\n     value = value.transpose(1, 2)\\\\\\\\n \\\\\\\\n-    # In PEFT, usually we cast the layer norms in float32 for training stability reasons\\\\\\\\n-    # therefore the input hidden states gets silently casted in float32. Hence, we need\\\\\\\\n-    # cast them back in the correct dtype just to be sure everything works as expected.\\\\\\\\n-    # This might slowdown training & inference so it is recommended to not cast the LayerNorms\\\\\\\\n-    # in fp32. (usually our RMSNorm modules handle it correctly)\\\\\\\\n-    target_dtype = None\\\\\\\\n-    if query.dtype == torch.float32:\\\\\\\\n-        if torch.is_autocast_enabled():\\\\\\\\n-            target_dtype = torch.get_autocast_gpu_dtype()\\\\\\\\n-        # Handle the case where the model is quantized\\\\\\\\n-        elif hasattr(module.config, \\\\\\\\\\\\\\\"_pre_quantization_dtype\\\\\\\\\\\\\\\"):\\\\\\\\n-            target_dtype = module.config._pre_quantization_dtype\\\\\\\\n-        else:\\\\\\\\n-            target_dtype = next(layer for layer in module.modules() if isinstance(layer, torch.nn.Linear)).weight.dtype\\\\\\\\n-\\\\\\\\n     # FA2 always relies on the value set in the module, so remove it if present in kwargs to avoid passing it twice\\\\\\\\n     kwargs.pop(\\\\\\\\\\\\\\\"is_causal\\\\\\\\\\\\\\\", None)\\\\\\\\n \\\\\\\\n@@ -202,7 +185,6 @@ def flash_attention_forward(\\\\\\\\n         sliding_window=sliding_window,\\\\\\\\n         softcap=softcap,\\\\\\\\n         use_top_left_mask=_flash_use_top_left_mask,\\\\\\\\n-        target_dtype=target_dtype,\\\\\\\\n         **kwargs,\\\\\\\\n     )\\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"2dda8346ced49127e2afccfcdbcb7992b6f06967\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/protocol.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":3,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fprotocol.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fprotocol.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fprotocol.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -191,7 +191,8 @@ def __len__(self):\\\\\\\\n     def __getitem__(self, item):\\\\\\\\n         tensor_data = self.batch[item]\\\\\\\\n         non_tensor_data = {key: val[item] for key, val in self.non_tensor_batch.items()}\\\\\\\\n-        return DataProtoItem(batch=tensor_data, non_tensor_batch=non_tensor_data, meta_info=self.meta_info)\\\\\\\\n+        return_type = DataProto if isinstance(item, slice) else DataProtoItem\\\\\\\\n+        return return_type(batch=tensor_data, non_tensor_batch=non_tensor_data, meta_info=self.meta_info)\\\\\\\\n \\\\\\\\n     def __getstate__(self):\\\\\\\\n         buffer = io.BytesIO()\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"b0e85a36fbd52d115b68fbd39c2d9a44fa158358\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/single_controller/base/decorator.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":39,\\\\\\\"deletions\\\\\\\":32,\\\\\\\"changes\\\\\\\":71,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fsingle_controller%2Fbase%2Fdecorator.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fsingle_controller%2Fbase%2Fdecorator.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fsingle_controller%2Fbase%2Fdecorator.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -15,7 +15,7 @@\\\\\\\\n from enum import Enum, auto\\\\\\\\n from functools import wraps\\\\\\\\n from types import FunctionType\\\\\\\\n-from typing import TYPE_CHECKING, List\\\\\\\\n+from typing import TYPE_CHECKING, Dict, List, Literal, Union\\\\\\\\n \\\\\\\\n import ray\\\\\\\\n \\\\\\\\n@@ -45,16 +45,16 @@ class Execute(Enum):\\\\\\\\n     RANK_ZERO = 1\\\\\\\\n \\\\\\\\n \\\\\\\\n-def _split_args_kwargs_data_proto(chunks, *args, **kwargs):\\\\\\\\n+def _split_args_kwargs_data_proto(chunks: int, *args, **kwargs):\\\\\\\\n     splitted_args = []\\\\\\\\n     for arg in args:\\\\\\\\n         assert isinstance(arg, (DataProto, DataProtoFuture))\\\\\\\\n         splitted_args.append(arg.chunk(chunks=chunks))\\\\\\\\n \\\\\\\\n     splitted_kwargs = {}\\\\\\\\n-    for key, val in kwargs.items():\\\\\\\\n-        assert isinstance(val, (DataProto, DataProtoFuture))\\\\\\\\n-        splitted_kwargs[key] = val.chunk(chunks=chunks)\\\\\\\\n+    for key, value in kwargs.items():\\\\\\\\n+        assert isinstance(value, (DataProto, DataProtoFuture))\\\\\\\\n+        splitted_kwargs[key] = value.chunk(chunks=chunks)\\\\\\\\n \\\\\\\\n     return splitted_args, splitted_kwargs\\\\\\\\n \\\\\\\\n@@ -73,34 +73,34 @@ def collect_all_to_all(worker_group: \\\\\\\\\\\\\\\"WorkerGroup\\\\\\\\\\\\\\\", output):\\\\\\\\n     return output\\\\\\\\n \\\\\\\\n \\\\\\\\n-def _concat_data_proto_or_future(output: List):\\\\\\\\n+def _concat_data_proto_or_future(outputs: List[DataProto]) -> DataProto:\\\\\\\\n     # make sure all the elements in output has the same type\\\\\\\\n-    for o in output:\\\\\\\\n-        assert type(o) is type(output[0])\\\\\\\\n+    for output in outputs:\\\\\\\\n+        assert type(output) is type(outputs[0])\\\\\\\\n \\\\\\\\n-    o = output[0]\\\\\\\\n+    output = outputs[0]\\\\\\\\n \\\\\\\\n-    if isinstance(o, DataProto):\\\\\\\\n-        return DataProto.concat(output)\\\\\\\\n-    elif isinstance(o, ray.ObjectRef):\\\\\\\\n-        return DataProtoFuture.concat(output)\\\\\\\\n+    if isinstance(output, DataProto):\\\\\\\\n+        return DataProto.concat(outputs)\\\\\\\\n+    elif isinstance(output, ray.ObjectRef):\\\\\\\\n+        return DataProtoFuture.concat(outputs)\\\\\\\\n     else:\\\\\\\\n         raise NotImplementedError\\\\\\\\n \\\\\\\\n \\\\\\\\n def dispatch_dp_compute(worker_group: \\\\\\\\\\\\\\\"WorkerGroup\\\\\\\\\\\\\\\", *args, **kwargs):\\\\\\\\n-    for value in args:\\\\\\\\n-        assert isinstance(value, (tuple, list)) and len(value) == worker_group.world_size\\\\\\\\n+    for arg in args:\\\\\\\\n+        assert isinstance(arg, (tuple, list)) and len(arg) == worker_group.world_size\\\\\\\\n \\\\\\\\n     for value in kwargs.values():\\\\\\\\n         assert isinstance(value, (tuple, list)) and len(value) == worker_group.world_size\\\\\\\\n \\\\\\\\n     return args, kwargs\\\\\\\\n \\\\\\\\n \\\\\\\\n-def collect_dp_compute(worker_group: \\\\\\\\\\\\\\\"WorkerGroup\\\\\\\\\\\\\\\", output):\\\\\\\\n-    assert len(output) == worker_group.world_size\\\\\\\\n-    return output\\\\\\\\n+def collect_dp_compute(worker_group: \\\\\\\\\\\\\\\"WorkerGroup\\\\\\\\\\\\\\\", outputs: List[DataProto]) -> List[DataProto]:\\\\\\\\n+    assert len(outputs) == worker_group.world_size\\\\\\\\n+    return outputs\\\\\\\\n \\\\\\\\n \\\\\\\\n def dispatch_dp_compute_data_proto(worker_group: \\\\\\\\\\\\\\\"WorkerGroup\\\\\\\\\\\\\\\", *args, **kwargs):\\\\\\\\n@@ -115,15 +115,15 @@ def dispatch_dp_compute_data_proto_with_func(worker_group: \\\\\\\\\\\\\\\"WorkerGroup\\\\\\\\\\\\\\\", *args,\\\\\\\\n     return splitted_args_with_func, splitted_kwargs\\\\\\\\n \\\\\\\\n \\\\\\\\n-def collect_dp_compute_data_proto(worker_group: \\\\\\\\\\\\\\\"WorkerGroup\\\\\\\\\\\\\\\", output):\\\\\\\\n-    for o in output:\\\\\\\\n-        assert isinstance(o, (DataProto, ray.ObjectRef)), f\\\\\\\\\\\\\\\"expecting {o} to be DataProto, but got {type(o)}.\\\\\\\\\\\\\\\"\\\\\\\\n+def collect_dp_compute_data_proto(worker_group: \\\\\\\\\\\\\\\"WorkerGroup\\\\\\\\\\\\\\\", outputs: List[DataProto]) -> DataProto:\\\\\\\\n+    for output in outputs:\\\\\\\\n+        assert isinstance(output, (DataProto, ray.ObjectRef)), f\\\\\\\\\\\\\\\"Expect a DataProto, but got {type(output)}\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n-    output = collect_dp_compute(worker_group, output)\\\\\\\\n-    return _concat_data_proto_or_future(output)\\\\\\\\n+    outputs = collect_dp_compute(worker_group, outputs)\\\\\\\\n+    return _concat_data_proto_or_future(outputs)\\\\\\\\n \\\\\\\\n \\\\\\\\n-def get_predefined_dispatch_fn(dispatch_mode):\\\\\\\\n+def get_predefined_dispatch_fn(dispatch_mode: Dispatch):\\\\\\\\n     predefined_dispatch_mode_fn = {\\\\\\\\n         Dispatch.ONE_TO_ALL: {\\\\\\\\n             \\\\\\\\\\\\\\\"dispatch_fn\\\\\\\\\\\\\\\": dispatch_one_to_all,\\\\\\\\n@@ -133,7 +133,10 @@ def get_predefined_dispatch_fn(dispatch_mode):\\\\\\\\n             \\\\\\\\\\\\\\\"dispatch_fn\\\\\\\\\\\\\\\": dispatch_all_to_all,\\\\\\\\n             \\\\\\\\\\\\\\\"collect_fn\\\\\\\\\\\\\\\": collect_all_to_all,\\\\\\\\n         },\\\\\\\\n-        Dispatch.DP_COMPUTE: {\\\\\\\\\\\\\\\"dispatch_fn\\\\\\\\\\\\\\\": dispatch_dp_compute, \\\\\\\\\\\\\\\"collect_fn\\\\\\\\\\\\\\\": collect_dp_compute},\\\\\\\\n+        Dispatch.DP_COMPUTE: {\\\\\\\\n+            \\\\\\\\\\\\\\\"dispatch_fn\\\\\\\\\\\\\\\": dispatch_dp_compute,\\\\\\\\n+            \\\\\\\\\\\\\\\"collect_fn\\\\\\\\\\\\\\\": collect_dp_compute,\\\\\\\\n+        },\\\\\\\\n         Dispatch.DP_COMPUTE_PROTO: {\\\\\\\\n             \\\\\\\\\\\\\\\"dispatch_fn\\\\\\\\\\\\\\\": dispatch_dp_compute_data_proto,\\\\\\\\n             \\\\\\\\\\\\\\\"collect_fn\\\\\\\\\\\\\\\": collect_dp_compute_data_proto,\\\\\\\\n@@ -142,12 +145,15 @@ def get_predefined_dispatch_fn(dispatch_mode):\\\\\\\\n             \\\\\\\\\\\\\\\"dispatch_fn\\\\\\\\\\\\\\\": dispatch_dp_compute_data_proto_with_func,\\\\\\\\n             \\\\\\\\\\\\\\\"collect_fn\\\\\\\\\\\\\\\": collect_dp_compute_data_proto,\\\\\\\\n         },\\\\\\\\n-        Dispatch.DP_COMPUTE_METRIC: {\\\\\\\\\\\\\\\"dispatch_fn\\\\\\\\\\\\\\\": dispatch_dp_compute_data_proto, \\\\\\\\\\\\\\\"collect_fn\\\\\\\\\\\\\\\": collect_dp_compute},\\\\\\\\n+        Dispatch.DP_COMPUTE_METRIC: {\\\\\\\\n+            \\\\\\\\\\\\\\\"dispatch_fn\\\\\\\\\\\\\\\": dispatch_dp_compute_data_proto,\\\\\\\\n+            \\\\\\\\\\\\\\\"collect_fn\\\\\\\\\\\\\\\": collect_dp_compute,\\\\\\\\n+        },\\\\\\\\n     }\\\\\\\\n     return predefined_dispatch_mode_fn[dispatch_mode]\\\\\\\\n \\\\\\\\n \\\\\\\\n-def get_predefined_execute_fn(execute_mode):\\\\\\\\n+def get_predefined_execute_fn(execute_mode: Execute):\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     Note that here we only asks execute_all and execute_rank_zero to be implemented\\\\\\\\n     Leave the choice of how these two functions handle argument 'blocking' to users\\\\\\\\n@@ -159,7 +165,7 @@ def get_predefined_execute_fn(execute_mode):\\\\\\\\n     return predefined_execute_mode_fn[execute_mode]\\\\\\\\n \\\\\\\\n \\\\\\\\n-def _check_dispatch_mode(dispatch_mode):\\\\\\\\n+def _check_dispatch_mode(dispatch_mode: Union[Dispatch, Dict[Literal[\\\\\\\\\\\\\\\"dispatch_fn\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"collect_fn\\\\\\\\\\\\\\\"], FunctionType]]):\\\\\\\\n     assert isinstance(dispatch_mode, (Dispatch, dict)), (\\\\\\\\n         f\\\\\\\\\\\\\\\"dispatch_mode must be a Dispatch or a Dict. Got {dispatch_mode}\\\\\\\\\\\\\\\"\\\\\\\\n     )\\\\\\\\n@@ -169,7 +175,7 @@ def _check_dispatch_mode(dispatch_mode):\\\\\\\\n             assert key in dispatch_mode, f\\\\\\\\\\\\\\\"key {key} should be in dispatch_mode if it is a dictionary\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n \\\\\\\\n-def _check_execute_mode(execute_mode):\\\\\\\\n+def _check_execute_mode(execute_mode: Execute):\\\\\\\\n     assert isinstance(execute_mode, Execute), f\\\\\\\\\\\\\\\"execute_mode must be a Execute. Got {execute_mode}\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n \\\\\\\\n@@ -180,9 +186,10 @@ def _materialize_futures(*args, **kwargs):\\\\\\\\n             arg = arg.get()\\\\\\\\n         # add more type to materialize\\\\\\\\n         new_args.append(arg)\\\\\\\\n-    for k, v in kwargs.items():\\\\\\\\n-        if isinstance(v, DataProtoFuture):\\\\\\\\n-            kwargs[k] = v.get()\\\\\\\\n+\\\\\\\\n+    for key, value in kwargs.items():\\\\\\\\n+        if isinstance(value, DataProtoFuture):\\\\\\\\n+            kwargs[key] = value.get()\\\\\\\\n \\\\\\\\n     new_args = tuple(new_args)\\\\\\\\n     return new_args, kwargs\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"9ecffca6b8d7589b292fbb25b5cf74c004cc4fad\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/single_controller/base/worker.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":29,\\\\\\\"deletions\\\\\\\":11,\\\\\\\"changes\\\\\\\":40,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fsingle_controller%2Fbase%2Fworker.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fsingle_controller%2Fbase%2Fworker.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fsingle_controller%2Fbase%2Fworker.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -18,8 +18,10 @@\\\\\\\\n import os\\\\\\\\n import socket\\\\\\\\n from dataclasses import dataclass\\\\\\\\n+from typing import Tuple\\\\\\\\n \\\\\\\\n import ray\\\\\\\\n+import torch\\\\\\\\n \\\\\\\\n from .decorator import Dispatch, Execute, register\\\\\\\\n from .register_center.ray import create_worker_group_register_center\\\\\\\\n@@ -40,7 +42,7 @@ class DistGlobalInfo:\\\\\\\\n \\\\\\\\n \\\\\\\\n class WorkerHelper:\\\\\\\\n-    def _get_node_ip(self):\\\\\\\\n+    def _get_node_ip(self) -> str:\\\\\\\\n         host_ipv4 = os.getenv(\\\\\\\\\\\\\\\"MY_HOST_IP\\\\\\\\\\\\\\\", None)\\\\\\\\n         host_ipv6 = os.getenv(\\\\\\\\\\\\\\\"MY_HOST_IPV6\\\\\\\\\\\\\\\", None)\\\\\\\\n         host_ip_by_env = host_ipv4 or host_ipv6\\\\\\\\n@@ -49,12 +51,12 @@ def _get_node_ip(self):\\\\\\\\n         host_ip = host_ip_by_env or host_ip_by_sdk\\\\\\\\n         return host_ip\\\\\\\\n \\\\\\\\n-    def _get_free_port(self):\\\\\\\\n+    def _get_free_port(self) -> int:\\\\\\\\n         with socket.socket() as sock:\\\\\\\\n             sock.bind((\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\", 0))\\\\\\\\n             return sock.getsockname()[1]\\\\\\\\n \\\\\\\\n-    def get_availale_master_addr_port(self):\\\\\\\\n+    def get_availale_master_addr_port(self) -> Tuple[str, str]:\\\\\\\\n         return self._get_node_ip(), str(self._get_free_port())\\\\\\\\n \\\\\\\\n     def _get_pid(self):\\\\\\\\n@@ -81,16 +83,26 @@ def to_dict(self):\\\\\\\\n \\\\\\\\n # we assume that in each WorkerGroup, there is a Master Worker\\\\\\\\n class Worker(WorkerHelper):\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"A (distributed) worker.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+    _world_size: int\\\\\\\\n+    _rank: int\\\\\\\\n+    _local_world_size: int\\\\\\\\n+    _local_rank: int\\\\\\\\n+    _master_addr: str\\\\\\\\n+    _master_port: str\\\\\\\\n+    _cuda_visible_devices: str\\\\\\\\n+\\\\\\\\n     def __new__(cls, *args, **kwargs):\\\\\\\\n         instance = super().__new__(cls)\\\\\\\\n \\\\\\\\n         # note that here we use int to distinguish\\\\\\\\n-        disable_worker_init = int(os.environ.get(\\\\\\\\\\\\\\\"DISABLE_WORKER_INIT\\\\\\\\\\\\\\\", 0))\\\\\\\\n+        disable_worker_init = int(os.getenv(\\\\\\\\\\\\\\\"DISABLE_WORKER_INIT\\\\\\\\\\\\\\\", 0))\\\\\\\\n         if disable_worker_init:\\\\\\\\n             return instance\\\\\\\\n \\\\\\\\n-        rank = os.environ.get(\\\\\\\\\\\\\\\"RANK\\\\\\\\\\\\\\\", None)\\\\\\\\n-        worker_group_prefix = os.environ.get(\\\\\\\\\\\\\\\"WG_PREFIX\\\\\\\\\\\\\\\", None)\\\\\\\\n+        rank = os.getenv(\\\\\\\\\\\\\\\"RANK\\\\\\\\\\\\\\\", None)\\\\\\\\n+        worker_group_prefix = os.getenv(\\\\\\\\\\\\\\\"WG_PREFIX\\\\\\\\\\\\\\\", None)\\\\\\\\n \\\\\\\\n         # when decorator @ray.remote applies, __new__ will be called while we don't want to apply _configure_before_init\\\\\\\\n         if None not in [rank, worker_group_prefix] and \\\\\\\\\\\\\\\"ActorClass(\\\\\\\\\\\\\\\" not in cls.__name__:\\\\\\\\n@@ -112,13 +124,19 @@ def _configure_before_init(self, register_center_name: str, rank: int):\\\\\\\\n \\\\\\\\n     def __init__(self, cuda_visible_devices=None) -> None:\\\\\\\\n         # construct a meta from envrionment variable. Note that the import must be inside the class because it is executed remotely\\\\\\\\n-        world_size = int(os.environ[\\\\\\\\\\\\\\\"WORLD_SIZE\\\\\\\\\\\\\\\"])\\\\\\\\n-        rank = int(os.environ[\\\\\\\\\\\\\\\"RANK\\\\\\\\\\\\\\\"])\\\\\\\\n+        world_size = int(os.getenv(\\\\\\\\\\\\\\\"WORLD_SIZE\\\\\\\\\\\\\\\"))\\\\\\\\n+        rank = int(os.getenv(\\\\\\\\\\\\\\\"RANK\\\\\\\\\\\\\\\"))\\\\\\\\n         self._rank = rank\\\\\\\\n         self._world_size = world_size\\\\\\\\n \\\\\\\\n-        master_addr = os.environ[\\\\\\\\\\\\\\\"MASTER_ADDR\\\\\\\\\\\\\\\"]\\\\\\\\n-        master_port = os.environ[\\\\\\\\\\\\\\\"MASTER_PORT\\\\\\\\\\\\\\\"]\\\\\\\\n+        if \\\\\\\\\\\\\\\"AMD\\\\\\\\\\\\\\\" in torch.cuda.get_device_name():\\\\\\\\n+            os.environ[\\\\\\\\\\\\\\\"CUDA_VISIBLE_DEVICES\\\\\\\\\\\\\\\"] = os.getenv(\\\\\\\\\\\\\\\"ROCR_VISIBLE_DEVICES\\\\\\\\\\\\\\\")\\\\\\\\n+            os.environ[\\\\\\\\\\\\\\\"LOCAL_RANK\\\\\\\\\\\\\\\"] = os.getenv(\\\\\\\\\\\\\\\"RAY_LOCAL_RANK\\\\\\\\\\\\\\\")\\\\\\\\n+            cuda_visible_devices = os.getenv(\\\\\\\\\\\\\\\"LOCAL_RANK\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"0\\\\\\\\\\\\\\\")\\\\\\\\n+            torch.cuda.set_device(int(cuda_visible_devices))\\\\\\\\n+\\\\\\\\n+        master_addr = os.getenv(\\\\\\\\\\\\\\\"MASTER_ADDR\\\\\\\\\\\\\\\")\\\\\\\\n+        master_port = os.getenv(\\\\\\\\\\\\\\\"MASTER_PORT\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         local_world_size = int(os.getenv(\\\\\\\\\\\\\\\"LOCAL_WORLD_SIZE\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"1\\\\\\\\\\\\\\\"))\\\\\\\\n         local_rank = int(os.getenv(\\\\\\\\\\\\\\\"LOCAL_RANK\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"0\\\\\\\\\\\\\\\"))\\\\\\\\n@@ -158,7 +176,7 @@ def get_master_addr_port(self):\\\\\\\\n         return self._master_addr, self._master_port\\\\\\\\n \\\\\\\\n     def get_cuda_visible_devices(self):\\\\\\\\n-        cuda_visible_devices = os.environ.get(\\\\\\\\\\\\\\\"CUDA_VISIBLE_DEVICES\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"not set\\\\\\\\\\\\\\\")\\\\\\\\n+        cuda_visible_devices = os.getenv(\\\\\\\\\\\\\\\"CUDA_VISIBLE_DEVICES\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"not set\\\\\\\\\\\\\\\")\\\\\\\\n         return cuda_visible_devices\\\\\\\\n \\\\\\\\n     def print_rank0(self, *args, **kwargs):\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"9556b4d490cdbc552bbd7108aa5ae5e7a479a1c5\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/single_controller/base/worker_group.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":5,\\\\\\\"deletions\\\\\\\":5,\\\\\\\"changes\\\\\\\":10,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fsingle_controller%2Fbase%2Fworker_group.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fsingle_controller%2Fbase%2Fworker_group.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fsingle_controller%2Fbase%2Fworker_group.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -19,15 +19,17 @@\\\\\\\\n import signal\\\\\\\\n import threading\\\\\\\\n import time\\\\\\\\n-from typing import Any, Callable, Dict, List\\\\\\\\n+from typing import Any, Callable, Dict, List, Optional\\\\\\\\n \\\\\\\\n from .decorator import MAGIC_ATTR, Dispatch, get_predefined_dispatch_fn, get_predefined_execute_fn\\\\\\\\n \\\\\\\\n \\\\\\\\n class ResourcePool:\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"The resource pool with meta info such as world_size.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"The resource pool with meta info such as world size.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n-    def __init__(self, process_on_nodes=None, max_collocate_count: int = 10, n_gpus_per_node=8) -> None:\\\\\\\\n+    def __init__(\\\\\\\\n+        self, process_on_nodes: Optional[Any] = None, max_collocate_count: int = 10, n_gpus_per_node: int = 8\\\\\\\\n+    ) -> None:\\\\\\\\n         if process_on_nodes is None:\\\\\\\\n             process_on_nodes = []\\\\\\\\n \\\\\\\\n@@ -76,8 +78,6 @@ def __call__(self) -> Any:\\\\\\\\n \\\\\\\\n \\\\\\\\n def check_workers_alive(workers: List, is_alive: Callable, gap_time: float = 1) -> None:\\\\\\\\n-    import time\\\\\\\\n-\\\\\\\\n     while True:\\\\\\\\n         for worker in workers:\\\\\\\\n             if not is_alive(worker):\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"e16fd63f0aa841eb54e6b7a434361dd0a945f901\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/single_controller/ray/base.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":21,\\\\\\\"deletions\\\\\\\":12,\\\\\\\"changes\\\\\\\":33,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fsingle_controller%2Fray%2Fbase.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fsingle_controller%2Fray%2Fbase.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fsingle_controller%2Fray%2Fbase.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -14,12 +14,14 @@\\\\\\\\n \\\\\\\\n import os\\\\\\\\n import random\\\\\\\\n+import re\\\\\\\\n import string\\\\\\\\n import time\\\\\\\\n-from typing import Any, Dict, List, Tuple\\\\\\\\n+from typing import Any, Dict, List, Optional, Tuple\\\\\\\\n from unittest.mock import patch\\\\\\\\n \\\\\\\\n import ray\\\\\\\\n+from ray.actor import ActorHandle\\\\\\\\n from ray.experimental.state.api import get_actor\\\\\\\\n from ray.util import list_named_actors\\\\\\\\n from ray.util.placement_group import PlacementGroup, placement_group\\\\\\\\n@@ -66,6 +68,7 @@ def sort_placement_group_by_node_ip(pgs: List[PlacementGroup]) -> List[Placement\\\\\\\\n         # all bunles should be on the same node\\\\\\\\n         node_id = specs[\\\\\\\\\\\\\\\"bundles_to_node_id\\\\\\\\\\\\\\\"][0]\\\\\\\\n         pg_ip[pg.id] = node_ip[node_id]\\\\\\\\n+\\\\\\\\n     return sorted(pgs, key=lambda pg: pg_ip[pg.id])\\\\\\\\n \\\\\\\\n \\\\\\\\n@@ -76,7 +79,7 @@ def __init__(\\\\\\\\n         use_gpu: bool = True,\\\\\\\\n         name_prefix: str = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\",\\\\\\\\n         max_colocate_count: int = 5,\\\\\\\\n-        detached=False,\\\\\\\\n+        detached: bool = False,\\\\\\\\n     ) -> None:\\\\\\\\n         super().__init__(process_on_nodes, max_colocate_count)\\\\\\\\n         self.use_gpu = use_gpu\\\\\\\\n@@ -85,7 +88,7 @@ def __init__(\\\\\\\\n         self.pgs = None\\\\\\\\n         self.detached = detached\\\\\\\\n \\\\\\\\n-    def get_placement_groups(self, strategy=\\\\\\\\\\\\\\\"STRICT_PACK\\\\\\\\\\\\\\\", name=None):\\\\\\\\n+    def get_placement_groups(self, strategy: str = \\\\\\\\\\\\\\\"STRICT_PACK\\\\\\\\\\\\\\\", name: Optional[str] = None) -> List[PlacementGroup]:\\\\\\\\n         if self.pgs is not None:\\\\\\\\n             return self.pgs\\\\\\\\n \\\\\\\\n@@ -116,7 +119,7 @@ def get_placement_groups(self, strategy=\\\\\\\\\\\\\\\"STRICT_PACK\\\\\\\\\\\\\\\", name=None):\\\\\\\\n \\\\\\\\n def extract_pg_from_exist(\\\\\\\\n     resource_pools: Dict[str, RayResourcePool], src_role_names: List[str], resource_pool: RayResourcePool\\\\\\\\n-) -> List:\\\\\\\\n+) -> List[PlacementGroup]:\\\\\\\\n     src_pgs = [\\\\\\\\n         pg\\\\\\\\n         for role_name, resource_pool in resource_pools.items()\\\\\\\\n@@ -170,7 +173,12 @@ def update_options(self, options: Dict):\\\\\\\\n         self._options.update(options)\\\\\\\\n \\\\\\\\n     def __call__(\\\\\\\\n-        self, placement_group, placement_group_bundle_idx, use_gpu: bool = True, num_gpus=1, sharing_with=None\\\\\\\\n+        self,\\\\\\\\n+        placement_group: PlacementGroup,\\\\\\\\n+        placement_group_bundle_idx: int,\\\\\\\\n+        use_gpu: bool = True,\\\\\\\\n+        num_gpus: int = 1,\\\\\\\\n+        sharing_with: Worker = None,\\\\\\\\n     ) -> Any:\\\\\\\\n         if sharing_with is not None:\\\\\\\\n             target_node_id = ray.get(sharing_with.get_node_id.remote())\\\\\\\\n@@ -207,8 +215,8 @@ def __init__(\\\\\\\\n         ray_cls_with_init: RayClassWithInitArgs = None,\\\\\\\\n         bin_pack: bool = True,\\\\\\\\n         name_prefix: str = None,\\\\\\\\n-        detached=False,\\\\\\\\n-        worker_names=None,\\\\\\\\n+        detached: bool = False,\\\\\\\\n+        worker_names: List[str] = None,\\\\\\\\n         **kwargs,\\\\\\\\n     ) -> None:\\\\\\\\n         super().__init__(resource_pool=resource_pool, **kwargs)\\\\\\\\n@@ -229,21 +237,24 @@ def __init__(\\\\\\\\n         if ray_cls_with_init is not None:\\\\\\\\n             self._bind_worker_method(self.ray_cls_with_init.cls, func_generator)\\\\\\\\n \\\\\\\\n-    def _is_worker_alive(self, worker: ray.actor.ActorHandle):\\\\\\\\n+    def _is_worker_alive(self, worker: ActorHandle) -> bool:\\\\\\\\n         worker_state_dict = get_actor(worker._actor_id.hex())\\\\\\\\n         return worker_state_dict.get(\\\\\\\\\\\\\\\"state\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"undefined\\\\\\\\\\\\\\\") == \\\\\\\\\\\\\\\"ALIVE\\\\\\\\\\\\\\\" if worker_state_dict is not None else False\\\\\\\\n \\\\\\\\n-    def _init_with_detached_workers(self, worker_names):\\\\\\\\n+    def _init_with_detached_workers(self, worker_names: List[str]) -> None:\\\\\\\\n         workers = [ray.get_actor(name=name) for name in worker_names]\\\\\\\\n         self._workers = workers\\\\\\\\n         self._world_size = len(worker_names)\\\\\\\\n \\\\\\\\n-    def _init_with_resource_pool(self, resource_pool, ray_cls_with_init, bin_pack, detached):\\\\\\\\n+    def _init_with_resource_pool(\\\\\\\\n+        self, resource_pool: RayResourcePool, ray_cls_with_init: RayClassWithInitArgs, bin_pack: bool, detached: bool\\\\\\\\n+    ):\\\\\\\\n         use_gpu = resource_pool.use_gpu\\\\\\\\n \\\\\\\\n         strategy = \\\\\\\\\\\\\\\"PACK\\\\\\\\\\\\\\\"\\\\\\\\n         if bin_pack:\\\\\\\\n             strategy = \\\\\\\\\\\\\\\"STRICT_PACK\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n         pgs = resource_pool.get_placement_groups(strategy=strategy)\\\\\\\\n         world_size = resource_pool.world_size\\\\\\\\n         self._world_size = world_size\\\\\\\\n@@ -270,8 +281,6 @@ def _init_with_resource_pool(self, resource_pool, ray_cls_with_init, bin_pack, d\\\\\\\\n                     env_vars[\\\\\\\\\\\\\\\"MASTER_ADDR\\\\\\\\\\\\\\\"] = self._master_addr\\\\\\\\n                     env_vars[\\\\\\\\\\\\\\\"MASTER_PORT\\\\\\\\\\\\\\\"] = self._master_port\\\\\\\\n \\\\\\\\n-                import re\\\\\\\\n-\\\\\\\\n                 cia_name = type(ray_cls_with_init.cls).__name__\\\\\\\\n                 match = re.search(r\\\\\\\\\\\\\\\"ActorClass\\\\\\\\\\\\\\\\(([^)]+)\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\", cia_name)  # ray.remote(Obj) -> \\\\\\\\\\\\\\\"ActorClass(Obj)\\\\\\\\\\\\\\\"\\\\\\\\n                 cia_name = match.group(1) if match else cia_name  # \\\\\\\\\\\\\\\"ActorClass(Obj)\\\\\\\\\\\\\\\" -> \\\\\\\\\\\\\\\"Obj\\\\\\\\\\\\\\\"\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"046bd778284aa966ca40abbe247fe726e879a176\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":5,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":6,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Ftrainer%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Ftrainer%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fconfig.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -53,9 +53,10 @@ class AlgorithmConfig:\\\\\\\\n     gamma: float = 1.0\\\\\\\\n     lam: float = 1.0\\\\\\\\n     adv_estimator: str = \\\\\\\\\\\\\\\"grpo\\\\\\\\\\\\\\\"\\\\\\\\n+    use_kl_loss: bool = False\\\\\\\\n     kl_penalty: str = \\\\\\\\\\\\\\\"kl\\\\\\\\\\\\\\\"\\\\\\\\n-    kl_type: str = \\\\\\\\\\\\\\\"fixed\\\\\\\\\\\\\\\"\\\\\\\\n     kl_coef: float = 1e-3\\\\\\\\n+    kl_type: str = \\\\\\\\\\\\\\\"fixed\\\\\\\\\\\\\\\"\\\\\\\\n     kl_horizon: float = 0.0\\\\\\\\n     kl_target: float = 0.0\\\\\\\\n \\\\\\\\n@@ -95,6 +96,9 @@ class PPOConfig:\\\\\\\\n     def post_init(self):\\\\\\\\n         self.worker.rollout.prompt_length = self.data.max_prompt_length\\\\\\\\n         self.worker.rollout.response_length = self.data.max_response_length\\\\\\\\n+        self.worker.actor.use_kl_loss = self.algorithm.use_kl_loss\\\\\\\\n+        self.worker.actor.kl_penalty = self.algorithm.kl_penalty\\\\\\\\n+        self.worker.actor.kl_coef = self.algorithm.kl_coef\\\\\\\\n \\\\\\\\n     def deep_post_init(self):\\\\\\\\n         recursive_post_init(self)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"30ba82cc987835eb14680e99a04e670e6ad07ec2\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/core_algos.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":19,\\\\\\\"deletions\\\\\\\":19,\\\\\\\"changes\\\\\\\":38,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Ftrainer%2Fcore_algos.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Ftrainer%2Fcore_algos.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fcore_algos.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -281,21 +281,21 @@ def compute_remax_outcome_advantage(\\\\\\\\n \\\\\\\\n def compute_rewards(\\\\\\\\n     token_level_scores: torch.Tensor,\\\\\\\\n-    old_log_prob: torch.Tensor,\\\\\\\\n-    ref_log_prob: torch.Tensor,\\\\\\\\n+    log_probs: torch.Tensor,\\\\\\\\n+    ref_log_probs: torch.Tensor,\\\\\\\\n     kl_ratio: float,\\\\\\\\n ) -> torch.Tensor:\\\\\\\\n-    kl = old_log_prob - ref_log_prob\\\\\\\\n+    kl = log_probs - ref_log_probs\\\\\\\\n     return token_level_scores - kl * kl_ratio\\\\\\\\n \\\\\\\\n \\\\\\\\n def compute_policy_loss(\\\\\\\\n-    old_log_prob: torch.Tensor,\\\\\\\\n-    log_prob: torch.Tensor,\\\\\\\\n+    old_log_probs: torch.Tensor,\\\\\\\\n+    log_probs: torch.Tensor,\\\\\\\\n     advantages: torch.Tensor,\\\\\\\\n     eos_mask: torch.Tensor,\\\\\\\\n     cliprange: float,\\\\\\\\n-) -> Tuple[torch.Tensor, float, float]:\\\\\\\\n+) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Compute the policy loss.\\\\\\\\n \\\\\\\\n     Adapted from https://github.com/huggingface/trl/blob/v0.15.0/trl/trainer/ppo_trainer.py#L568\\\\\\\\n@@ -318,7 +318,7 @@ def compute_policy_loss(\\\\\\\\n         pg_clipfrac: (float)\\\\\\\\n             a float number indicating the fraction of policy gradient loss being clipped\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    negative_approx_kl = log_prob - old_log_prob\\\\\\\\n+    negative_approx_kl = log_probs - old_log_probs\\\\\\\\n     # clamp the ratio before exp to avoid nan\\\\\\\\n     # see: https://github.com/pytorch/pytorch/issues/10729\\\\\\\\n     ratio = torch.exp(negative_approx_kl)\\\\\\\\n@@ -370,36 +370,36 @@ def compute_value_loss(\\\\\\\\n     return vf_loss, vf_clipfrac\\\\\\\\n \\\\\\\\n \\\\\\\\n-def kl_penalty(logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor, kl_penalty: str) -> torch.Tensor:\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Compute KL divergence given logprob and ref_logprob.\\\\\\\\n+def kl_penalty(log_probs: torch.FloatTensor, ref_log_probs: torch.FloatTensor, kl_penalty: str) -> torch.Tensor:\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Compute KL divergence given log_probs and ref_log_probs.\\\\\\\\n     Copied from https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_trainer.py#L1104\\\\\\\\n \\\\\\\\n     Args:\\\\\\\\n-        logprob: torch.Tensor\\\\\\\\n-        ref_logprob: torch.Tensor\\\\\\\\n+        log_probs: torch.Tensor\\\\\\\\n+        ref_log_probs: torch.Tensor\\\\\\\\n \\\\\\\\n     Returns:\\\\\\\\n         kl_div: torch.Tensor\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    log_probs, ref_log_probs = log_probs.float(), ref_log_probs.float()\\\\\\\\n     if kl_penalty == \\\\\\\\\\\\\\\"kl\\\\\\\\\\\\\\\":\\\\\\\\n-        return logprob - ref_logprob\\\\\\\\n+        return log_probs - ref_log_probs\\\\\\\\n \\\\\\\\n     if kl_penalty == \\\\\\\\\\\\\\\"abs\\\\\\\\\\\\\\\":\\\\\\\\n-        return (logprob - ref_logprob).abs()\\\\\\\\n+        return (log_probs - ref_log_probs).abs()\\\\\\\\n \\\\\\\\n     if kl_penalty == \\\\\\\\\\\\\\\"mse\\\\\\\\\\\\\\\":\\\\\\\\n-        return 0.5 * (logprob - ref_logprob).square()\\\\\\\\n+        return 0.5 * (log_probs - ref_log_probs).square()\\\\\\\\n \\\\\\\\n     # J. Schulman. Approximating kl divergence, 2020.\\\\\\\\n-    # # URL http://joschu.net/blog/kl-approx.html.\\\\\\\\n+    # URL http://joschu.net/blog/kl-approx.html\\\\\\\\n     if kl_penalty == \\\\\\\\\\\\\\\"low_var_kl\\\\\\\\\\\\\\\":\\\\\\\\n-        kl = ref_logprob - logprob\\\\\\\\n-        ratio = torch.exp(kl)\\\\\\\\n-        kld = (ratio - kl - 1).contiguous()\\\\\\\\n+        kl = ref_log_probs - log_probs\\\\\\\\n+        kld = (kl.exp() - kl - 1).contiguous()\\\\\\\\n         return torch.clamp(kld, min=-10, max=10)\\\\\\\\n \\\\\\\\n     if kl_penalty == \\\\\\\\\\\\\\\"full\\\\\\\\\\\\\\\":\\\\\\\\n-        # so, here logprob and ref_logprob should contain the logits for every token in vocabulary\\\\\\\\n+        # here log_probs and ref_log_probs should contain the logits for every token in vocabulary\\\\\\\\n         raise NotImplementedError\\\\\\\\n \\\\\\\\n     raise NotImplementedError\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"394929f6417496ca83853f82c4b74fbb3d4da835\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/main.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":67,\\\\\\\"deletions\\\\\\\":60,\\\\\\\"changes\\\\\\\":127,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Ftrainer%2Fmain.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Ftrainer%2Fmain.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fmain.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -28,78 +28,85 @@\\\\\\\\n from .ray_trainer import RayPPOTrainer, ResourcePoolManager, Role\\\\\\\\n \\\\\\\\n \\\\\\\\n+# please make sure main_task is not scheduled on head\\\\\\\\n @ray.remote(num_cpus=1)\\\\\\\\n-def main_task(config: PPOConfig):\\\\\\\\n-    # please make sure main_task is not scheduled on head\\\\\\\\n-    # print config\\\\\\\\n-    config.deep_post_init()\\\\\\\\n-    print(json.dumps(config.to_dict(), indent=2))\\\\\\\\n-\\\\\\\\n-    # instantiate tokenizer\\\\\\\\n-    tokenizer = get_tokenizer(\\\\\\\\n-        config.worker.actor.model.model_path,\\\\\\\\n-        trust_remote_code=config.worker.actor.model.trust_remote_code,\\\\\\\\n-        use_fast=True,\\\\\\\\n-    )\\\\\\\\n-    processor = get_processor(\\\\\\\\n-        config.worker.actor.model.model_path,\\\\\\\\n-        trust_remote_code=config.worker.actor.model.trust_remote_code,\\\\\\\\n-        use_fast=True,\\\\\\\\n-    )\\\\\\\\n-\\\\\\\\n-    # define worker classes\\\\\\\\n-    ray_worker_group_cls = RayWorkerGroup\\\\\\\\n-    role_worker_mapping = {\\\\\\\\n-        Role.ActorRollout: ray.remote(FSDPWorker),\\\\\\\\n-        Role.Critic: ray.remote(FSDPWorker),\\\\\\\\n-        Role.RefPolicy: ray.remote(FSDPWorker),\\\\\\\\n-    }\\\\\\\\n-    global_pool_id = \\\\\\\\\\\\\\\"global_pool\\\\\\\\\\\\\\\"\\\\\\\\n-    resource_pool_spec = {\\\\\\\\n-        global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes,\\\\\\\\n-    }\\\\\\\\n-    mapping = {\\\\\\\\n-        Role.ActorRollout: global_pool_id,\\\\\\\\n-        Role.Critic: global_pool_id,\\\\\\\\n-        Role.RefPolicy: global_pool_id,\\\\\\\\n-    }\\\\\\\\n-    resource_pool_manager = ResourcePoolManager(resource_pool_spec=resource_pool_spec, mapping=mapping)\\\\\\\\n-\\\\\\\\n-    reward_fn = CustomRewardManager(\\\\\\\\n-        tokenizer=tokenizer, num_examine=1, compute_score=config.worker.reward.compute_score\\\\\\\\n-    )\\\\\\\\n-    val_reward_fn = CustomRewardManager(\\\\\\\\n-        tokenizer=tokenizer, num_examine=1, compute_score=config.worker.reward.compute_score\\\\\\\\n-    )\\\\\\\\n-\\\\\\\\n-    trainer = RayPPOTrainer(\\\\\\\\n-        config=config,\\\\\\\\n-        tokenizer=tokenizer,\\\\\\\\n-        processor=processor,\\\\\\\\n-        role_worker_mapping=role_worker_mapping,\\\\\\\\n-        resource_pool_manager=resource_pool_manager,\\\\\\\\n-        ray_worker_group_cls=ray_worker_group_cls,\\\\\\\\n-        reward_fn=reward_fn,\\\\\\\\n-        val_reward_fn=val_reward_fn,\\\\\\\\n-    )\\\\\\\\n-    trainer.init_workers()\\\\\\\\n-    trainer.fit()\\\\\\\\n+class Runner:\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"A runner for RL training.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+    def run(self, config: PPOConfig):\\\\\\\\n+        # print config\\\\\\\\n+        config.deep_post_init()\\\\\\\\n+        print(json.dumps(config.to_dict(), indent=2))\\\\\\\\n+\\\\\\\\n+        # instantiate tokenizer\\\\\\\\n+        tokenizer = get_tokenizer(\\\\\\\\n+            config.worker.actor.model.model_path,\\\\\\\\n+            trust_remote_code=config.worker.actor.model.trust_remote_code,\\\\\\\\n+            use_fast=True,\\\\\\\\n+        )\\\\\\\\n+        processor = get_processor(\\\\\\\\n+            config.worker.actor.model.model_path,\\\\\\\\n+            trust_remote_code=config.worker.actor.model.trust_remote_code,\\\\\\\\n+            use_fast=True,\\\\\\\\n+        )\\\\\\\\n+\\\\\\\\n+        # define worker classes\\\\\\\\n+        ray_worker_group_cls = RayWorkerGroup\\\\\\\\n+        role_worker_mapping = {\\\\\\\\n+            Role.ActorRollout: ray.remote(FSDPWorker),\\\\\\\\n+            Role.Critic: ray.remote(FSDPWorker),\\\\\\\\n+            Role.RefPolicy: ray.remote(FSDPWorker),\\\\\\\\n+        }\\\\\\\\n+        global_pool_id = \\\\\\\\\\\\\\\"global_pool\\\\\\\\\\\\\\\"\\\\\\\\n+        resource_pool_spec = {\\\\\\\\n+            global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes,\\\\\\\\n+        }\\\\\\\\n+        mapping = {\\\\\\\\n+            Role.ActorRollout: global_pool_id,\\\\\\\\n+            Role.Critic: global_pool_id,\\\\\\\\n+            Role.RefPolicy: global_pool_id,\\\\\\\\n+        }\\\\\\\\n+        resource_pool_manager = ResourcePoolManager(resource_pool_spec=resource_pool_spec, mapping=mapping)\\\\\\\\n+\\\\\\\\n+        reward_fn = CustomRewardManager(\\\\\\\\n+            tokenizer=tokenizer, num_examine=1, compute_score=config.worker.reward.compute_score\\\\\\\\n+        )\\\\\\\\n+        val_reward_fn = CustomRewardManager(\\\\\\\\n+            tokenizer=tokenizer, num_examine=1, compute_score=config.worker.reward.compute_score\\\\\\\\n+        )\\\\\\\\n+\\\\\\\\n+        trainer = RayPPOTrainer(\\\\\\\\n+            config=config,\\\\\\\\n+            tokenizer=tokenizer,\\\\\\\\n+            processor=processor,\\\\\\\\n+            role_worker_mapping=role_worker_mapping,\\\\\\\\n+            resource_pool_manager=resource_pool_manager,\\\\\\\\n+            ray_worker_group_cls=ray_worker_group_cls,\\\\\\\\n+            reward_fn=reward_fn,\\\\\\\\n+            val_reward_fn=val_reward_fn,\\\\\\\\n+        )\\\\\\\\n+        trainer.init_workers()\\\\\\\\n+        trainer.fit()\\\\\\\\n \\\\\\\\n \\\\\\\\n def main():\\\\\\\\n     cli_args = OmegaConf.from_cli()\\\\\\\\n-    file_config = OmegaConf.load(getattr(cli_args, \\\\\\\\\\\\\\\"config\\\\\\\\\\\\\\\"))\\\\\\\\n-    cli_args.pop(\\\\\\\\\\\\\\\"config\\\\\\\\\\\\\\\", None)\\\\\\\\n-\\\\\\\\n     default_config = OmegaConf.structured(PPOConfig())\\\\\\\\n-    ppo_config = OmegaConf.merge(default_config, file_config, cli_args)\\\\\\\\n+\\\\\\\\n+    if hasattr(cli_args, \\\\\\\\\\\\\\\"config\\\\\\\\\\\\\\\"):\\\\\\\\n+        config_path = cli_args.pop(\\\\\\\\\\\\\\\"config\\\\\\\\\\\\\\\", None)\\\\\\\\n+        file_config = OmegaConf.load(config_path)\\\\\\\\n+        default_config = OmegaConf.merge(default_config, file_config)\\\\\\\\n+\\\\\\\\n+    ppo_config = OmegaConf.merge(default_config, cli_args)\\\\\\\\n     ppo_config = OmegaConf.to_object(ppo_config)\\\\\\\\n \\\\\\\\n     if not ray.is_initialized():\\\\\\\\n         # this is for local ray cluster\\\\\\\\n         ray.init(runtime_env={\\\\\\\\\\\\\\\"env_vars\\\\\\\\\\\\\\\": {\\\\\\\\\\\\\\\"TOKENIZERS_PARALLELISM\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"true\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"NCCL_DEBUG\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"WARN\\\\\\\\\\\\\\\"}})\\\\\\\\n \\\\\\\\n-    ray.get(main_task.remote(ppo_config))\\\\\\\\n+    runner = Runner.remote()\\\\\\\\n+    ray.get(runner.run.remote(ppo_config))\\\\\\\\n \\\\\\\\n \\\\\\\\n if __name__ == \\\\\\\\\\\\\\\"__main__\\\\\\\\\\\\\\\":\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"fd91c6a7f5b5390411d06ad5ed0e9e2fe82c649c\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/metrics.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Ftrainer%2Fmetrics.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Ftrainer%2Fmetrics.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fmetrics.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -115,8 +115,8 @@ def compute_timing_metrics(batch: DataProto, timing_raw: Dict[str, float]) -> Di\\\\\\\\n     num_response_tokens = torch.sum(response_info[\\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\"]).item()\\\\\\\\n     num_overall_tokens = num_prompt_tokens + num_response_tokens\\\\\\\\n     num_tokens_of_section = {\\\\\\\\n-        \\\\\\\\\\\\\\\"gen\\\\\\\\\\\\\\\": num_response_tokens,\\\\\\\\n-        **{name: num_overall_tokens for name in [\\\\\\\\\\\\\\\"ref\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"values\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"adv\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"update_critic\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"update_actor\\\\\\\\\\\\\\\"]},\\\\\\\\n+        **dict.fromkeys([\\\\\\\\\\\\\\\"gen\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"reward\\\\\\\\\\\\\\\"], num_response_tokens),\\\\\\\\n+        **dict.fromkeys([\\\\\\\\\\\\\\\"ref\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"old\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"values\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"adv\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"update_critic\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"update_actor\\\\\\\\\\\\\\\"], num_overall_tokens),\\\\\\\\n     }\\\\\\\\n     return {\\\\\\\\n         **{f\\\\\\\\\\\\\\\"timing_s/{name}\\\\\\\\\\\\\\\": value for name, value in timing_raw.items()},\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"492b85dfc5a1c41b6bd915084ed87902eb5ad106\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/ray_trainer.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":39,\\\\\\\"deletions\\\\\\\":27,\\\\\\\"changes\\\\\\\":66,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fray_trainer.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -16,8 +16,10 @@\\\\\\\\n This trainer supports model-agonistic model initialization with huggingface\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n+import json\\\\\\\\n import os\\\\\\\\n import uuid\\\\\\\\n+from collections import defaultdict\\\\\\\\n from contextlib import contextmanager\\\\\\\\n from copy import deepcopy\\\\\\\\n from dataclasses import dataclass, field\\\\\\\\n@@ -28,6 +30,7 @@\\\\\\\\n import ray\\\\\\\\n import torch\\\\\\\\n from codetiming import Timer\\\\\\\\n+from ray.experimental.tqdm_ray import tqdm\\\\\\\\n from torch.utils.data import RandomSampler, SequentialSampler\\\\\\\\n from torchdata.stateful_dataloader import StatefulDataLoader\\\\\\\\n from transformers import PreTrainedTokenizer, ProcessorMixin\\\\\\\\n@@ -130,9 +133,9 @@ def apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.KLController, kl_penal\\\\\\\\n     response_mask = attention_mask[:, -response_length:]\\\\\\\\n \\\\\\\\n     # compute kl between ref_policy and current policy\\\\\\\\n-    if \\\\\\\\\\\\\\\"ref_log_prob\\\\\\\\\\\\\\\" in data.batch.keys():\\\\\\\\n+    if \\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\" in data.batch.keys():\\\\\\\\n         kld = core_algos.kl_penalty(\\\\\\\\n-            data.batch[\\\\\\\\\\\\\\\"old_log_probs\\\\\\\\\\\\\\\"], data.batch[\\\\\\\\\\\\\\\"ref_log_prob\\\\\\\\\\\\\\\"], kl_penalty=kl_penalty\\\\\\\\n+            data.batch[\\\\\\\\\\\\\\\"old_log_probs\\\\\\\\\\\\\\\"], data.batch[\\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\"], kl_penalty=kl_penalty\\\\\\\\n         )  # (batch_size, response_length)\\\\\\\\n         kld = kld * response_mask\\\\\\\\n         beta = kl_ctrl.value\\\\\\\\n@@ -264,7 +267,7 @@ def __init__(\\\\\\\\n \\\\\\\\n         self.role_worker_mapping = role_worker_mapping\\\\\\\\n         self.resource_pool_manager = resource_pool_manager\\\\\\\\n-        self.use_reference_policy = Role.RefPolicy in role_worker_mapping\\\\\\\\n+        self.use_reference_policy = Role.RefPolicy in role_worker_mapping and config.algorithm.kl_coef > 1e-6\\\\\\\\n         self.use_reward_model = Role.RewardModel in role_worker_mapping\\\\\\\\n         self.ray_worker_group_cls = ray_worker_group_cls\\\\\\\\n         self.val_generations_logger = ValGenerationsLogger()\\\\\\\\n@@ -273,7 +276,7 @@ def __init__(\\\\\\\\n         if self.use_reference_policy:\\\\\\\\n             self.kl_ctrl = core_algos.get_kl_controller(config.algorithm)\\\\\\\\n         else:\\\\\\\\n-            self.kl_ctrl = core_algos.FixedKLController(kl_coef=0.0)\\\\\\\\n+            self.kl_ctrl = core_algos.FixedKLController(init_kl_coef=0.0)\\\\\\\\n \\\\\\\\n         if config.algorithm.adv_estimator not in list(AdvantageEstimator):\\\\\\\\n             raise NotImplementedError(f\\\\\\\\\\\\\\\"Unknown advantage estimator: {config.algorithm.adv_estimator}.\\\\\\\\\\\\\\\")\\\\\\\\n@@ -380,6 +383,7 @@ def _validate(self) -> Dict[str, Any]:\\\\\\\\n         reward_tensor_lst = []\\\\\\\\n         # Lists to collect samples for the table\\\\\\\\n         sample_inputs, sample_outputs, sample_scores = [], [], []\\\\\\\\n+        reward_metrics_lst = defaultdict(list)\\\\\\\\n         for test_data in self.val_dataloader:\\\\\\\\n             test_batch = DataProto.from_single_dict(test_data)\\\\\\\\n             # Store original inputs\\\\\\\\n@@ -412,17 +416,20 @@ def _validate(self) -> Dict[str, Any]:\\\\\\\\n             test_batch = test_batch.union(test_output_gen_batch)\\\\\\\\n \\\\\\\\n             # evaluate using reward_function\\\\\\\\n-            reward_tensor = self.val_reward_fn(test_batch)\\\\\\\\n+            reward_tensor, reward_metrics = self.val_reward_fn(test_batch)\\\\\\\\n \\\\\\\\n             # Store scores\\\\\\\\n             scores = reward_tensor.sum(-1).cpu().tolist()\\\\\\\\n             sample_scores.extend(scores)\\\\\\\\n \\\\\\\\n             reward_tensor_lst.append(reward_tensor)\\\\\\\\n+            for key, value in reward_metrics.items():\\\\\\\\n+                reward_metrics_lst[key].extend(value)\\\\\\\\n \\\\\\\\n         self._maybe_log_val_generations(inputs=sample_inputs, outputs=sample_outputs, scores=sample_scores)\\\\\\\\n         reward_score = torch.cat(reward_tensor_lst, dim=0).sum(-1).mean().item()\\\\\\\\n-        return {\\\\\\\\\\\\\\\"val/test_score\\\\\\\\\\\\\\\": reward_score}\\\\\\\\n+        val_reward_metrics = {f\\\\\\\\\\\\\\\"val/{key}_reward\\\\\\\\\\\\\\\": value for key, value in reduce_metrics(reward_metrics_lst).items()}\\\\\\\\n+        return {\\\\\\\\\\\\\\\"val/reward_score\\\\\\\\\\\\\\\": reward_score, **val_reward_metrics}\\\\\\\\n \\\\\\\\n     def init_workers(self) -> None:\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Init resource pool and worker group\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n@@ -586,13 +593,13 @@ def fit(self):\\\\\\\\n         # currently, we only support validation using the reward_function.\\\\\\\\n         if self.val_reward_fn is not None and self.config.trainer.val_before_train:\\\\\\\\n             val_metrics = self._validate()\\\\\\\\n-            print(f\\\\\\\\\\\\\\\"Initial validation metrics: {val_metrics}.\\\\\\\\\\\\\\\")\\\\\\\\n+            print(f\\\\\\\\\\\\\\\"Initial validation metrics: {json.dumps(val_metrics, indent=2)}.\\\\\\\\\\\\\\\")\\\\\\\\n             logger.log(data=val_metrics, step=self.global_step)\\\\\\\\n             if self.config.trainer.val_only:\\\\\\\\n                 return\\\\\\\\n \\\\\\\\n-        for _ in range(self.config.trainer.total_episodes):\\\\\\\\n-            for batch_dict in self.train_dataloader:\\\\\\\\n+        for _ in tqdm(range(self.config.trainer.total_episodes), desc=\\\\\\\\\\\\\\\"Episode\\\\\\\\\\\\\\\", position=0):\\\\\\\\n+            for batch_dict in tqdm(self.train_dataloader, desc=\\\\\\\\\\\\\\\"Running step\\\\\\\\\\\\\\\", position=1):\\\\\\\\n                 self.global_step += 1\\\\\\\\n                 if self.global_step > self.training_steps:\\\\\\\\n                     break\\\\\\\\n@@ -624,7 +631,7 @@ def fit(self):\\\\\\\\n                             gen_baseline_output = self.actor_rollout_wg.generate_sequences(gen_baseline_batch)\\\\\\\\n \\\\\\\\n                             batch = batch.union(gen_baseline_output)\\\\\\\\n-                            reward_baseline_tensor = self.reward_fn(batch)\\\\\\\\n+                            reward_baseline_tensor, _ = self.reward_fn(batch)\\\\\\\\n                             reward_baseline_tensor = reward_baseline_tensor.sum(dim=-1)\\\\\\\\n \\\\\\\\n                             batch.pop(batch_keys=list(gen_baseline_output.batch.keys()))\\\\\\\\n@@ -638,6 +645,19 @@ def fit(self):\\\\\\\\n                     batch = batch.repeat(repeat_times=self.config.worker.rollout.n, interleave=True)\\\\\\\\n                     batch = batch.union(gen_batch_output)\\\\\\\\n \\\\\\\\n+                    # compute reward\\\\\\\\n+                    with _timer(\\\\\\\\\\\\\\\"reward\\\\\\\\\\\\\\\", timing_raw):\\\\\\\\n+                        if self.use_reward_model:\\\\\\\\n+                            raise NotImplementedError(\\\\\\\\\\\\\\\"RM is not supported for PPO yet.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+                        # we combine with rule-based rm\\\\\\\\n+                        reward_tensor, reward_metrics = self.reward_fn(batch)\\\\\\\\n+                        batch.batch[\\\\\\\\\\\\\\\"token_level_scores\\\\\\\\\\\\\\\"] = reward_tensor\\\\\\\\n+                        reward_metrics = {\\\\\\\\n+                            f\\\\\\\\\\\\\\\"reward/{key}\\\\\\\\\\\\\\\": value for key, value in reduce_metrics(reward_metrics).items()\\\\\\\\n+                        }\\\\\\\\n+                        metrics.update(reward_metrics)\\\\\\\\n+\\\\\\\\n                     # balance the number of valid tokens on each dp rank.\\\\\\\\n                     # Note that this breaks the order of data inside the batch.\\\\\\\\n                     # Please take care when you implement group based adv computation such as GRPO and rloo\\\\\\\\n@@ -647,15 +667,15 @@ def fit(self):\\\\\\\\n                     batch.meta_info[\\\\\\\\\\\\\\\"global_token_num\\\\\\\\\\\\\\\"] = torch.sum(batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"], dim=-1).tolist()\\\\\\\\n \\\\\\\\n                     # recompute old_log_probs\\\\\\\\n-                    with _timer(\\\\\\\\\\\\\\\"old_log_prob\\\\\\\\\\\\\\\", timing_raw):\\\\\\\\n-                        old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)\\\\\\\\n-                        batch = batch.union(old_log_prob)\\\\\\\\n+                    with _timer(\\\\\\\\\\\\\\\"old\\\\\\\\\\\\\\\", timing_raw):\\\\\\\\n+                        old_log_probs = self.actor_rollout_wg.compute_log_probs(batch)\\\\\\\\n+                        batch = batch.union(old_log_probs)\\\\\\\\n \\\\\\\\n                     if self.use_reference_policy:\\\\\\\\n                         # compute reference log_prob\\\\\\\\n                         with _timer(\\\\\\\\\\\\\\\"ref\\\\\\\\\\\\\\\", timing_raw):\\\\\\\\n-                            ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(batch)\\\\\\\\n-                            batch = batch.union(ref_log_prob)\\\\\\\\n+                            ref_log_probs = self.ref_policy_wg.compute_ref_log_probs(batch)\\\\\\\\n+                            batch = batch.union(ref_log_probs)\\\\\\\\n \\\\\\\\n                     # compute values\\\\\\\\n                     if self.use_critic:\\\\\\\\n@@ -664,16 +684,8 @@ def fit(self):\\\\\\\\n                             batch = batch.union(values)\\\\\\\\n \\\\\\\\n                     with _timer(\\\\\\\\\\\\\\\"adv\\\\\\\\\\\\\\\", timing_raw):\\\\\\\\n-                        # reward_fn should combine the results from reward model and rule-based results\\\\\\\\n-                        if self.use_reward_model:\\\\\\\\n-                            raise NotImplementedError(\\\\\\\\\\\\\\\"RM is not supported for PPO yet.\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-                        # we combine with rule-based rm\\\\\\\\n-                        reward_tensor = self.reward_fn(batch)\\\\\\\\n-                        batch.batch[\\\\\\\\\\\\\\\"token_level_scores\\\\\\\\\\\\\\\"] = reward_tensor\\\\\\\\n-\\\\\\\\n-                        # compute rewards. apply_kl_penalty if available\\\\\\\\n-                        if not self.config.worker.actor.use_kl_loss:  # not grpo's kl loss\\\\\\\\n+                        # apply kl penalty if available\\\\\\\\n+                        if not self.config.algorithm.use_kl_loss:  # apply kl penalty to reward\\\\\\\\n                             batch, kl_metrics = apply_kl_penalty(\\\\\\\\n                                 batch, kl_ctrl=self.kl_ctrl, kl_penalty=self.config.algorithm.kl_penalty\\\\\\\\n                             )\\\\\\\\n@@ -711,7 +723,7 @@ def fit(self):\\\\\\\\n                         and self.config.trainer.val_freq > 0\\\\\\\\n                         and self.global_step % self.config.trainer.val_freq == 0\\\\\\\\n                     ):\\\\\\\\n-                        with _timer(\\\\\\\\\\\\\\\"testing\\\\\\\\\\\\\\\", timing_raw):\\\\\\\\n+                        with _timer(\\\\\\\\\\\\\\\"validation\\\\\\\\\\\\\\\", timing_raw):\\\\\\\\n                             val_metrics = self._validate()\\\\\\\\n \\\\\\\\n                         metrics.update(val_metrics)\\\\\\\\n@@ -735,7 +747,7 @@ def fit(self):\\\\\\\\n                 val_metrics = self._validate()\\\\\\\\n                 logger.log(data=val_metrics, step=self.global_step)\\\\\\\\n \\\\\\\\n-            print(f\\\\\\\\\\\\\\\"Final validation metrics: {val_metrics}.\\\\\\\\\\\\\\\")\\\\\\\\n+            print(f\\\\\\\\\\\\\\\"Final validation metrics: {json.dumps(val_metrics, indent=2)}.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         if self.config.trainer.save_freq > 0 and self.global_step % self.config.trainer.save_freq != 0:\\\\\\\\n             self._save_checkpoint()\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"6c9cd3d06378eff435e4209141cd6674ec13476c\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/checkpoint/checkpoint_manager.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":10,\\\\\\\"deletions\\\\\\\":9,\\\\\\\"changes\\\\\\\":19,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Futils%2Fcheckpoint%2Fcheckpoint_manager.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Futils%2Fcheckpoint%2Fcheckpoint_manager.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fcheckpoint%2Fcheckpoint_manager.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -17,7 +17,7 @@\\\\\\\\n import shutil\\\\\\\\n import tempfile\\\\\\\\n from abc import ABC, abstractmethod\\\\\\\\n-from typing import Union\\\\\\\\n+from typing import Any, Dict, Union\\\\\\\\n \\\\\\\\n import numpy as np\\\\\\\\n import torch\\\\\\\\n@@ -27,6 +27,9 @@\\\\\\\\n from transformers import PreTrainedTokenizer, ProcessorMixin\\\\\\\\n \\\\\\\\n \\\\\\\\n+CHECKPOINT_TRACKER = \\\\\\\\\\\\\\\"latest_checkpointed_iteration.txt\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+\\\\\\\\n class BaseCheckpointManager(ABC):\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     A checkpoint manager that saves and loads\\\\\\\\n@@ -82,7 +85,7 @@ def remove_previous_save_local_path(self):\\\\\\\\n         shutil.rmtree(abs_path, ignore_errors=True)\\\\\\\\n \\\\\\\\n     @staticmethod\\\\\\\\n-    def local_mkdir(path):\\\\\\\\n+    def local_mkdir(path) -> str:\\\\\\\\n         if not os.path.isabs(path):\\\\\\\\n             working_dir = os.getcwd()\\\\\\\\n             path = os.path.join(working_dir, path)\\\\\\\\n@@ -92,18 +95,16 @@ def local_mkdir(path):\\\\\\\\n         lock_path = os.path.join(tempfile.gettempdir(), lock_filename)\\\\\\\\n \\\\\\\\n         try:\\\\\\\\n-            with FileLock(lock_path, timeout=60):  # Add timeout\\\\\\\\n-                # make a new dir\\\\\\\\n+            with FileLock(lock_path, timeout=60):\\\\\\\\n                 os.makedirs(path, exist_ok=True)\\\\\\\\n         except Exception as e:\\\\\\\\n             print(f\\\\\\\\\\\\\\\"Warning: Failed to acquire lock for {path}: {e}\\\\\\\\\\\\\\\")\\\\\\\\n-            # Even if the lock is not acquired, try to create the directory\\\\\\\\n-            os.makedirs(path, exist_ok=True)\\\\\\\\n+            os.makedirs(path, exist_ok=True)  # even if the lock is not acquired, try to create the directory\\\\\\\\n \\\\\\\\n         return path\\\\\\\\n \\\\\\\\n     @staticmethod\\\\\\\\n-    def get_rng_state():\\\\\\\\n+    def get_rng_state() -> Dict[str, Any]:\\\\\\\\n         rng_state = {\\\\\\\\n             \\\\\\\\\\\\\\\"cpu\\\\\\\\\\\\\\\": torch.get_rng_state(),\\\\\\\\n             \\\\\\\\\\\\\\\"cuda\\\\\\\\\\\\\\\": torch.cuda.get_rng_state(),\\\\\\\\n@@ -113,7 +114,7 @@ def get_rng_state():\\\\\\\\n         return rng_state\\\\\\\\n \\\\\\\\n     @staticmethod\\\\\\\\n-    def load_rng_state(rng_state):\\\\\\\\n+    def load_rng_state(rng_state: Dict[str, Any]) -> None:\\\\\\\\n         torch.set_rng_state(rng_state[\\\\\\\\\\\\\\\"cpu\\\\\\\\\\\\\\\"])\\\\\\\\n         torch.cuda.set_rng_state(rng_state[\\\\\\\\\\\\\\\"cuda\\\\\\\\\\\\\\\"])\\\\\\\\n         np.random.set_state(rng_state[\\\\\\\\\\\\\\\"numpy\\\\\\\\\\\\\\\"])\\\\\\\\n@@ -144,4 +145,4 @@ def get_checkpoint_tracker_filename(root_path: str):\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     Tracker file rescords the latest chckpoint during training to restart from.\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    return os.path.join(root_path, \\\\\\\\\\\\\\\"latest_checkpointed_iteration.txt\\\\\\\\\\\\\\\")\\\\\\\\n+    return os.path.join(root_path, CHECKPOINT_TRACKER)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"d1271b2c1f81022be315fa2c3525a4d1c8cfed23\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/reward_score/math.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":9,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":11,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Futils%2Freward_score%2Fmath.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Futils%2Freward_score%2Fmath.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Freward_score%2Fmath.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -13,6 +13,7 @@\\\\\\\\n # limitations under the License.\\\\\\\\n \\\\\\\\n import re\\\\\\\\n+from typing import Dict\\\\\\\\n \\\\\\\\n from mathruler.grader import extract_boxed_content, grade_answer\\\\\\\\n \\\\\\\\n@@ -28,5 +29,11 @@ def math_acc_reward(predict_str: str, ground_truth: str) -> float:\\\\\\\\n     return 1.0 if grade_answer(answer, ground_truth) else 0.0\\\\\\\\n \\\\\\\\n \\\\\\\\n-def math_compute_score(predict_str: str, ground_truth: str) -> float:\\\\\\\\n-    return 0.9 * math_acc_reward(predict_str, ground_truth) + 0.1 * math_format_reward(predict_str)\\\\\\\\n+def math_compute_score(predict_str: str, ground_truth: str) -> Dict[str, float]:\\\\\\\\n+    format = math_format_reward(predict_str)\\\\\\\\n+    accuracy = math_acc_reward(predict_str, ground_truth)\\\\\\\\n+    return {\\\\\\\\n+        \\\\\\\\\\\\\\\"overall\\\\\\\\\\\\\\\": 0.9 * accuracy + 0.1 * format,\\\\\\\\n+        \\\\\\\\\\\\\\\"format\\\\\\\\\\\\\\\": format,\\\\\\\\n+        \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\": accuracy,\\\\\\\\n+    }\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"c116e3fee0be6fa9c5a3f96bed7c32218a3e3f92\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/reward_score/r1v.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":10,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":12,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Futils%2Freward_score%2Fr1v.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Futils%2Freward_score%2Fr1v.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Freward_score%2Fr1v.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -13,6 +13,7 @@\\\\\\\\n # limitations under the License.\\\\\\\\n \\\\\\\\n import re\\\\\\\\n+from typing import Dict\\\\\\\\n \\\\\\\\n from mathruler.grader import grade_answer\\\\\\\\n \\\\\\\\n@@ -30,11 +31,18 @@ def r1v_accuracy_reward(predict_str: str, ground_truth: str) -> float:\\\\\\\\n         given_answer = content_match.group(1).strip() if content_match else predict_str.strip()\\\\\\\\n         if grade_answer(given_answer, ground_truth):\\\\\\\\n             return 1.0\\\\\\\\n+\\\\\\\\n     except Exception:\\\\\\\\n         pass\\\\\\\\n \\\\\\\\n     return 0.0\\\\\\\\n \\\\\\\\n \\\\\\\\n-def r1v_compute_score(predict_str: str, ground_truth: str) -> float:\\\\\\\\n-    return 0.5 * r1v_accuracy_reward(predict_str, ground_truth) + 0.5 * r1v_format_reward(predict_str)\\\\\\\\n+def r1v_compute_score(predict_str: str, ground_truth: str) -> Dict[str, float]:\\\\\\\\n+    format = r1v_format_reward(predict_str)\\\\\\\\n+    accuracy = r1v_accuracy_reward(predict_str, ground_truth)\\\\\\\\n+    return {\\\\\\\\n+        \\\\\\\\\\\\\\\"overall\\\\\\\\\\\\\\\": 0.5 * accuracy + 0.5 * format,\\\\\\\\n+        \\\\\\\\\\\\\\\"format\\\\\\\\\\\\\\\": format,\\\\\\\\n+        \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\": accuracy,\\\\\\\\n+    }\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"8ae460adc30f95a6382276169e82fa7fff0f8019\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/torch_functional.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":4,\\\\\\\"deletions\\\\\\\":4,\\\\\\\"changes\\\\\\\":8,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Futils%2Ftorch_functional.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Futils%2Ftorch_functional.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Ftorch_functional.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -35,7 +35,7 @@\\\\\\\\n \\\\\\\\n \\\\\\\\n @torch.compiler.disable()\\\\\\\\n-def logprobs_from_logits_flash_attn(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\\\\\\\\n+def log_probs_from_logits_flash_attn(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\\\\\\\\n     output = cross_entropy_loss(logits, labels, inplace_backward=True)\\\\\\\\n     if not isinstance(output, tuple):\\\\\\\\n         raise ValueError(\\\\\\\\n@@ -45,7 +45,7 @@ def logprobs_from_logits_flash_attn(logits: torch.Tensor, labels: torch.Tensor)\\\\\\\\n     return -output[0]\\\\\\\\n \\\\\\\\n \\\\\\\\n-def logprobs_from_logits(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\\\\\\\\n+def log_probs_from_logits(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     We may use torch compile to speed up computing.\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n@@ -54,7 +54,7 @@ def logprobs_from_logits(logits: torch.Tensor, labels: torch.Tensor) -> torch.Te\\\\\\\\n     logits = logits.contiguous().view(-1, vocab_dim)\\\\\\\\n     labels = labels.contiguous().view(-1)\\\\\\\\n     if FLAH_ATTN_CROSS_ENTROPY_LOSS_AVAILABLE:\\\\\\\\n-        output = logprobs_from_logits_flash_attn(logits, labels)\\\\\\\\n+        output = log_probs_from_logits_flash_attn(logits, labels)\\\\\\\\n     else:  # fall back to torch kernel, upcast logits to fp32\\\\\\\\n         output = F.cross_entropy(logits.float(), labels, reduction=\\\\\\\\\\\\\\\"none\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n@@ -120,7 +120,7 @@ def pad_2d_list_to_length(\\\\\\\\n     response: List[List[int]], pad_token_id: int, max_length: Optional[int] = None\\\\\\\\n ) -> torch.Tensor:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    pad a 2D list (e.g. responses, logprobs) to a 2D tensor.\\\\\\\\n+    pad a 2D list (e.g. responses, log_probs) to a 2D tensor.\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     response_length = max(len(sub_list) for sub_list in response)\\\\\\\\n     if max_length is not None and max_length > response_length:\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"28ef4234eb0d3e839aa2f8c33c5122a176ff607c\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/actor/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":5,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":8,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fworkers%2Factor%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fworkers%2Factor%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Factor%2Fconfig.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -72,18 +72,19 @@ class ActorConfig:\\\\\\\\n     micro_batch_size_per_device_for_experience: int = 16\\\\\\\\n     max_grad_norm: float = 1.0\\\\\\\\n     clip_ratio: float = 0.2\\\\\\\\n-    use_kl_loss: bool = True\\\\\\\\n-    kl_loss_coef: float = 1e-3\\\\\\\\n-    kl_loss_type: str = \\\\\\\\\\\\\\\"low_var_kl\\\\\\\\\\\\\\\"\\\\\\\\n     ppo_epochs: int = 1\\\\\\\\n     padding_free: bool = False\\\\\\\\n     ulysses_sequence_parallel_size: int = 1\\\\\\\\n+    use_torch_compile: bool = True\\\\\\\\n     model: ModelConfig = field(default_factory=ModelConfig)\\\\\\\\n     optim: OptimConfig = field(default_factory=OptimConfig)\\\\\\\\n     fsdp: FSDPConfig = field(default_factory=FSDPConfig)\\\\\\\\n     offload: OffloadConfig = field(default_factory=OffloadConfig)\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"auto keys\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     global_batch_size_per_device: int = field(default=-1, init=False)\\\\\\\\n+    use_kl_loss: bool = field(default=False, init=False)\\\\\\\\n+    kl_penalty: str = field(default=\\\\\\\\\\\\\\\"kl\\\\\\\\\\\\\\\", init=False)\\\\\\\\n+    kl_coef: float = field(default=0.0, init=False)\\\\\\\\n \\\\\\\\n \\\\\\\\n @dataclass\\\\\\\\n@@ -95,3 +96,4 @@ class RefConfig:\\\\\\\\n     micro_batch_size_per_device_for_experience: int = field(default=-1, init=False)\\\\\\\\n     padding_free: bool = field(default=False, init=False)\\\\\\\\n     ulysses_sequence_parallel_size: int = field(default=1, init=False)\\\\\\\\n+    use_torch_compile: bool = field(default=True, init=False)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"6a53786bf8bea04ac8130146b4c7766e7ea2693c\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/actor/dp_actor.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":32,\\\\\\\"deletions\\\\\\\":23,\\\\\\\"changes\\\\\\\":55,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fworkers%2Factor%2Fdp_actor.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fworkers%2Factor%2Fdp_actor.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Factor%2Fdp_actor.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -20,9 +20,9 @@\\\\\\\\n from typing import Any, Dict, Optional\\\\\\\\n \\\\\\\\n import torch\\\\\\\\n+from ray.experimental.tqdm_ray import tqdm\\\\\\\\n from torch import nn\\\\\\\\n from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\\\\\\\\n-from tqdm import tqdm\\\\\\\\n \\\\\\\\n from ...protocol import DataProto\\\\\\\\n from ...trainer import core_algos\\\\\\\\n@@ -56,7 +56,10 @@ def __init__(\\\\\\\\n         self.rank = int(os.getenv(\\\\\\\\\\\\\\\"RANK\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"0\\\\\\\\\\\\\\\"))\\\\\\\\n         self.actor_module = actor_module\\\\\\\\n         self.actor_optimizer = actor_optimizer\\\\\\\\n-        self.logprobs_from_logits = torch.compile(VF.logprobs_from_logits, dynamic=True)\\\\\\\\n+        if config.use_torch_compile:\\\\\\\\n+            self.log_probs_from_logits = torch.compile(VF.log_probs_from_logits, dynamic=True)\\\\\\\\n+        else:\\\\\\\\n+            self.log_probs_from_logits = VF.log_probs_from_logits\\\\\\\\n \\\\\\\\n     def _forward_micro_batch(self, micro_batch: Dict[str, torch.Tensor], temperature: float) -> torch.Tensor:\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n@@ -122,7 +125,7 @@ def _forward_micro_batch(self, micro_batch: Dict[str, torch.Tensor], temperature\\\\\\\\n             logits_rmpad = output.logits.squeeze(0)  # (total_nnz, vocab_size)\\\\\\\\n             logits_rmpad.div_(temperature)\\\\\\\\n             # ((total_nnz / sp) + pad)\\\\\\\\n-            log_probs = self.logprobs_from_logits(logits=logits_rmpad, labels=input_ids_rmpad_rolled)\\\\\\\\n+            log_probs = self.log_probs_from_logits(logits=logits_rmpad, labels=input_ids_rmpad_rolled)\\\\\\\\n \\\\\\\\n             # gather log_prob if sp > 1\\\\\\\\n             if self.config.ulysses_sequence_parallel_size > 1:\\\\\\\\n@@ -145,7 +148,7 @@ def _forward_micro_batch(self, micro_batch: Dict[str, torch.Tensor], temperature\\\\\\\\n             logits: torch.Tensor = output.logits\\\\\\\\n             logits.div_(temperature)\\\\\\\\n             logits = logits[:, -response_length - 1 : -1, :]  # (bsz, response_length, vocab_size)\\\\\\\\n-            log_probs = self.logprobs_from_logits(logits, responses)  # (bsz, response_length)\\\\\\\\n+            log_probs = self.log_probs_from_logits(logits, responses)  # (bsz, response_length)\\\\\\\\n \\\\\\\\n         return log_probs\\\\\\\\n \\\\\\\\n@@ -156,6 +159,7 @@ def _optimizer_step(self) -> torch.Tensor:\\\\\\\\n             grad_norm = nn.utils.clip_grad_norm_(self.actor_module.parameters(), max_norm=self.config.max_grad_norm)\\\\\\\\n \\\\\\\\n         self.actor_optimizer.step()\\\\\\\\n+        self.actor_optimizer.zero_grad()\\\\\\\\n         return grad_norm\\\\\\\\n \\\\\\\\n     @torch.no_grad()\\\\\\\\n@@ -190,7 +194,10 @@ def compute_log_prob(self, data: DataProto) -> torch.Tensor:\\\\\\\\n             self.config.micro_batch_size_per_device_for_experience\\\\\\\\n         )\\\\\\\\n         log_probs_lst = []\\\\\\\\n-        for micro_batch in tqdm(micro_batches, desc=\\\\\\\\\\\\\\\"Compute log probs\\\\\\\\\\\\\\\", disable=(self.rank != 0)):\\\\\\\\n+        if self.rank == 0:\\\\\\\\n+            micro_batches = tqdm(micro_batches, desc=\\\\\\\\\\\\\\\"Compute log probs\\\\\\\\\\\\\\\", position=2)\\\\\\\\n+\\\\\\\\n+        for micro_batch in micro_batches:\\\\\\\\n             model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}\\\\\\\\n             log_probs = self._forward_micro_batch(model_inputs, temperature=temperature)\\\\\\\\n             log_probs_lst.append(log_probs)\\\\\\\\n@@ -203,8 +210,8 @@ def update_policy(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n \\\\\\\\n         temperature = data.meta_info[\\\\\\\\\\\\\\\"temperature\\\\\\\\\\\\\\\"]  # temperature must be in the data.meta_info to avoid slient error\\\\\\\\n         select_keys = [\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"old_log_probs\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"advantages\\\\\\\\\\\\\\\"]\\\\\\\\n-        if self.config.use_kl_loss:\\\\\\\\n-            select_keys.append(\\\\\\\\\\\\\\\"ref_log_prob\\\\\\\\\\\\\\\")\\\\\\\\n+        if self.config.use_kl_loss and self.config.kl_coef > 1e-6:\\\\\\\\n+            select_keys.append(\\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         if \\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\" in data.non_tensor_batch.keys():\\\\\\\\n             non_tensor_select_keys = [\\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\"]\\\\\\\\n@@ -216,48 +223,51 @@ def update_policy(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n         mini_batches = data.select(select_keys, non_tensor_select_keys).split(self.config.global_batch_size_per_device)\\\\\\\\n \\\\\\\\n         metrics = defaultdict(list)\\\\\\\\n-        n = len(mini_batches)\\\\\\\\n         for _ in range(self.config.ppo_epochs):\\\\\\\\n-            for i, mini_batch in enumerate(mini_batches):\\\\\\\\n+            if self.rank == 0:\\\\\\\\n+                mini_batches = tqdm(mini_batches, desc=\\\\\\\\\\\\\\\"Train mini-batches\\\\\\\\\\\\\\\", position=2)\\\\\\\\n+\\\\\\\\n+            for mini_batch in mini_batches:\\\\\\\\n                 gradient_accumulation = (\\\\\\\\n                     self.config.global_batch_size_per_device // self.config.micro_batch_size_per_device_for_update\\\\\\\\n                 )\\\\\\\\n                 micro_batches = mini_batch.split(self.config.micro_batch_size_per_device_for_update)\\\\\\\\n+                if self.rank == 0:\\\\\\\\n+                    micro_batches = tqdm(micro_batches, desc=\\\\\\\\\\\\\\\"Update policy\\\\\\\\\\\\\\\", position=3)\\\\\\\\n \\\\\\\\n-                self.actor_optimizer.zero_grad()\\\\\\\\n-                for micro_batch in tqdm(micro_batches, desc=f\\\\\\\\\\\\\\\"Update policy [{i + 1}/{n}]\\\\\\\\\\\\\\\", disable=(self.rank != 0)):\\\\\\\\n+                for micro_batch in micro_batches:\\\\\\\\n                     model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}\\\\\\\\n                     responses = model_inputs[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n                     response_length = responses.size(1)\\\\\\\\n                     attention_mask = model_inputs[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n                     response_mask = attention_mask[:, -response_length:]\\\\\\\\n-                    old_log_prob = model_inputs[\\\\\\\\\\\\\\\"old_log_probs\\\\\\\\\\\\\\\"]\\\\\\\\n+                    old_log_probs = model_inputs[\\\\\\\\\\\\\\\"old_log_probs\\\\\\\\\\\\\\\"]\\\\\\\\n                     advantages = model_inputs[\\\\\\\\\\\\\\\"advantages\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n                     clip_ratio = self.config.clip_ratio\\\\\\\\n \\\\\\\\n                     # all return: (bsz, response_length)\\\\\\\\n-                    log_prob = self._forward_micro_batch(model_inputs, temperature=temperature)\\\\\\\\n+                    log_probs = self._forward_micro_batch(model_inputs, temperature=temperature)\\\\\\\\n \\\\\\\\n                     pg_loss, pg_clipfrac, ppo_kl = core_algos.compute_policy_loss(\\\\\\\\n-                        old_log_prob=old_log_prob,\\\\\\\\n-                        log_prob=log_prob,\\\\\\\\n+                        old_log_probs=old_log_probs,\\\\\\\\n+                        log_probs=log_probs,\\\\\\\\n                         advantages=advantages,\\\\\\\\n                         eos_mask=response_mask,\\\\\\\\n                         cliprange=clip_ratio,\\\\\\\\n                     )\\\\\\\\n-                    if self.config.use_kl_loss:\\\\\\\\n-                        ref_log_prob = model_inputs[\\\\\\\\\\\\\\\"ref_log_prob\\\\\\\\\\\\\\\"]\\\\\\\\n+                    if self.config.use_kl_loss and self.config.kl_coef > 1e-6:\\\\\\\\n+                        ref_log_probs = model_inputs[\\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\"]\\\\\\\\n                         # compute kl loss\\\\\\\\n                         kld = core_algos.kl_penalty(\\\\\\\\n-                            logprob=log_prob,\\\\\\\\n-                            ref_logprob=ref_log_prob,\\\\\\\\n-                            kl_penalty=self.config.kl_loss_type,\\\\\\\\n+                            log_probs=log_probs,\\\\\\\\n+                            ref_log_probs=ref_log_probs,\\\\\\\\n+                            kl_penalty=self.config.kl_penalty,\\\\\\\\n                         )\\\\\\\\n                         kl_loss = VF.masked_mean(kld, response_mask)\\\\\\\\n-                        pg_loss = pg_loss + kl_loss * self.config.kl_loss_coef\\\\\\\\n+                        pg_loss = pg_loss + kl_loss * self.config.kl_coef\\\\\\\\n                         metrics[\\\\\\\\\\\\\\\"actor/kl_loss\\\\\\\\\\\\\\\"] = kl_loss.detach().item()\\\\\\\\n-                        metrics[\\\\\\\\\\\\\\\"actor/kl_coef\\\\\\\\\\\\\\\"] = self.config.kl_loss_coef\\\\\\\\n+                        metrics[\\\\\\\\\\\\\\\"actor/kl_coef\\\\\\\\\\\\\\\"] = self.config.kl_coef\\\\\\\\n \\\\\\\\n                     loss = pg_loss / gradient_accumulation\\\\\\\\n                     loss.backward()\\\\\\\\n@@ -272,5 +282,4 @@ def update_policy(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n                 grad_norm = self._optimizer_step()\\\\\\\\n                 append_to_dict(metrics, {\\\\\\\\\\\\\\\"actor/grad_norm\\\\\\\\\\\\\\\": grad_norm.detach().item()})\\\\\\\\n \\\\\\\\n-        self.actor_optimizer.zero_grad()\\\\\\\\n         return metrics\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"ba21b0e79d65788689608b103be53adf45a2c246\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":3,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fworkers%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fworkers%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Fconfig.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -46,6 +46,7 @@ class WorkerConfig:\\\\\\\\n     rollout: RolloutConfig = field(default_factory=RolloutConfig)\\\\\\\\n \\\\\\\\n     def post_init(self):\\\\\\\\n+        self.ref.micro_batch_size_per_device_for_experience = self.actor.micro_batch_size_per_device_for_experience\\\\\\\\n         self.ref.padding_free = self.actor.padding_free\\\\\\\\n         self.ref.ulysses_sequence_parallel_size = self.actor.ulysses_sequence_parallel_size\\\\\\\\n-        self.ref.micro_batch_size_per_device_for_experience = self.actor.micro_batch_size_per_device_for_experience\\\\\\\\n+        self.ref.use_torch_compile = self.actor.use_torch_compile\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"98261159a46b368fb545e0e2a2c96875ccbc06e4\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/critic/dp_critic.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fworkers%2Fcritic%2Fdp_critic.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fworkers%2Fcritic%2Fdp_critic.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Fcritic%2Fdp_critic.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -192,8 +192,8 @@ def update_critic(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n                     vpreds = self._forward_micro_batch(data)\\\\\\\\n                     vf_loss, vf_clipfrac = core_algos.compute_value_loss(\\\\\\\\n                         vpreds=vpreds,\\\\\\\\n-                        values=values,\\\\\\\\n                         returns=returns,\\\\\\\\n+                        values=values,\\\\\\\\n                         eos_mask=eos_mask,\\\\\\\\n                         cliprange_value=self.config.cliprange_value,\\\\\\\\n                     )\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"ca6e343f8deb678ab099a2f39ad0386efc047e69\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/fsdp_workers.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":8,\\\\\\\"deletions\\\\\\\":8,\\\\\\\"changes\\\\\\\":16,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Ffsdp_workers.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -443,7 +443,7 @@ def load_checkpoint(self, path: str, remove_ckpt_after_load: bool = False):\\\\\\\\n     @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\\\\\\\\n     def update_actor(self, data: DataProto):\\\\\\\\n         assert self._is_actor\\\\\\\\n-        data = data.to(\\\\\\\\\\\\\\\"cuda\\\\\\\\\\\\\\\")\\\\\\\\n+        data = data.to(torch.cuda.current_device())\\\\\\\\n \\\\\\\\n         if self._use_param_offload:\\\\\\\\n             load_fsdp_model(self.fsdp_module)\\\\\\\\n@@ -518,9 +518,9 @@ def generate_sequences(self, prompts: DataProto):\\\\\\\\n         return output\\\\\\\\n \\\\\\\\n     @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\\\\\\\\n-    def compute_log_prob(self, data: DataProto):\\\\\\\\n+    def compute_log_probs(self, data: DataProto):\\\\\\\\n         assert self._is_actor\\\\\\\\n-        data = data.to(\\\\\\\\\\\\\\\"cuda\\\\\\\\\\\\\\\")\\\\\\\\n+        data = data.to(torch.cuda.current_device())\\\\\\\\n         if self._use_param_offload:\\\\\\\\n             load_fsdp_model(self.fsdp_module)\\\\\\\\n \\\\\\\\n@@ -547,17 +547,17 @@ def compute_log_prob(self, data: DataProto):\\\\\\\\n         return output\\\\\\\\n \\\\\\\\n     @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\\\\\\\\n-    def compute_ref_log_prob(self, data: DataProto):\\\\\\\\n+    def compute_ref_log_probs(self, data: DataProto):\\\\\\\\n         assert self._is_ref\\\\\\\\n-        data = data.to(\\\\\\\\\\\\\\\"cuda\\\\\\\\\\\\\\\")\\\\\\\\n+        data = data.to(torch.cuda.current_device())\\\\\\\\n         if self._use_param_offload:\\\\\\\\n             load_fsdp_model(self.fsdp_module)\\\\\\\\n \\\\\\\\n         data.meta_info[\\\\\\\\\\\\\\\"temperature\\\\\\\\\\\\\\\"] = self.config.rollout.temperature\\\\\\\\n         with self.ulysses_sharding_manager:\\\\\\\\n             data = self.ulysses_sharding_manager.preprocess_data(data)\\\\\\\\n             output = self.ref_policy.compute_log_prob(data=data)\\\\\\\\n-            output = DataProto.from_dict(tensors={\\\\\\\\\\\\\\\"ref_log_prob\\\\\\\\\\\\\\\": output})\\\\\\\\n+            output = DataProto.from_dict(tensors={\\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\": output})\\\\\\\\n             output = self.ulysses_sharding_manager.postprocess_data(output)\\\\\\\\n \\\\\\\\n         # https://pytorch.org/docs/stable/notes/fsdp.html#fsdp-notes\\\\\\\\n@@ -576,7 +576,7 @@ def compute_ref_log_prob(self, data: DataProto):\\\\\\\\n     @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\\\\\\\\n     def compute_values(self, data: DataProto):\\\\\\\\n         assert self._is_critic\\\\\\\\n-        data = data.to(\\\\\\\\\\\\\\\"cuda\\\\\\\\\\\\\\\")\\\\\\\\n+        data = data.to(torch.cuda.current_device())\\\\\\\\n         if self._use_param_offload:\\\\\\\\n             load_fsdp_model(self.fsdp_module)\\\\\\\\n \\\\\\\\n@@ -594,7 +594,7 @@ def compute_values(self, data: DataProto):\\\\\\\\n \\\\\\\\n     @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\\\\\\\\n     def update_critic(self, data: DataProto):\\\\\\\\n-        data = data.to(\\\\\\\\\\\\\\\"cuda\\\\\\\\\\\\\\\")\\\\\\\\n+        data = data.to(torch.cuda.current_device())\\\\\\\\n         if self._use_param_offload:\\\\\\\\n             load_fsdp_model(self.fsdp_module)\\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"6b52f7b715f400f90e9d9dc69867ae32f021e9e9\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/reward/custom.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":17,\\\\\\\"deletions\\\\\\\":5,\\\\\\\"changes\\\\\\\":22,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fworkers%2Freward%2Fcustom.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fworkers%2Freward%2Fcustom.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Freward%2Fcustom.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -13,26 +13,36 @@\\\\\\\\n # limitations under the License.\\\\\\\\n \\\\\\\\n \\\\\\\\n+from collections import defaultdict\\\\\\\\n+from typing import Any, Callable, Dict, Tuple, TypedDict\\\\\\\\n+\\\\\\\\n import torch\\\\\\\\n from transformers import PreTrainedTokenizer\\\\\\\\n \\\\\\\\n from ...protocol import DataProto\\\\\\\\n from ...utils.reward_score import math_compute_score, r1v_compute_score\\\\\\\\n \\\\\\\\n \\\\\\\\n+class RewardScore(TypedDict):\\\\\\\\n+    overall: float\\\\\\\\n+    format: float\\\\\\\\n+    accuracy: float\\\\\\\\n+\\\\\\\\n+\\\\\\\\n class CustomRewardManager:\\\\\\\\n     def __init__(self, tokenizer: PreTrainedTokenizer, num_examine: int, compute_score: str):\\\\\\\\n         self.tokenizer = tokenizer\\\\\\\\n         self.num_examine = num_examine\\\\\\\\n         if compute_score == \\\\\\\\\\\\\\\"math\\\\\\\\\\\\\\\":\\\\\\\\n-            self.compute_score = math_compute_score\\\\\\\\n+            self.compute_score: Callable[[str, str], RewardScore] = math_compute_score\\\\\\\\n         elif compute_score == \\\\\\\\\\\\\\\"r1v\\\\\\\\\\\\\\\":\\\\\\\\n-            self.compute_score = r1v_compute_score\\\\\\\\n+            self.compute_score: Callable[[str, str], RewardScore] = r1v_compute_score\\\\\\\\n         else:\\\\\\\\n             raise NotImplementedError()\\\\\\\\n \\\\\\\\n-    def __call__(self, data: DataProto) -> torch.Tensor:\\\\\\\\n+    def __call__(self, data: DataProto) -> Tuple[torch.Tensor, Dict[str, Any]]:\\\\\\\\n         reward_tensor = torch.zeros_like(data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"], dtype=torch.float32)\\\\\\\\n+        reward_metrics = defaultdict(list)\\\\\\\\n         already_print = 0\\\\\\\\n \\\\\\\\n         for i in range(len(data)):\\\\\\\\n@@ -55,7 +65,9 @@ def __call__(self, data: DataProto) -> torch.Tensor:\\\\\\\\n             ground_truth = data_item.non_tensor_batch[\\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n             score = self.compute_score(response_str, ground_truth)\\\\\\\\n-            reward_tensor[i, valid_response_length - 1] = score\\\\\\\\n+            reward_tensor[i, valid_response_length - 1] = score[\\\\\\\\\\\\\\\"overall\\\\\\\\\\\\\\\"]\\\\\\\\n+            for key, value in score.items():\\\\\\\\n+                reward_metrics[key].append(value)\\\\\\\\n \\\\\\\\n             if already_print < self.num_examine:\\\\\\\\n                 already_print += 1\\\\\\\\n@@ -64,4 +76,4 @@ def __call__(self, data: DataProto) -> torch.Tensor:\\\\\\\\n                 print(\\\\\\\\\\\\\\\"[ground_truth]\\\\\\\\\\\\\\\", ground_truth)\\\\\\\\n                 print(\\\\\\\\\\\\\\\"[score]\\\\\\\\\\\\\\\", score)\\\\\\\\n \\\\\\\\n-        return reward_tensor\\\\\\\\n+        return reward_tensor, reward_metrics\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"e2e437ecd31a925158b61e5a84443a55fe06881f\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":3,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/6c0bba7ebc1dcec2101ca6a620db503c108c6f9e/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py?ref=6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -18,6 +18,7 @@\\\\\\\\n - Utilize state_dict from the FSDP to synchronize the weights among tp ranks in vLLM\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n+import os\\\\\\\\n from contextlib import contextmanager\\\\\\\\n from typing import Any, List, Union\\\\\\\\n \\\\\\\\n@@ -52,6 +53,7 @@ def __init__(self, model_path: str, config: RolloutConfig, tokenizer: PreTrained\\\\\\\\n             tokenizer: the task/model tokenizer\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n         super().__init__()\\\\\\\\n+        self.rank = int(os.getenv(\\\\\\\\\\\\\\\"RANK\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"0\\\\\\\\\\\\\\\"))\\\\\\\\n         self.config = config\\\\\\\\n         self.pad_token_id = tokenizer.pad_token_id\\\\\\\\n         if config.tensor_parallel_size > torch.distributed.get_world_size():\\\\\\\\n@@ -139,7 +141,7 @@ def generate_sequences(self, prompts: DataProto) -> DataProto:\\\\\\\\n         # users can customize different sampling_params at different run\\\\\\\\n         with self.update_sampling_params(**prompts.meta_info):\\\\\\\\n             completions: List[RequestOutput] = self.inference_engine.generate(\\\\\\\\n-                prompts=vllm_inputs, sampling_params=self.sampling_params\\\\\\\\n+                prompts=vllm_inputs, sampling_params=self.sampling_params, use_tqdm=(self.rank == 0)\\\\\\\\n             )\\\\\\\\n             response_ids = [output.token_ids for completion in completions for output in completion.outputs]\\\\\\\\n             response_ids = VF.pad_2d_list_to_length(\\\\\\\"}],\\\\\\\"sha\\\\\\\":\\\\\\\"6c0bba7ebc1dcec2101ca6a620db503c108c6f9e\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"committer\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"noreply@github.com\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-24T12:43:43Z\\\\\\\"},\\\\\\\"author\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"hiyouga@buaa.edu.cn\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-24T12:43:43Z\\\\\\\"}}},\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Great! Now I found it! Looking at this commit `6c0bba7ebc1dcec2101ca6a620db503c108c6f9e`, I can see the exact parameter changes I was looking for in the `examples/config.yaml` file. From the patch, I can see:\n\n```\n    micro_batch_size_per_device_for_update: 4\n    micro_batch_size_per_device_for_experience: 16\n```\n\nThese parameters were changed from their previous values! But I still need to find the commit where they changed `4` to `1` and the exact before/after values for the `micro_batch_size_per_device_for_experience` parameter. Let me check an even earlier commit to see what the original values were.\n\nLet me check a few commits back to see what the original values were:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"ref\\\": \\\"1e675810f9c6de29a78ec09980d237a84a33747c\\\"}\", \"include_output_fields\": [\"sha\", \"commit.author\", \"commit.committer\", \"files\"]}",
    "call_id": "toolu_01P9aTTFGWFahuWkyva8KWwL",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01P9aTTFGWFahuWkyva8KWwL",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"files\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"0699957ad18ca7faf37456e179621a3dec3e817f\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/config.yaml\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":3,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":3,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/1e675810f9c6de29a78ec09980d237a84a33747c/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/1e675810f9c6de29a78ec09980d237a84a33747c/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fconfig.yaml?ref=1e675810f9c6de29a78ec09980d237a84a33747c\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -52,6 +52,9 @@ worker:\\\\\\\\n     enable_chunked_prefill: false\\\\\\\\n     tensor_parallel_size: 2\\\\\\\\n     limit_images: 0\\\\\\\\n+    val_override_config:\\\\\\\\n+      temperature: 0.5\\\\\\\\n+      n: 1\\\\\\\\n \\\\\\\\n   ref:\\\\\\\\n     fsdp:\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"23d41c980536528640f6b748605c9437590e0487\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/ray_trainer.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":3,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":6,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/1e675810f9c6de29a78ec09980d237a84a33747c/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/1e675810f9c6de29a78ec09980d237a84a33747c/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fray_trainer.py?ref=1e675810f9c6de29a78ec09980d237a84a33747c\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -149,7 +149,7 @@ def apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.KLController, kl_penal\\\\\\\\n     kl_ctrl.update(current_kl=current_kl, n_steps=batch_size)\\\\\\\\n     data.batch[\\\\\\\\\\\\\\\"token_level_rewards\\\\\\\\\\\\\\\"] = token_level_rewards\\\\\\\\n \\\\\\\\n-    metrics = {\\\\\\\\\\\\\\\"critic/kl\\\\\\\\\\\\\\\": current_kl, \\\\\\\\\\\\\\\"critic/kl_coeff\\\\\\\\\\\\\\\": beta}\\\\\\\\n+    metrics = {\\\\\\\\\\\\\\\"critic/kl\\\\\\\\\\\\\\\": current_kl, \\\\\\\\\\\\\\\"critic/kl_coef\\\\\\\\\\\\\\\": beta}\\\\\\\\n     return data, metrics\\\\\\\\n \\\\\\\\n \\\\\\\\n@@ -398,7 +398,7 @@ def _validate(self) -> Dict[str, Any]:\\\\\\\\n                     non_tensor_batch_keys=[\\\\\\\\\\\\\\\"raw_prompt_ids\\\\\\\\\\\\\\\"],\\\\\\\\n                 )\\\\\\\\n \\\\\\\\n-            test_gen_batch.meta_info = {\\\\\\\\\\\\\\\"do_sample\\\\\\\\\\\\\\\": False}\\\\\\\\n+            test_gen_batch.meta_info = self.config.worker.rollout.val_override_config\\\\\\\\n             test_gen_batch, pad_size = pad_dataproto_to_divisor(test_gen_batch, self.actor_rollout_wg.world_size)\\\\\\\\n             test_output_gen_batch = self.actor_rollout_wg.generate_sequences(test_gen_batch)\\\\\\\\n             test_output_gen_batch = unpad_dataproto(test_output_gen_batch, pad_size=pad_size)\\\\\\\\n@@ -620,7 +620,7 @@ def fit(self):\\\\\\\\n                     if self.config.algorithm.adv_estimator == \\\\\\\\\\\\\\\"remax\\\\\\\\\\\\\\\":\\\\\\\\n                         with _timer(\\\\\\\\\\\\\\\"gen_max\\\\\\\\\\\\\\\", timing_raw):\\\\\\\\n                             gen_baseline_batch = deepcopy(gen_batch)\\\\\\\\n-                            gen_baseline_batch.meta_info[\\\\\\\\\\\\\\\"do_sample\\\\\\\\\\\\\\\"] = False\\\\\\\\n+                            gen_baseline_batch.meta_info[\\\\\\\\\\\\\\\"temperature\\\\\\\\\\\\\\\"] = 0.0\\\\\\\\n                             gen_baseline_output = self.actor_rollout_wg.generate_sequences(gen_baseline_batch)\\\\\\\\n \\\\\\\\n                             batch = batch.union(gen_baseline_output)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"3e6372d4efb7503ab8d55e6f2be68c3e33334cf7\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/critic/dp_critic.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/1e675810f9c6de29a78ec09980d237a84a33747c/verl%2Fworkers%2Fcritic%2Fdp_critic.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/1e675810f9c6de29a78ec09980d237a84a33747c/verl%2Fworkers%2Fcritic%2Fdp_critic.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Fcritic%2Fdp_critic.py?ref=1e675810f9c6de29a78ec09980d237a84a33747c\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -103,9 +103,7 @@ def _forward_micro_batch(self, micro_batch: Dict[str, torch.Tensor]) -> torch.Te\\\\\\\\n \\\\\\\\n             # gather output if sp > 1\\\\\\\\n             if self.config.ulysses_sequence_parallel_size > 1:\\\\\\\\n-                values_rmpad = gather_outputs_and_unpad(\\\\\\\\n-                    values_rmpad, gather_dim=0, unpad_dim=0, padding_size=pad_size\\\\\\\\n-                )\\\\\\\\n+                values_rmpad = gather_outputs_and_unpad(values_rmpad, gather_dim=0, unpad_dim=0, padding_size=pad_size)\\\\\\\\n \\\\\\\\n             # pad it back\\\\\\\\n             values = pad_input(values_rmpad, indices=indices, batch=batch_size, seqlen=seqlen).squeeze(-1)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"4895fae6e2f2936b28ab0c039631d1a23a49d9f4\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/rollout/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":5,\\\\\\\"deletions\\\\\\\":4,\\\\\\\"changes\\\\\\\":9,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/1e675810f9c6de29a78ec09980d237a84a33747c/verl%2Fworkers%2Frollout%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/1e675810f9c6de29a78ec09980d237a84a33747c/verl%2Fworkers%2Frollout%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Frollout%2Fconfig.py?ref=1e675810f9c6de29a78ec09980d237a84a33747c\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -16,14 +16,17 @@\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n from dataclasses import asdict, dataclass, field\\\\\\\\n+from typing import Any, Dict\\\\\\\\n \\\\\\\\n \\\\\\\\n @dataclass\\\\\\\\n class RolloutConfig:\\\\\\\\n     name: str = \\\\\\\\\\\\\\\"vllm\\\\\\\\\\\\\\\"\\\\\\\\n+    n: int = 1\\\\\\\\n     temperature: float = 1.0\\\\\\\\n-    top_k: int = -1\\\\\\\\n     top_p: float = 1.0\\\\\\\\n+    top_k: int = -1\\\\\\\\n+    limit_images: int = 0\\\\\\\\n     dtype: str = \\\\\\\\\\\\\\\"bf16\\\\\\\\\\\\\\\"\\\\\\\\n     gpu_memory_utilization: float = 0.5\\\\\\\\n     ignore_eos: bool = False\\\\\\\\n@@ -34,9 +37,7 @@ class RolloutConfig:\\\\\\\\n     max_num_batched_tokens: int = 8192\\\\\\\\n     max_num_seqs: int = 1024\\\\\\\\n     disable_log_stats: bool = True\\\\\\\\n-    do_sample: bool = True\\\\\\\\n-    n: int = 1\\\\\\\\n-    limit_images: int = 0\\\\\\\\n+    val_override_config: Dict[str, Any] = field(default_factory=dict)\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"auto keys\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     prompt_length: int = field(default=-1, init=False)\\\\\\\\n     response_length: int = field(default=-1, init=False)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"dfc68ecc12981a8ac6409eddad35eb54f6bec0fb\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":16,\\\\\\\"deletions\\\\\\\":31,\\\\\\\"changes\\\\\\\":47,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/1e675810f9c6de29a78ec09980d237a84a33747c/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/1e675810f9c6de29a78ec09980d237a84a33747c/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py?ref=1e675810f9c6de29a78ec09980d237a84a33747c\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -113,24 +113,14 @@ def update_sampling_params(self, **kwargs):\\\\\\\\n             setattr(self.sampling_params, key, value)\\\\\\\\n \\\\\\\\n     @torch.no_grad()\\\\\\\\n-    def generate_sequences(self, prompts: DataProto, **kwargs) -> DataProto:\\\\\\\\n+    def generate_sequences(self, prompts: DataProto) -> DataProto:\\\\\\\\n         # left-padded attention_mask\\\\\\\\n         input_ids: torch.Tensor = prompts.batch[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"]  # (bs, prompt_length)\\\\\\\\n         attention_mask: torch.Tensor = prompts.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n         position_ids: torch.Tensor = prompts.batch[\\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\"]\\\\\\\\n         eos_token_id: int = prompts.meta_info[\\\\\\\\\\\\\\\"eos_token_id\\\\\\\\\\\\\\\"]\\\\\\\\n         batch_size = input_ids.size(0)\\\\\\\\n \\\\\\\\n-        do_sample = prompts.meta_info.get(\\\\\\\\\\\\\\\"do_sample\\\\\\\\\\\\\\\", True)\\\\\\\\n-        if not do_sample:\\\\\\\\n-            kwargs = {\\\\\\\\n-                \\\\\\\\\\\\\\\"n\\\\\\\\\\\\\\\": 1,\\\\\\\\n-                \\\\\\\\\\\\\\\"temperature\\\\\\\\\\\\\\\": 0.0,\\\\\\\\n-                \\\\\\\\\\\\\\\"top_p\\\\\\\\\\\\\\\": 1.0,\\\\\\\\n-                \\\\\\\\\\\\\\\"top_k\\\\\\\\\\\\\\\": -1,\\\\\\\\n-                \\\\\\\\\\\\\\\"min_p\\\\\\\\\\\\\\\": 0.0,\\\\\\\\n-            }\\\\\\\\n-\\\\\\\\n         non_tensor_batch = prompts.non_tensor_batch\\\\\\\\n         if batch_size != len(non_tensor_batch[\\\\\\\\\\\\\\\"raw_prompt_ids\\\\\\\\\\\\\\\"]):\\\\\\\\n             raise RuntimeError(\\\\\\\\\\\\\\\"vllm sharding manager is not work properly.\\\\\\\\\\\\\\\")\\\\\\\\n@@ -147,29 +137,24 @@ def generate_sequences(self, prompts: DataProto, **kwargs) -> DataProto:\\\\\\\\n             ]\\\\\\\\n \\\\\\\\n         # users can customize different sampling_params at different run\\\\\\\\n-        with self.update_sampling_params(**kwargs):\\\\\\\\n+        with self.update_sampling_params(**prompts.meta_info):\\\\\\\\n             completions: List[RequestOutput] = self.inference_engine.generate(\\\\\\\\n                 prompts=vllm_inputs, sampling_params=self.sampling_params\\\\\\\\n             )\\\\\\\\n-\\\\\\\\n-        response_ids = []\\\\\\\\n-        for completion in completions:\\\\\\\\n-            for output in completion.outputs:\\\\\\\\n-                response_ids.append(output.token_ids)\\\\\\\\n-\\\\\\\\n-        response_ids = VF.pad_2d_list_to_length(\\\\\\\\n-            response_ids, self.pad_token_id, max_length=self.config.response_length\\\\\\\\n-        ).to(input_ids.device)\\\\\\\\n-\\\\\\\\n-        if self.config.n > 1 and do_sample:\\\\\\\\n-            batch_size = batch_size * self.config.n\\\\\\\\n-            input_ids = _repeat_interleave(input_ids, self.config.n)\\\\\\\\n-            attention_mask = _repeat_interleave(attention_mask, self.config.n)\\\\\\\\n-            position_ids = _repeat_interleave(position_ids, self.config.n)\\\\\\\\n-            if \\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\" in non_tensor_batch.keys():\\\\\\\\n-                non_tensor_batch[\\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\"] = _repeat_interleave(\\\\\\\\n-                    non_tensor_batch[\\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\"], self.config.n\\\\\\\\n-                )\\\\\\\\n+            response_ids = [output.token_ids for completion in completions for output in completion.outputs]\\\\\\\\n+            response_ids = VF.pad_2d_list_to_length(\\\\\\\\n+                response_ids, self.pad_token_id, max_length=self.config.response_length\\\\\\\\n+            ).to(input_ids.device)\\\\\\\\n+\\\\\\\\n+            if self.sampling_params.n > 1:\\\\\\\\n+                batch_size = batch_size * self.sampling_params.n\\\\\\\\n+                input_ids = _repeat_interleave(input_ids, self.sampling_params.n)\\\\\\\\n+                attention_mask = _repeat_interleave(attention_mask, self.sampling_params.n)\\\\\\\\n+                position_ids = _repeat_interleave(position_ids, self.sampling_params.n)\\\\\\\\n+                if \\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\" in non_tensor_batch.keys():\\\\\\\\n+                    non_tensor_batch[\\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\"] = _repeat_interleave(\\\\\\\\n+                        non_tensor_batch[\\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\"], self.sampling_params.n\\\\\\\\n+                    )\\\\\\\\n \\\\\\\\n         sequence_ids = torch.cat([input_ids, response_ids], dim=-1)\\\\\\\\n         response_length = response_ids.size(1)\\\\\\\"}],\\\\\\\"sha\\\\\\\":\\\\\\\"1e675810f9c6de29a78ec09980d237a84a33747c\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"committer\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"noreply@github.com\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-17T19:04:15Z\\\\\\\"},\\\\\\\"author\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"hiyouga@buaa.edu.cn\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-17T19:04:15Z\\\\\\\"}}},\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "This commit doesn't have the parameter changes I'm looking for. Let me search for a commit that actually changes the `micro_batch_size_per_device_for_update` from 4 to 1. Let me search through the commit history more systematically.\n\nLet me look at a much earlier commit - based on my analysis so far, it seems like the commit `6c0bba7ebc1dcec2101ca6a620db503c108c6f9e` shows these values as `micro_batch_size_per_device_for_update: 4` and `micro_batch_size_per_device_for_experience: 16`, so I need to find the commit that changed these FROM something else TO these values, or more likely, changed them FROM these values TO something else (like 1 and 2).\n\nLet me check a few more recent commits to find where the parameter changes to 1 and 2 occurred. Let me go back to checking more commits after the one I found:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"ref\\\": \\\"26c2181ad8c7286e822216c778177a5f3a93bc14\\\"}\", \"include_output_fields\": [\"sha\", \"commit.author\", \"commit.committer\", \"files\"]}",
    "call_id": "toolu_014HA1n2Ey5V6Ns5Wu6KWxYw",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_014HA1n2Ey5V6Ns5Wu6KWxYw",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"files\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"3a99fb72ae572c508b336d00c6634c3d565c3c7b\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"README.md\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":3,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":6,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/README.md\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/README.md\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/README.md?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -52,9 +52,9 @@ docker pull hiyouga/verl:ngc-th2.6.0-cu120-vllm0.8.2\\\\\\\\n \\\\\\\\n \\\\\\\\\\\\\\\\* *estimated*\\\\\\\\n \\\\\\\\n-| Method                   | Bits |  1.5B  |   3B   |   7B   |\\\\\\\\n-| ------------------------ | ---- | ------ | ------ | ------ |\\\\\\\\n-| GRPO Full Fine-Tuning    |  AMP | 2*24GB | 4*40GB | 8*40GB |\\\\\\\\n+| Method                   | Bits |  1.5B  |   3B   |   7B   |   32B  |\\\\\\\\n+| ------------------------ | ---- | ------ | ------ | ------ | ------ |\\\\\\\\n+| GRPO Full Fine-Tuning    |  AMP | 2*24GB | 4*40GB | 8*40GB | 8*80GB |\\\\\\\\n \\\\\\\\n > [!NOTE]\\\\\\\\n > We are working hard to reduce the VRAM in RL training, LoRA support will be integrated in next updates.\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"1bf9f527068558009ef09da68423df1300a06b43\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/config.yaml\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fconfig.yaml?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -74,6 +74,7 @@ trainer:\\\\\\\\n   experiment_name: qwen2_5_7b_math_grpo\\\\\\\\n   n_gpus_per_node: 8\\\\\\\\n   nnodes: 1\\\\\\\\n+  report_kl: true\\\\\\\\n   val_freq: 5  # -1 to disable\\\\\\\\n   val_before_train: true\\\\\\\\n   val_only: false\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"7f39780bf90cf96780d4f93da6d8075d0408ba52\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_7b_math_grpo.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":3,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":3,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/examples%2Fqwen2_5_7b_math_grpo.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/examples%2Fqwen2_5_7b_math_grpo.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_7b_math_grpo.sh?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -7,6 +7,9 @@ SYSTEM_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal mono\\\\\\\\n \\\\\\\\n python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n+    data.train_files=hiyouga/math12k@train \\\\\\\\\\\\\\\\\\\\\\\\n+    data.val_files=hiyouga/math12k@test \\\\\\\\\\\\\\\\\\\\\\\\n     data.system_prompt=\\\\\\\\\\\\\\\"${SYSTEM_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n+    trainer.experiment_name=qwen2_5_7b_math_grpo \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.n_gpus_per_node=8\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"53618704e542eb1863b83b1afa3fa7d841cd1cd2\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -13,6 +13,7 @@ python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.micro_batch_size_per_device_for_update=1 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.micro_batch_size_per_device_for_experience=8 \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.actor.fsdp.torch_dtype=bf16 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.tensor_parallel_size=8 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.enable_chunked_prefill=false \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.experiment_name=qwen2_5_vl_32b_geo_grpo \\\\\\\\\\\\\\\\\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"5cf7c78f91c50caaeeb7489023e71d1c8bbd0adb\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/protocol.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":37,\\\\\\\"deletions\\\\\\\":7,\\\\\\\"changes\\\\\\\":44,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Fprotocol.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Fprotocol.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fprotocol.py?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -32,7 +32,6 @@\\\\\\\\n from torch.utils.data import DataLoader\\\\\\\\n \\\\\\\\n from .utils.py_functional import union_two_dict\\\\\\\\n-from .utils.torch_functional import allgather_dict_tensors\\\\\\\\n \\\\\\\\n \\\\\\\\n try:\\\\\\\\n@@ -616,13 +615,44 @@ def dispatch_fn(x, i, chunks):\\\\\\\\n         return arg_future_lst\\\\\\\\n \\\\\\\\n     def get(self):\\\\\\\\n-        output = ray.get(self.futures)  # dp_size.\\\\\\\\n-        for o in output:\\\\\\\\n-            assert isinstance(o, DataProto)\\\\\\\\n-        output = self.collect_fn(output)  # select dp, concat\\\\\\\\n+        outputs = ray.get(self.futures)  # dp_size.\\\\\\\\n+        for output in outputs:\\\\\\\\n+            assert isinstance(output, DataProto)\\\\\\\\n+\\\\\\\\n+        outputs = self.collect_fn(outputs)  # select dp, concat\\\\\\\\n         if self.dispatch_fn is not None:\\\\\\\\n-            output = self.dispatch_fn(output)  # split in batch dim, select using dp\\\\\\\\n-        return output\\\\\\\\n+            outputs = self.dispatch_fn(outputs)  # split in batch dim, select using dp\\\\\\\\n+\\\\\\\\n+        return outputs\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def allgather_dict_tensors(\\\\\\\\n+    tensors: Union[Dict[str, torch.Tensor], TensorDict], size: int, group: ProcessGroup, dim: int = 0\\\\\\\\n+) -> Union[Dict[str, torch.Tensor], TensorDict]:\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    TODO: optimize this.\\\\\\\\n+    - We can use async ops\\\\\\\\n+    - We can use only one allgather\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    if isinstance(tensors, TensorDict):\\\\\\\\n+        is_tensor_dict = True\\\\\\\\n+        tensors_as_dict = tensors.to_dict()\\\\\\\\n+    else:\\\\\\\\n+        tensors_as_dict = tensors\\\\\\\\n+        is_tensor_dict = False\\\\\\\\n+\\\\\\\\n+    output = {}\\\\\\\\n+    sorted_keys = sorted(tensors_as_dict.keys())\\\\\\\\n+    for key in sorted_keys:\\\\\\\\n+        val = tensors_as_dict[key]\\\\\\\\n+        output[key] = [torch.empty_like(val) for _ in range(size)]\\\\\\\\n+        torch.distributed.all_gather(output[key], val, group=group, async_op=False)\\\\\\\\n+        output[key] = torch.cat(output[key], dim=dim)\\\\\\\\n+\\\\\\\\n+    if is_tensor_dict:\\\\\\\\n+        output = TensorDict(source=output, batch_size=tensors.batch_size[0] * size)\\\\\\\\n+\\\\\\\\n+    return output\\\\\\\\n \\\\\\\\n \\\\\\\\n def all_gather_data_proto(data: DataProto, size: int, group: ProcessGroup) -> None:\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"c629bc063ae4da3143ab15a58b6ffdfb31fd61ad\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Ftrainer%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Ftrainer%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fconfig.py?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -70,6 +70,7 @@ class TrainerConfig:\\\\\\\\n     logger: Tuple[str] = (\\\\\\\\\\\\\\\"console\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"wandb\\\\\\\\\\\\\\\")\\\\\\\\n     nnodes: int = 1\\\\\\\\n     n_gpus_per_node: int = 8\\\\\\\\n+    report_kl: bool = True\\\\\\\\n     critic_warmup: int = 0\\\\\\\\n     val_freq: int = -1\\\\\\\\n     val_before_train: bool = True\\\\\\\\n@@ -98,6 +99,7 @@ def post_init(self):\\\\\\\\n         self.worker.actor.use_kl_loss = self.algorithm.use_kl_loss\\\\\\\\n         self.worker.actor.kl_penalty = self.algorithm.kl_penalty\\\\\\\\n         self.worker.actor.kl_coef = self.algorithm.kl_coef\\\\\\\\n+        self.worker.actor.report_kl = self.trainer.report_kl\\\\\\\\n \\\\\\\\n     def deep_post_init(self):\\\\\\\\n         recursive_post_init(self)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"fccef62d21382cd2ea864b94ada28b7a569c8e2e\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/core_algos.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":13,\\\\\\\"deletions\\\\\\\":14,\\\\\\\"changes\\\\\\\":27,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Ftrainer%2Fcore_algos.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Ftrainer%2Fcore_algos.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fcore_algos.py?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -24,6 +24,7 @@\\\\\\\\n \\\\\\\\n import numpy as np\\\\\\\\n import torch\\\\\\\\n+import torch.nn.functional as F\\\\\\\\n \\\\\\\\n from ..utils import torch_functional as VF\\\\\\\\n \\\\\\\\n@@ -38,10 +39,7 @@ def update(self, current_kl: float, n_steps: int) -> None: ...\\\\\\\\n \\\\\\\\n \\\\\\\\n class AdaptiveKLController(KLController):\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    Adaptive KL controller described in the paper:\\\\\\\\n-    https://arxiv.org/pdf/1909.08593.pdf\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Adaptive KL controller described in: https://arxiv.org/pdf/1909.08593.pdf\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n     def __init__(self, init_kl_coef: float, target_kl: float, horizon: float):\\\\\\\\n         self.value = init_kl_coef\\\\\\\\n@@ -66,6 +64,7 @@ def update(self, current_kl: float, n_steps: int) -> None:\\\\\\\\n \\\\\\\\n \\\\\\\\n def get_kl_controller(algorithm_config: \\\\\\\\\\\\\\\"AlgorithmConfig\\\\\\\\\\\\\\\") -> KLController:\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Adapted from https://github.com/huggingface/trl/blob/v0.11.0/trl/trainer/ppo_trainer.py#L319\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     if algorithm_config.kl_type == \\\\\\\\\\\\\\\"fixed\\\\\\\\\\\\\\\":\\\\\\\\n         kl_ctrl = FixedKLController(init_kl_coef=algorithm_config.kl_coef)\\\\\\\\n     elif algorithm_config.kl_type == \\\\\\\\\\\\\\\"adaptive\\\\\\\\\\\\\\\":\\\\\\\\n@@ -89,7 +88,7 @@ def compute_gae_advantage_return(\\\\\\\\n     gamma: torch.Tensor,\\\\\\\\n     lam: torch.Tensor,\\\\\\\\n ) -> Tuple[torch.Tensor, torch.Tensor]:\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Adapted from https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_trainer.py\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Adapted from https://github.com/huggingface/trl/blob/v0.16.0/trl/trainer/ppo_trainer.py#L513\\\\\\\\n \\\\\\\\n     Args:\\\\\\\\n         token_level_rewards: `(torch.Tensor)`\\\\\\\\n@@ -120,8 +119,8 @@ def compute_gae_advantage_return(\\\\\\\\n         advantages_reversed.append(lastgaelam)\\\\\\\\n \\\\\\\\n     advantages = torch.stack(advantages_reversed[::-1], dim=1)\\\\\\\\n-    returns = advantages + values\\\\\\\\n-    advantages = VF.masked_whiten(advantages, eos_mask)\\\\\\\\n+    returns = (advantages + values) * eos_mask\\\\\\\\n+    advantages = VF.masked_whiten(advantages, eos_mask) * eos_mask\\\\\\\\n     return advantages, returns\\\\\\\\n \\\\\\\\n \\\\\\\\n@@ -245,7 +244,8 @@ def compute_reinforce_plus_plus_outcome_advantage(\\\\\\\\n         running_return = running_return * eos_mask[:, t]\\\\\\\\n \\\\\\\\n     advantages = VF.masked_whiten(returns, eos_mask)\\\\\\\\n-    advantages = advantages * eos_mask\\\\\\\\n+    advantages *= eos_mask\\\\\\\\n+    returns *= eos_mask\\\\\\\\n     return advantages, returns\\\\\\\\n \\\\\\\\n \\\\\\\\n@@ -274,7 +274,7 @@ def compute_remax_outcome_advantage(\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     response_length = token_level_rewards.shape[-1]\\\\\\\\n     # scores = token_level_rewards.sum(dim=-1)\\\\\\\\n-    returns = (token_level_rewards * eos_mask).flip(dims=[-1]).cumsum(dim=-1).flip(dims=[-1])\\\\\\\\n+    returns = (token_level_rewards * eos_mask).flip(dims=[-1]).cumsum(dim=-1).flip(dims=[-1]) * eos_mask\\\\\\\\n     advantages = returns - reward_baselines.unsqueeze(-1).tile([1, response_length]) * eos_mask\\\\\\\\n     return advantages, returns\\\\\\\\n \\\\\\\\n@@ -362,7 +362,7 @@ def compute_value_loss(\\\\\\\\n         vf_clipfrac: a float\\\\\\\\n             The ratio of vf being clipped\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    vpredclipped = VF.clip_by_value(vpreds, values - cliprange_value, values + cliprange_value)\\\\\\\\n+    vpredclipped = torch.clamp(vpreds, values - cliprange_value, values + cliprange_value)\\\\\\\\n     vf_losses1 = torch.square(vpreds - returns)\\\\\\\\n     vf_losses2 = torch.square(vpredclipped - returns)\\\\\\\\n     vf_loss = 0.5 * VF.masked_mean(torch.max(vf_losses1, vf_losses2), eos_mask)\\\\\\\\n@@ -372,7 +372,7 @@ def compute_value_loss(\\\\\\\\n \\\\\\\\n def kl_penalty(log_probs: torch.FloatTensor, ref_log_probs: torch.FloatTensor, kl_penalty: str) -> torch.Tensor:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Compute KL divergence given log_probs and ref_log_probs.\\\\\\\\n-    Copied from https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_trainer.py#L1104\\\\\\\\n+    Copied from https://github.com/huggingface/trl/blob/v0.11.0/trl/trainer/ppo_trainer.py#L1150\\\\\\\\n \\\\\\\\n     Args:\\\\\\\\n         log_probs: torch.Tensor\\\\\\\\n@@ -399,7 +399,6 @@ def kl_penalty(log_probs: torch.FloatTensor, ref_log_probs: torch.FloatTensor, k\\\\\\\\n         return torch.clamp(kld, min=-10, max=10)\\\\\\\\n \\\\\\\\n     if kl_penalty == \\\\\\\\\\\\\\\"full\\\\\\\\\\\\\\\":\\\\\\\\n-        # here log_probs and ref_log_probs should contain the logits for every token in vocabulary\\\\\\\\n-        raise NotImplementedError\\\\\\\\n+        return F.kl_div(ref_log_probs, log_probs, log_target=True, reduction=\\\\\\\\\\\\\\\"none\\\\\\\\\\\\\\\").sum(-1)\\\\\\\\n \\\\\\\\n-    raise NotImplementedError\\\\\\\\n+    raise NotImplementedError(f\\\\\\\\\\\\\\\"Unknown KL penalty: {kl_penalty}.\\\\\\\\\\\\\\\")\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"94274bba06bef7616f7101116c6e5cab98cb51e2\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/ray_trainer.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":30,\\\\\\\"deletions\\\\\\\":56,\\\\\\\"changes\\\\\\\":86,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fray_trainer.py?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -157,72 +157,40 @@ def apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.KLController, kl_penal\\\\\\\\n     return data, metrics\\\\\\\\n \\\\\\\\n \\\\\\\\n-def compute_advantage(data: DataProto, adv_estimator: AdvantageEstimator, gamma=1.0, lam=1.0):\\\\\\\\n-    # prepare response group\\\\\\\\n-    # TODO: add other ways to estimate advantages\\\\\\\\n+def compute_advantage(data: DataProto, adv_estimator: AdvantageEstimator, gamma: float = 1.0, lam: float = 1.0):\\\\\\\\n+    responses = data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n+    response_length = responses.size(-1)\\\\\\\\n+    attention_mask = data.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n+    response_mask = attention_mask[:, -response_length:]\\\\\\\\n+    token_level_rewards = data.batch[\\\\\\\\\\\\\\\"token_level_rewards\\\\\\\\\\\\\\\"]\\\\\\\\n+    index = data.non_tensor_batch[\\\\\\\\\\\\\\\"uid\\\\\\\\\\\\\\\"]\\\\\\\\n     if adv_estimator == AdvantageEstimator.GAE:\\\\\\\\n         values = data.batch[\\\\\\\\\\\\\\\"values\\\\\\\\\\\\\\\"]\\\\\\\\n-        responses = data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-        response_length = responses.size(-1)\\\\\\\\n-        attention_mask = data.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n-        response_mask = attention_mask[:, -response_length:]\\\\\\\\n-        token_level_rewards = data.batch[\\\\\\\\\\\\\\\"token_level_rewards\\\\\\\\\\\\\\\"]\\\\\\\\n         advantages, returns = core_algos.compute_gae_advantage_return(\\\\\\\\n             token_level_rewards=token_level_rewards, values=values, eos_mask=response_mask, gamma=gamma, lam=lam\\\\\\\\n         )\\\\\\\\n-        data.batch[\\\\\\\\\\\\\\\"advantages\\\\\\\\\\\\\\\"] = advantages\\\\\\\\n-        data.batch[\\\\\\\\\\\\\\\"returns\\\\\\\\\\\\\\\"] = returns\\\\\\\\n     elif adv_estimator == AdvantageEstimator.GRPO:\\\\\\\\n-        token_level_rewards = data.batch[\\\\\\\\\\\\\\\"token_level_rewards\\\\\\\\\\\\\\\"]\\\\\\\\n-        index = data.non_tensor_batch[\\\\\\\\\\\\\\\"uid\\\\\\\\\\\\\\\"]\\\\\\\\n-        responses = data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-        response_length = responses.size(-1)\\\\\\\\n-        attention_mask = data.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n-        response_mask = attention_mask[:, -response_length:]\\\\\\\\n         advantages, returns = core_algos.compute_grpo_outcome_advantage(\\\\\\\\n             token_level_rewards=token_level_rewards, eos_mask=response_mask, index=index\\\\\\\\n         )\\\\\\\\n-        data.batch[\\\\\\\\\\\\\\\"advantages\\\\\\\\\\\\\\\"] = advantages\\\\\\\\n-        data.batch[\\\\\\\\\\\\\\\"returns\\\\\\\\\\\\\\\"] = returns\\\\\\\\n     elif adv_estimator == AdvantageEstimator.REINFORCE_PLUS_PLUS:\\\\\\\\n-        token_level_rewards = data.batch[\\\\\\\\\\\\\\\"token_level_rewards\\\\\\\\\\\\\\\"]\\\\\\\\n-        responses = data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-        response_length = responses.size(-1)\\\\\\\\n-        attention_mask = data.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n-        response_mask = attention_mask[:, -response_length:]\\\\\\\\n         advantages, returns = core_algos.compute_reinforce_plus_plus_outcome_advantage(\\\\\\\\n             token_level_rewards=token_level_rewards, eos_mask=response_mask, gamma=gamma\\\\\\\\n         )\\\\\\\\n-        data.batch[\\\\\\\\\\\\\\\"advantages\\\\\\\\\\\\\\\"] = advantages\\\\\\\\n-        data.batch[\\\\\\\\\\\\\\\"returns\\\\\\\\\\\\\\\"] = returns\\\\\\\\n     elif adv_estimator == AdvantageEstimator.REMAX:\\\\\\\\n-        token_level_rewards = data.batch[\\\\\\\\\\\\\\\"token_level_rewards\\\\\\\\\\\\\\\"]\\\\\\\\n-        index = data.non_tensor_batch[\\\\\\\\\\\\\\\"uid\\\\\\\\\\\\\\\"]\\\\\\\\n-        responses = data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-        response_length = responses.size(-1)\\\\\\\\n-        attention_mask = data.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n-        response_mask = attention_mask[:, -response_length:]\\\\\\\\n         reward_baselines = data.batch[\\\\\\\\\\\\\\\"reward_baselines\\\\\\\\\\\\\\\"]\\\\\\\\n         advantages, returns = core_algos.compute_remax_outcome_advantage(\\\\\\\\n             token_level_rewards=token_level_rewards, reward_baselines=reward_baselines, eos_mask=response_mask\\\\\\\\n         )\\\\\\\\n-        data.batch[\\\\\\\\\\\\\\\"advantages\\\\\\\\\\\\\\\"] = advantages\\\\\\\\n-        data.batch[\\\\\\\\\\\\\\\"returns\\\\\\\\\\\\\\\"] = returns\\\\\\\\n     elif adv_estimator == AdvantageEstimator.RLOO:\\\\\\\\n-        token_level_rewards = data.batch[\\\\\\\\\\\\\\\"token_level_rewards\\\\\\\\\\\\\\\"]\\\\\\\\n-        index = data.non_tensor_batch[\\\\\\\\\\\\\\\"uid\\\\\\\\\\\\\\\"]\\\\\\\\n-        responses = data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-        response_length = responses.size(-1)\\\\\\\\n-        attention_mask = data.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n-        response_mask = attention_mask[:, -response_length:]\\\\\\\\n         advantages, returns = core_algos.compute_rloo_outcome_advantage(\\\\\\\\n             token_level_rewards=token_level_rewards, eos_mask=response_mask, index=index\\\\\\\\n         )\\\\\\\\n-        data.batch[\\\\\\\\\\\\\\\"advantages\\\\\\\\\\\\\\\"] = advantages\\\\\\\\n-        data.batch[\\\\\\\\\\\\\\\"returns\\\\\\\\\\\\\\\"] = returns\\\\\\\\n     else:\\\\\\\\n         raise NotImplementedError\\\\\\\\n \\\\\\\\n+    data.batch[\\\\\\\\\\\\\\\"advantages\\\\\\\\\\\\\\\"] = advantages\\\\\\\\n+    data.batch[\\\\\\\\\\\\\\\"returns\\\\\\\\\\\\\\\"] = returns\\\\\\\\n     return data\\\\\\\\n \\\\\\\\n \\\\\\\\n@@ -239,8 +207,6 @@ class RayPPOTrainer:\\\\\\\\n     Note that this trainer runs on the driver process on a single CPU/GPU node.\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n-    # TODO: support each role have individual ray_worker_group_cls,\\\\\\\\n-    # i.e., support different backend of different role\\\\\\\\n     def __init__(\\\\\\\\n         self,\\\\\\\\n         config: PPOConfig,\\\\\\\\n@@ -268,31 +234,35 @@ def __init__(\\\\\\\\n \\\\\\\\n         self.role_worker_mapping = role_worker_mapping\\\\\\\\n         self.resource_pool_manager = resource_pool_manager\\\\\\\\n-        self.use_reference_policy = Role.RefPolicy in role_worker_mapping and config.algorithm.kl_coef > 1e-6\\\\\\\\n         self.use_reward_model = Role.RewardModel in role_worker_mapping\\\\\\\\n         self.ray_worker_group_cls = ray_worker_group_cls\\\\\\\\n         self.val_generations_logger = ValGenerationsLogger()\\\\\\\\n \\\\\\\\n         # define KL control\\\\\\\\n-        if self.use_reference_policy:\\\\\\\\n+        if Role.RefPolicy in role_worker_mapping and config.trainer.report_kl:\\\\\\\\n+            self.use_reference_policy = True\\\\\\\\n             self.kl_ctrl = core_algos.get_kl_controller(config.algorithm)\\\\\\\\n         else:\\\\\\\\n+            self.use_reference_policy = False\\\\\\\\n             self.kl_ctrl = core_algos.FixedKLController(init_kl_coef=0.0)\\\\\\\\n \\\\\\\\n-        if config.algorithm.adv_estimator not in list(AdvantageEstimator):\\\\\\\\n-            raise NotImplementedError(f\\\\\\\\\\\\\\\"Unknown advantage estimator: {config.algorithm.adv_estimator}.\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n         if config.algorithm.adv_estimator == AdvantageEstimator.GAE:\\\\\\\\n             self.use_critic = True\\\\\\\\n         else:\\\\\\\\n             self.use_critic = False\\\\\\\\n \\\\\\\\n-        if self.config.data.rollout_batch_size % self.config.worker.actor.global_batch_size != 0:\\\\\\\\n+        if config.algorithm.adv_estimator not in list(AdvantageEstimator):\\\\\\\\n+            raise NotImplementedError(f\\\\\\\\\\\\\\\"Unknown advantage estimator: {config.algorithm.adv_estimator}.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+        if config.data.rollout_batch_size % config.worker.actor.global_batch_size != 0:\\\\\\\\n             raise ValueError(\\\\\\\\\\\\\\\"Rollout batch size must be divisible by global batch size.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n-        if self.use_critic and self.config.data.rollout_batch_size % self.config.worker.critic.global_batch_size != 0:\\\\\\\\n+        if self.use_critic and config.data.rollout_batch_size % config.worker.critic.global_batch_size != 0:\\\\\\\\n             raise ValueError(\\\\\\\\\\\\\\\"Rollout batch size must be divisible by global batch size.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n+        if config.algorithm.kl_coef > 1e-8 and not config.trainer.report_kl:\\\\\\\\n+            raise ValueError(\\\\\\\\\\\\\\\"KL coefficient must be 0 if report_kl is False.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n         self._create_dataloader()\\\\\\\\n \\\\\\\\n     def _create_dataloader(self) -> None:\\\\\\\\n@@ -584,7 +554,7 @@ def fit(self):\\\\\\\\n         # currently, we only support validation using the reward_function.\\\\\\\\n         if self.val_reward_fn is not None and self.config.trainer.val_before_train:\\\\\\\\n             val_metrics = self._validate()\\\\\\\\n-            print(f\\\\\\\\\\\\\\\"Initial validation metrics: {json.dumps(val_metrics, indent=2)}.\\\\\\\\\\\\\\\")\\\\\\\\n+            print(f\\\\\\\\\\\\\\\"Initial validation metrics: {json.dumps(val_metrics, indent=2)}\\\\\\\\\\\\\\\")\\\\\\\\n             logger.log(data=val_metrics, step=self.global_step)\\\\\\\\n             if self.config.trainer.val_only:\\\\\\\\n                 return\\\\\\\\n@@ -639,7 +609,7 @@ def fit(self):\\\\\\\\n                     # compute reward\\\\\\\\n                     with _timer(\\\\\\\\\\\\\\\"reward\\\\\\\\\\\\\\\", timing_raw):\\\\\\\\n                         if self.use_reward_model:\\\\\\\\n-                            raise NotImplementedError(\\\\\\\\\\\\\\\"RM is not supported for PPO yet.\\\\\\\\\\\\\\\")\\\\\\\\n+                            raise NotImplementedError(\\\\\\\\\\\\\\\"Reward model is not supported yet.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n                         # we combine with rule-based rm\\\\\\\\n                         reward_tensor, reward_metrics = self.reward_fn(batch)\\\\\\\\n@@ -662,8 +632,8 @@ def fit(self):\\\\\\\\n                         old_log_probs = self.actor_rollout_wg.compute_log_probs(batch)\\\\\\\\n                         batch = batch.union(old_log_probs)\\\\\\\\n \\\\\\\\n+                    # compute ref_log_probs\\\\\\\\n                     if self.use_reference_policy:\\\\\\\\n-                        # compute reference log_prob\\\\\\\\n                         with _timer(\\\\\\\\\\\\\\\"ref\\\\\\\\\\\\\\\", timing_raw):\\\\\\\\n                             ref_log_probs = self.ref_policy_wg.compute_ref_log_probs(batch)\\\\\\\\n                             batch = batch.union(ref_log_probs)\\\\\\\\n@@ -734,11 +704,15 @@ def fit(self):\\\\\\\\n \\\\\\\\n         # perform validation after training\\\\\\\\n         if self.val_reward_fn is not None:\\\\\\\\n-            if val_metrics is None or self.global_step % self.config.trainer.val_freq != 0:\\\\\\\\n+            if (\\\\\\\\n+                val_metrics is None\\\\\\\\n+                or self.config.trainer.val_freq <= 0\\\\\\\\n+                or self.global_step % self.config.trainer.val_freq != 0\\\\\\\\n+            ):\\\\\\\\n                 val_metrics = self._validate()\\\\\\\\n                 logger.log(data=val_metrics, step=self.global_step)\\\\\\\\n \\\\\\\\n-            print(f\\\\\\\\\\\\\\\"Final validation metrics: {json.dumps(val_metrics, indent=2)}.\\\\\\\\\\\\\\\")\\\\\\\\n+            print(f\\\\\\\\\\\\\\\"Final validation metrics: {json.dumps(val_metrics, indent=2)}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n-        if self.config.trainer.save_freq > 0 and self.global_step % self.config.trainer.save_freq != 0:\\\\\\\\n+        if self.config.trainer.save_freq <= 0 or self.global_step % self.config.trainer.save_freq != 0:\\\\\\\\n             self._save_checkpoint()\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"71d4fe2c4e9c481972242d5b20cf82e669f68e30\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/model_utils.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":14,\\\\\\\"deletions\\\\\\\":11,\\\\\\\"changes\\\\\\\":25,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Futils%2Fmodel_utils.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Futils%2Fmodel_utils.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fmodel_utils.py?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -16,7 +16,7 @@\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n from functools import lru_cache\\\\\\\\n-from typing import Tuple\\\\\\\\n+from typing import Optional, Tuple\\\\\\\\n \\\\\\\\n import torch\\\\\\\\n import torch.distributed as dist\\\\\\\\n@@ -28,14 +28,15 @@ def is_rank0() -> int:\\\\\\\\n     return (not dist.is_initialized()) or (dist.get_rank() == 0)\\\\\\\\n \\\\\\\\n \\\\\\\\n-def print_gpu_memory_usage(prefix: str) -> None:\\\\\\\\n+def print_gpu_memory_usage(prefix: str = \\\\\\\\\\\\\\\"GPU memory usage\\\\\\\\\\\\\\\") -> None:\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Report the current GPU VRAM usage.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     if is_rank0():\\\\\\\\n-        memory_allocated = torch.cuda.memory_allocated() / (1024**3)\\\\\\\\n-        memory_reserved = torch.cuda.memory_reserved() / (1024**3)\\\\\\\\n-        print(f\\\\\\\\\\\\\\\"{prefix} memory allocated: {memory_allocated:.2f} GB, memory reserved: {memory_reserved:.2f} GB.\\\\\\\\\\\\\\\")\\\\\\\\n+        free_mem, total_mem = torch.cuda.mem_get_info()\\\\\\\\n+        print(f\\\\\\\\\\\\\\\"{prefix}: {(total_mem - free_mem) / (1024**3):.2f} GB / {total_mem / (1024**3):.2f} GB.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n \\\\\\\\n-def get_model_size(model: nn.Module, scale: str = \\\\\\\\\\\\\\\"auto\\\\\\\\\\\\\\\") -> Tuple[float, str]:\\\\\\\\n+def _get_model_size(model: nn.Module, scale: str = \\\\\\\\\\\\\\\"auto\\\\\\\\\\\\\\\") -> Tuple[float, str]:\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Compute the model size.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     n_params = sum(p.numel() for p in model.parameters())\\\\\\\\n \\\\\\\\n     if scale == \\\\\\\\\\\\\\\"auto\\\\\\\\\\\\\\\":\\\\\\\\n@@ -62,9 +63,11 @@ def get_model_size(model: nn.Module, scale: str = \\\\\\\\\\\\\\\"auto\\\\\\\\\\\\\\\") -> Tuple[float, str]:\\\\\\\\n     return n_params, scale\\\\\\\\n \\\\\\\\n \\\\\\\\n-def print_model_size(model: nn.Module, name: str = None) -> None:\\\\\\\\n-    n_params, scale = get_model_size(model, scale=\\\\\\\\\\\\\\\"auto\\\\\\\\\\\\\\\")\\\\\\\\n-    if name is None:\\\\\\\\n-        name = model.__class__.__name__\\\\\\\\n+def print_model_size(model: nn.Module, name: Optional[str] = None) -> None:\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Print the model size.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    if is_rank0():\\\\\\\\n+        n_params, scale = _get_model_size(model, scale=\\\\\\\\\\\\\\\"auto\\\\\\\\\\\\\\\")\\\\\\\\n+        if name is None:\\\\\\\\n+            name = model.__class__.__name__\\\\\\\\n \\\\\\\\n-    print(f\\\\\\\\\\\\\\\"{name} contains {n_params:.2f}{scale} parameters\\\\\\\\\\\\\\\")\\\\\\\\n+        print(f\\\\\\\\\\\\\\\"{name} contains {n_params:.2f}{scale} parameters.\\\\\\\\\\\\\\\")\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"48c1a7f18836617048db546e24ba52495665f852\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/torch_functional.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":36,\\\\\\\"deletions\\\\\\\":116,\\\\\\\"changes\\\\\\\":152,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Futils%2Ftorch_functional.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Futils%2Ftorch_functional.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Ftorch_functional.py?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -15,14 +15,11 @@\\\\\\\\n Contain small torch utilities\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n-import math\\\\\\\\n-from typing import Dict, List, Literal, Optional, Union\\\\\\\\n+from typing import List, Literal, Optional, Union\\\\\\\\n \\\\\\\\n import torch\\\\\\\\n import torch.distributed\\\\\\\\n import torch.nn.functional as F\\\\\\\\n-from tensordict import TensorDict\\\\\\\\n-from torch.optim import Optimizer\\\\\\\\n from torch.optim.lr_scheduler import LambdaLR\\\\\\\\n \\\\\\\\n \\\\\\\\n@@ -46,8 +43,16 @@ def log_probs_from_logits_flash_attn(logits: torch.Tensor, labels: torch.Tensor)\\\\\\\\n \\\\\\\\n \\\\\\\\n def log_probs_from_logits(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Compute log probs on the label ids given logits.\\\\\\\\n+\\\\\\\\n     We may use torch compile to speed up computing.\\\\\\\\n+\\\\\\\\n+    Args:\\\\\\\\n+        logits (torch.Tensor): logits of the model, shape (batch_size, seqlen, vocab_size)\\\\\\\\n+        labels (torch.Tensor): labels of the model, shape (batch_size, seqlen)\\\\\\\\n+\\\\\\\\n+    Returns:\\\\\\\\n+        torch.Tensor: log probs of the labels, shape (batch_size, seqlen)\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     batch_dim = logits.shape[:-1]\\\\\\\\n     vocab_dim = logits.shape[-1]\\\\\\\\n@@ -61,14 +66,6 @@ def log_probs_from_logits(logits: torch.Tensor, labels: torch.Tensor) -> torch.T\\\\\\\\n     return output.view(*batch_dim)\\\\\\\\n \\\\\\\\n \\\\\\\\n-def clip_by_value(tensor: torch.Tensor, tensor_min: torch.Tensor, tensor_max: torch.Tensor) -> torch.Tensor:\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    Tensor extenstion to torch.clamp\\\\\\\\n-    https://github.com/pytorch/pytorch/issues/2793#issuecomment-428784713\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    return torch.max(torch.min(tensor, tensor_max), tensor_min)\\\\\\\\n-\\\\\\\\n-\\\\\\\\n def masked_mean(values: torch.Tensor, mask: torch.Tensor, dim: int = None) -> torch.Tensor:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Compute mean of tensor with a masked values.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     return (values * mask).sum(dim=dim) / mask.sum(dim=dim)\\\\\\\\n@@ -96,19 +93,22 @@ def masked_whiten(values: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\\\\\\\\n     return (values - mean) * torch.rsqrt(var + 1e-8)\\\\\\\\n \\\\\\\\n \\\\\\\\n-def get_eos_mask(response_ids: torch.Tensor, eos_token: Union[int, List[int]] = 2, dtype: torch.dtype = torch.int64):\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    end of sentence token can be int or list: 1 or [1, 2]\\\\\\\\n-    e.g. eos_token=1\\\\\\\\n-    response_ids: [0, 0, 2, 42, 3, 5, 1, 0, 0]\\\\\\\\n-    eos_mask:     [1, 1, 1, 1,  1, 1, 1, 0, 0]\\\\\\\\n+def get_eos_mask(response_ids: torch.Tensor, eos_token_id: Union[int, List[int]] = 2, dtype: torch.dtype = torch.long):\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Get the mask for the response ids, the mask will be 0 after the first eos token.\\\\\\\\n+\\\\\\\\n+    eos_token_id can be int or list: 1 or [1, 2].\\\\\\\\n+    ```\\\\\\\\n+    e.g. eos_token = 1\\\\\\\\n+    response_ids: [0, 0, 2, 4, 3, 5, 1, 0, 0]\\\\\\\\n+    eos_mask:     [1, 1, 1, 1, 1, 1, 1, 0, 0]\\\\\\\\n+    ```\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    if isinstance(eos_token, int):\\\\\\\\n-        eos_token = [eos_token]\\\\\\\\n+    if isinstance(eos_token_id, int):\\\\\\\\n+        eos_token_id = [eos_token_id]\\\\\\\\n \\\\\\\\n     eos_mask = torch.zeros_like(response_ids, dtype=torch.bool)\\\\\\\\n-    for token in eos_token:\\\\\\\\n-        eos_mask |= response_ids.eq(token)\\\\\\\\n+    for token_id in eos_token_id:\\\\\\\\n+        eos_mask |= response_ids.eq(token_id)\\\\\\\\n \\\\\\\\n     eos_mask = eos_mask.long()\\\\\\\\n     eos_mask = (torch.cumsum(eos_mask, dim=1) - eos_mask).bool()\\\\\\\\n@@ -119,14 +119,13 @@ def get_eos_mask(response_ids: torch.Tensor, eos_token: Union[int, List[int]] =\\\\\\\\n def pad_2d_list_to_length(\\\\\\\\n     response: List[List[int]], pad_token_id: int, max_length: Optional[int] = None\\\\\\\\n ) -> torch.Tensor:\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    pad a 2D list (e.g. responses, log_probs) to a 2D tensor.\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    response_length = max(len(sub_list) for sub_list in response)\\\\\\\\n-    if max_length is not None and max_length > response_length:\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Pad a 2D list (e.g. responses, log_probs) to a 2D tensor.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    max_response_length = max(len(sub_list) for sub_list in response)\\\\\\\\n+    if max_length is not None and max_length > max_response_length:\\\\\\\\n         target_length = max_length\\\\\\\\n     else:\\\\\\\\n-        target_length = response_length\\\\\\\\n+        target_length = max_response_length\\\\\\\\n+\\\\\\\\n     padded_response = [tuple(sub_list) + (pad_token_id,) * (target_length - len(sub_list)) for sub_list in response]\\\\\\\\n     tensor = torch.tensor(padded_response)\\\\\\\\n     return tensor\\\\\\\\n@@ -135,9 +134,7 @@ def pad_2d_list_to_length(\\\\\\\\n def pad_sequence_to_length(\\\\\\\\n     tensor: torch.Tensor, max_seq_len: int, pad_token_id: int, left_pad: bool = False\\\\\\\\n ) -> torch.Tensor:\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    Pad a nD tensors in the last dim to max_seq_len.\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Pad a nD tensors in the last dim to max_seq_len.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     if tensor.size(-1) >= max_seq_len:\\\\\\\\n         return tensor\\\\\\\\n \\\\\\\\n@@ -156,9 +153,7 @@ def postprocess_data(\\\\\\\\n     left_pad: bool = True,\\\\\\\\n     truncation: Literal[\\\\\\\\\\\\\\\"left\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"error\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"error\\\\\\\\\\\\\\\",\\\\\\\\n ):\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    Pad or truncate data.\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Pad or truncate data.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     assert truncation in [\\\\\\\\\\\\\\\"left\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"error\\\\\\\\\\\\\\\"]\\\\\\\\n     seq_length = len(input_ids)\\\\\\\\n     if seq_length < max_length:\\\\\\\\n@@ -186,89 +181,14 @@ def postprocess_data(\\\\\\\\n     return input_ids, attention_mask, position_ids\\\\\\\\n \\\\\\\\n \\\\\\\\n-def get_cosine_schedule_with_warmup(\\\\\\\\n-    optimizer: Optimizer,\\\\\\\\n-    num_warmup_steps: int,\\\\\\\\n-    num_training_steps: int,\\\\\\\\n-    min_lr_ratio: float = 0.0,\\\\\\\\n-    num_cycles: float = 0.5,\\\\\\\\n-    last_epoch: int = -1,\\\\\\\\n-):\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    Create a schedule with a learning rate that decreases following the values of the cosine function between the\\\\\\\\n-    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\\\\\\\\n-    initial lr set in the optimizer.\\\\\\\\n-    Args:\\\\\\\\n-        optimizer (:class:`~torch.optim.Optimizer`):\\\\\\\\n-            The optimizer for which to schedule the learning rate.\\\\\\\\n-        num_warmup_steps (:obj:`int`):\\\\\\\\n-            The number of steps for the warmup phase.\\\\\\\\n-        num_training_steps (:obj:`int`):\\\\\\\\n-            The total number of training steps.\\\\\\\\n-        min_lr_ratio (:obj:`float`, `optional`, defaults to 0.0):\\\\\\\\n-            The minimum lr ratio w.r.t the maximum.\\\\\\\\n-        num_cycles (:obj:`float`, `optional`, defaults to 0.5):\\\\\\\\n-            The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\\\\\\\\n-            following a half-cosine).\\\\\\\\n-        last_epoch (:obj:`int`, `optional`, defaults to -1):\\\\\\\\n-            The index of the last epoch when resuming training.\\\\\\\\n-    Return:\\\\\\\\n-        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    assert min_lr_ratio >= 0 and min_lr_ratio <= 1.0\\\\\\\\n-    coef = (1 - min_lr_ratio) * 0.5\\\\\\\\n-    intercept = (1 + min_lr_ratio) * 0.5\\\\\\\\n-\\\\\\\\n-    def lr_lambda(current_step):\\\\\\\\n-        if current_step < num_warmup_steps:\\\\\\\\n-            return float(current_step) / float(max(1, num_warmup_steps))\\\\\\\\n-        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\\\\\\\\n-        x = math.cos(math.pi * float(num_cycles) * 2.0 * progress)\\\\\\\\n-        return max(0.0, x * coef + intercept)\\\\\\\\n-\\\\\\\\n-    return LambdaLR(optimizer, lr_lambda, last_epoch)\\\\\\\\n-\\\\\\\\n-\\\\\\\\n def get_constant_schedule_with_warmup(\\\\\\\\n-    optimizer: Optimizer,\\\\\\\\n+    optimizer: torch.optim.Optimizer,\\\\\\\\n     num_warmup_steps: int,\\\\\\\\n     last_epoch: int = -1,\\\\\\\\n-):\\\\\\\\n-    def lr_lambda(current_step):\\\\\\\\n-        return min(1, float(current_step) / float(max(1, num_warmup_steps)))\\\\\\\\n-\\\\\\\\n-    return LambdaLR(optimizer, lr_lambda, last_epoch)\\\\\\\\n+) -> torch.optim.lr_scheduler.LRScheduler:\\\\\\\\n+    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Get the lr scheduler for constant lr.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n+    def lr_lambda(current_step: int) -> float:\\\\\\\\n+        return min(1.0, float(current_step) / float(max(1, num_warmup_steps)))\\\\\\\\n \\\\\\\\n-def allgather_dict_tensors(tensors: Union[Dict[str, torch.Tensor], TensorDict], size, group, dim=0):\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    TODO: optimize this.\\\\\\\\n-    - We can use async ops\\\\\\\\n-    - We can use only one allgather\\\\\\\\n-    Args:\\\\\\\\n-        tensors:\\\\\\\\n-        size:\\\\\\\\n-        group:\\\\\\\\n-\\\\\\\\n-    Returns:\\\\\\\\n-\\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-    if isinstance(tensors, TensorDict):\\\\\\\\n-        is_tensor_dict = True\\\\\\\\n-        tensors_as_dict = tensors.to_dict()\\\\\\\\n-    else:\\\\\\\\n-        tensors_as_dict = tensors\\\\\\\\n-        is_tensor_dict = False\\\\\\\\n-\\\\\\\\n-    output = {}\\\\\\\\n-    sorted_keys = sorted(tensors_as_dict.keys())\\\\\\\\n-    for key in sorted_keys:\\\\\\\\n-        val = tensors_as_dict[key]\\\\\\\\n-        output[key] = [torch.empty_like(val) for _ in range(size)]\\\\\\\\n-        torch.distributed.all_gather(output[key], val, group=group, async_op=False)\\\\\\\\n-        output[key] = torch.cat(output[key], dim=dim)\\\\\\\\n-\\\\\\\\n-    if is_tensor_dict:\\\\\\\\n-        output = TensorDict(source=output, batch_size=tensors.batch_size[0] * size)\\\\\\\\n-\\\\\\\\n-    return output\\\\\\\\n+    return LambdaLR(optimizer, lr_lambda, last_epoch)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"7785a100d740ccf641fa9fa9539148b150793fe8\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/actor/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Fworkers%2Factor%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Fworkers%2Factor%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Factor%2Fconfig.py?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -82,6 +82,7 @@ class ActorConfig:\\\\\\\\n     offload: OffloadConfig = field(default_factory=OffloadConfig)\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"auto keys\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     global_batch_size_per_device: int = field(default=-1, init=False)\\\\\\\\n+    report_kl: bool = field(default=False, init=False)\\\\\\\\n     use_kl_loss: bool = field(default=False, init=False)\\\\\\\\n     kl_penalty: str = field(default=\\\\\\\\\\\\\\\"kl\\\\\\\\\\\\\\\", init=False)\\\\\\\\n     kl_coef: float = field(default=0.0, init=False)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"55a1e2e0ca16dacf4e9d4d7aa1d54dfbb5cc8f26\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/actor/dp_actor.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":3,\\\\\\\"deletions\\\\\\\":5,\\\\\\\"changes\\\\\\\":8,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Fworkers%2Factor%2Fdp_actor.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Fworkers%2Factor%2Fdp_actor.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Factor%2Fdp_actor.py?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -210,7 +210,7 @@ def update_policy(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n \\\\\\\\n         temperature = data.meta_info[\\\\\\\\\\\\\\\"temperature\\\\\\\\\\\\\\\"]  # temperature must be in the data.meta_info to avoid slient error\\\\\\\\n         select_keys = [\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"old_log_probs\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"advantages\\\\\\\\\\\\\\\"]\\\\\\\\n-        if self.config.use_kl_loss and self.config.kl_coef > 1e-6:\\\\\\\\n+        if self.config.use_kl_loss and self.config.report_kl:\\\\\\\\n             select_keys.append(\\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         if \\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\" in data.non_tensor_batch.keys():\\\\\\\\n@@ -244,8 +244,6 @@ def update_policy(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n                     old_log_probs = model_inputs[\\\\\\\\\\\\\\\"old_log_probs\\\\\\\\\\\\\\\"]\\\\\\\\n                     advantages = model_inputs[\\\\\\\\\\\\\\\"advantages\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n-                    clip_ratio = self.config.clip_ratio\\\\\\\\n-\\\\\\\\n                     # all return: (bsz, response_length)\\\\\\\\n                     log_probs = self._forward_micro_batch(model_inputs, temperature=temperature)\\\\\\\\n \\\\\\\\n@@ -254,9 +252,9 @@ def update_policy(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n                         log_probs=log_probs,\\\\\\\\n                         advantages=advantages,\\\\\\\\n                         eos_mask=response_mask,\\\\\\\\n-                        cliprange=clip_ratio,\\\\\\\\n+                        cliprange=self.config.clip_ratio,\\\\\\\\n                     )\\\\\\\\n-                    if self.config.use_kl_loss and self.config.kl_coef > 1e-6:\\\\\\\\n+                    if \\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\" in model_inputs:\\\\\\\\n                         ref_log_probs = model_inputs[\\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\"]\\\\\\\\n                         # compute kl loss\\\\\\\\n                         kld = core_algos.kl_penalty(\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"cddf44f6cc1718138bcb6b1a87e77b9dedb60419\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/fsdp_workers.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":60,\\\\\\\"deletions\\\\\\\":92,\\\\\\\"changes\\\\\\\":152,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Ffsdp_workers.py?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -15,8 +15,7 @@\\\\\\\\n The main entry point to run the PPO algorithm\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n-import os\\\\\\\\n-from typing import Literal, Optional\\\\\\\\n+from typing import Literal, Optional, Union\\\\\\\\n \\\\\\\\n import numpy as np\\\\\\\\n import psutil\\\\\\\\n@@ -56,7 +55,7 @@\\\\\\\\n from ..utils.torch_dtypes import PrecisionType\\\\\\\\n from ..utils.torch_functional import get_constant_schedule_with_warmup\\\\\\\\n from .actor import DataParallelPPOActor\\\\\\\\n-from .config import FSDPConfig, ModelConfig, OptimConfig, WorkerConfig\\\\\\\\n+from .config import ActorConfig, CriticConfig, FSDPConfig, ModelConfig, OptimConfig, RefConfig, WorkerConfig\\\\\\\\n from .critic import DataParallelPPOCritic\\\\\\\\n from .rollout.vllm_rollout import vLLMRollout\\\\\\\\n from .sharding_manager import FSDPVLLMShardingManager\\\\\\\\n@@ -72,106 +71,74 @@ def __init__(\\\\\\\\n         super().__init__()\\\\\\\\n         self.config = config\\\\\\\\n         self.role = role\\\\\\\\n+\\\\\\\\n+        if not dist.is_initialized():\\\\\\\\n+            dist.init_process_group(backend=\\\\\\\\\\\\\\\"nccl\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n         self._is_actor = self.role in [\\\\\\\\\\\\\\\"actor\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"actor_rollout\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"actor_rollout_ref\\\\\\\\\\\\\\\"]\\\\\\\\n         self._is_critic = self.role == \\\\\\\\\\\\\\\"critic\\\\\\\\\\\\\\\"\\\\\\\\n         self._is_rollout = self.role in [\\\\\\\\\\\\\\\"rollout\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"actor_rollout\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"actor_rollout_ref\\\\\\\\\\\\\\\"]\\\\\\\\n         self._is_ref = self.role in [\\\\\\\\\\\\\\\"ref\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"actor_rollout_ref\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n-        if not dist.is_initialized():\\\\\\\\n-            dist.init_process_group(backend=\\\\\\\\\\\\\\\"nccl\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-        # build device mesh for FSDP\\\\\\\\n+        self._use_param_offload = False\\\\\\\\n+        self._use_optimizer_offload = False\\\\\\\\n         if self._is_actor:\\\\\\\\n-            fsdp_size = self.config.actor.fsdp.fsdp_size\\\\\\\\n+            self._use_param_offload = self.config.actor.offload.offload_params\\\\\\\\n+            self._use_optimizer_offload = self.config.actor.offload.offload_optimizer\\\\\\\\n+            self._init_config(self.config.actor, \\\\\\\\\\\\\\\"actor\\\\\\\\\\\\\\\")\\\\\\\\n         elif self._is_critic:\\\\\\\\n-            fsdp_size = self.config.critic.fsdp.fsdp_size\\\\\\\\n-        elif self._is_ref:\\\\\\\\n-            fsdp_size = self.config.ref.fsdp.fsdp_size\\\\\\\\n+            self._use_param_offload = self.config.critic.offload.offload_params\\\\\\\\n+            self._use_optimizer_offload = self.config.critic.offload.offload_optimizer\\\\\\\\n+            self._init_config(self.config.critic, \\\\\\\\\\\\\\\"critic\\\\\\\\\\\\\\\")\\\\\\\\n+        elif self._is_ref:  # NOTE: it seems that manual offload is slower than FSDP offload\\\\\\\\n+            self._use_param_offload = self.config.ref.offload.offload_params\\\\\\\\n+            self._init_config(self.config.ref, \\\\\\\\\\\\\\\"ref\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n-        local_rank = os.getenv(\\\\\\\\\\\\\\\"LOCAL_RANK\\\\\\\\\\\\\\\")\\\\\\\\n+    def _init_config(self, config: Union[ActorConfig, CriticConfig, RefConfig], role: Literal[\\\\\\\\\\\\\\\"actor\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"critic\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"ref\\\\\\\\\\\\\\\"]):\\\\\\\\n         world_size = dist.get_world_size()\\\\\\\\n-        torch.cuda.set_device(f\\\\\\\\\\\\\\\"cuda:{local_rank}\\\\\\\\\\\\\\\")\\\\\\\\n-        if fsdp_size < 0 or fsdp_size > world_size:\\\\\\\\n+        fsdp_size = config.fsdp.fsdp_size\\\\\\\\n+        if fsdp_size <= 0 or fsdp_size >= world_size:\\\\\\\\n             self.device_mesh = init_device_mesh(\\\\\\\\\\\\\\\"cuda\\\\\\\\\\\\\\\", mesh_shape=(world_size,), mesh_dim_names=(\\\\\\\\\\\\\\\"fsdp\\\\\\\\\\\\\\\",))\\\\\\\\n         else:  # hsdp\\\\\\\\n             self.device_mesh = init_device_mesh(\\\\\\\\n                 \\\\\\\\\\\\\\\"cuda\\\\\\\\\\\\\\\", mesh_shape=(world_size // fsdp_size, fsdp_size), mesh_dim_names=(\\\\\\\\\\\\\\\"ddp\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"fsdp\\\\\\\\\\\\\\\")\\\\\\\\n             )\\\\\\\\n \\\\\\\\n-        # build device mesh for Ulysses Sequence Parallel\\\\\\\\n-        self.ulysses_sequence_parallel_size = self.config.actor.ulysses_sequence_parallel_size\\\\\\\\n-        if self.ulysses_sequence_parallel_size > 1:\\\\\\\\n+        if config.ulysses_sequence_parallel_size > 1:\\\\\\\\n             self.ulysses_device_mesh = init_device_mesh(\\\\\\\\n                 \\\\\\\\\\\\\\\"cuda\\\\\\\\\\\\\\\",\\\\\\\\n-                mesh_shape=(world_size // self.ulysses_sequence_parallel_size, self.ulysses_sequence_parallel_size),\\\\\\\\n+                mesh_shape=(\\\\\\\\n+                    world_size // config.ulysses_sequence_parallel_size,\\\\\\\\n+                    config.ulysses_sequence_parallel_size,\\\\\\\\n+                ),\\\\\\\\n                 mesh_dim_names=(\\\\\\\\\\\\\\\"dp\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"sp\\\\\\\\\\\\\\\"),\\\\\\\\n             )\\\\\\\\n         else:\\\\\\\\n             self.ulysses_device_mesh = None\\\\\\\\n \\\\\\\\n         self.ulysses_sharding_manager = FSDPUlyssesShardingManager(self.ulysses_device_mesh)\\\\\\\\n \\\\\\\\n-        self._use_param_offload = False\\\\\\\\n-        self._use_optimizer_offload = False\\\\\\\\n-        if self._is_actor:\\\\\\\\n-            self._use_param_offload = self.config.actor.offload.offload_params\\\\\\\\n-            self._use_optimizer_offload = self.config.actor.offload.offload_optimizer\\\\\\\\n-        elif self._is_critic:\\\\\\\\n-            self._use_param_offload = self.config.critic.offload.offload_params\\\\\\\\n-            self._use_optimizer_offload = self.config.critic.offload.offload_optimizer\\\\\\\\n-        elif self._is_ref:  # NOTE: it seems that manual offload is slowly than FSDP offload\\\\\\\\n-            self._use_param_offload = self.config.ref.offload.offload_params\\\\\\\\n+        if not hasattr(config, \\\\\\\\\\\\\\\"global_batch_size\\\\\\\\\\\\\\\"):  # ref model\\\\\\\\n+            return\\\\\\\\n \\\\\\\\n-        # normalize config\\\\\\\\n-        if self._is_actor:\\\\\\\\n-            if self.config.rollout.n > 1:\\\\\\\\n-                self.config.actor.global_batch_size *= self.config.rollout.n\\\\\\\\n-                self.print_rank0(f\\\\\\\\\\\\\\\"Use global batch size {self.config.actor.global_batch_size}.\\\\\\\\\\\\\\\")\\\\\\\\n+        if self.config.rollout.n > 1:\\\\\\\\n+            config.global_batch_size *= self.config.rollout.n\\\\\\\\n+            self.print_rank0(f\\\\\\\\\\\\\\\"{role} will use global batch size {config.global_batch_size}.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n-            self.config.actor.global_batch_size_per_device = (\\\\\\\\n-                self.config.actor.global_batch_size // self.device_mesh.size() * self.ulysses_sequence_parallel_size\\\\\\\\n-            )\\\\\\\\n-            if self.config.actor.global_batch_size_per_device == 0:\\\\\\\\n-                raise ValueError(\\\\\\\\\\\\\\\"Global batch size must be larger than num gpus.\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-            if (\\\\\\\\n-                self.config.actor.global_batch_size_per_device\\\\\\\\n-                % self.config.actor.micro_batch_size_per_device_for_update\\\\\\\\n-                != 0\\\\\\\\n-            ):\\\\\\\\n-                raise ValueError(\\\\\\\\\\\\\\\"Global batch size per device must be divisible by the micro batch size.\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-            if (\\\\\\\\n-                self.config.actor.fsdp.enable_cpu_offload\\\\\\\\n-                and self.config.actor.global_batch_size_per_device\\\\\\\\n-                != self.config.actor.micro_batch_size_per_device_for_update\\\\\\\\n-            ):\\\\\\\\n-                raise ValueError(\\\\\\\\\\\\\\\"Cannot use FSDP's CPU offload when gradient accumulation is enabled.\\\\\\\\\\\\\\\")\\\\\\\\n+        config.global_batch_size_per_device = (\\\\\\\\n+            config.global_batch_size // self.device_mesh.size() * config.ulysses_sequence_parallel_size\\\\\\\\n+        )\\\\\\\\n+        if config.global_batch_size_per_device == 0:\\\\\\\\n+            raise ValueError(f\\\\\\\\\\\\\\\"{role} global batch size must be larger than num gpus.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n-        elif self._is_critic:\\\\\\\\n-            if self.config.rollout.n > 1:\\\\\\\\n-                self.config.critic.global_batch_size *= self.config.rollout.n\\\\\\\\n-                self.print_rank0(f\\\\\\\\\\\\\\\"Use global batch size {self.config.critic.global_batch_size}.\\\\\\\\\\\\\\\")\\\\\\\\n+        if config.global_batch_size_per_device % config.micro_batch_size_per_device_for_update != 0:\\\\\\\\n+            raise ValueError(f\\\\\\\\\\\\\\\"{role} global batch size per device must be divisible by the micro batch size.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n-            self.config.critic.global_batch_size_per_device = (\\\\\\\\n-                self.config.critic.global_batch_size // self.device_mesh.size() * self.ulysses_sequence_parallel_size\\\\\\\\n-            )\\\\\\\\n-            if self.config.critic.global_batch_size_per_device == 0:\\\\\\\\n-                raise ValueError(\\\\\\\\\\\\\\\"Global batch size must be larger than num gpus.\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-            if (\\\\\\\\n-                self.config.critic.global_batch_size_per_device\\\\\\\\n-                % self.config.critic.micro_batch_size_per_device_for_update\\\\\\\\n-                != 0\\\\\\\\n-            ):\\\\\\\\n-                raise ValueError(\\\\\\\\\\\\\\\"Global batch size per device must be divisible by the micro batch size.\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-            if (\\\\\\\\n-                self.config.critic.fsdp.enable_cpu_offload\\\\\\\\n-                and self.config.critic.global_batch_size_per_device\\\\\\\\n-                != self.config.critic.micro_batch_size_per_device_for_update\\\\\\\\n-            ):\\\\\\\\n-                raise ValueError(\\\\\\\\\\\\\\\"Cannot use FSDP's CPU offload when gradient accumulation is enabled.\\\\\\\\\\\\\\\")\\\\\\\\n+        if (\\\\\\\\n+            config.fsdp.enable_cpu_offload\\\\\\\\n+            and config.global_batch_size_per_device != config.micro_batch_size_per_device_for_update\\\\\\\\n+        ):\\\\\\\\n+            raise ValueError(f\\\\\\\\\\\\\\\"{role} cannot use FSDP's CPU offload when gradient accumulation is enabled.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n     def _build_model_optimizer(\\\\\\\\n         self,\\\\\\\\n@@ -259,9 +226,7 @@ def _build_model_optimizer(\\\\\\\\n                 self.print_rank0(\\\\\\\\\\\\\\\"No vision tower found.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         dist.barrier()\\\\\\\\n-        if self.rank == 0:\\\\\\\\n-            print_model_size(model)\\\\\\\\n-\\\\\\\\n+        print_model_size(model)\\\\\\\\n         print_gpu_memory_usage(\\\\\\\\\\\\\\\"After huggingface model init\\\\\\\\\\\\\\\")\\\\\\\\n         mixed_precision = MixedPrecision(\\\\\\\\n             param_dtype=PrecisionType.to_dtype(fsdp_config.mp_param_dtype),\\\\\\\\n@@ -321,13 +286,11 @@ def _build_model_optimizer(\\\\\\\\n             self.lr_scheduler = get_constant_schedule_with_warmup(\\\\\\\\n                 optimizer=self.optimizer, num_warmup_steps=num_warmup_steps\\\\\\\\n             )\\\\\\\\n+            print_gpu_memory_usage(\\\\\\\\\\\\\\\"After optimizer init\\\\\\\\\\\\\\\")\\\\\\\\n         else:\\\\\\\\n             self.optimizer, self.lr_scheduler = None, None\\\\\\\\n \\\\\\\\n-        print_gpu_memory_usage(\\\\\\\\\\\\\\\"After optimizer init\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n     def _build_rollout(self) -> None:\\\\\\\\n-        # TODO(sgm): support FSDP hybrid shard for larger model\\\\\\\\n         tp_size = self.config.rollout.tensor_parallel_size\\\\\\\\n         dp_size = self.world_size // tp_size\\\\\\\\n         assert self.world_size % tp_size == 0, (\\\\\\\\n@@ -353,18 +316,21 @@ def init_model(self):\\\\\\\\n             fsdp_config = self.config.critic.fsdp\\\\\\\\n             optim_config = self.config.critic.optim\\\\\\\\n             padding_free = self.config.critic.padding_free\\\\\\\\n+            role = \\\\\\\\\\\\\\\"critic\\\\\\\\\\\\\\\"\\\\\\\\n         elif self._is_actor:\\\\\\\\n             model_config = self.config.actor.model\\\\\\\\n             fsdp_config = self.config.actor.fsdp\\\\\\\\n             optim_config = self.config.actor.optim\\\\\\\\n             padding_free = self.config.actor.padding_free\\\\\\\\n+            role = \\\\\\\\\\\\\\\"actor\\\\\\\\\\\\\\\"\\\\\\\\n         elif self._is_ref:\\\\\\\\n             model_config = self.config.actor.model\\\\\\\\n             fsdp_config = self.config.ref.fsdp\\\\\\\\n             optim_config = None\\\\\\\\n             padding_free = self.config.ref.padding_free\\\\\\\\n+            role = \\\\\\\\\\\\\\\"ref\\\\\\\\\\\\\\\"\\\\\\\\n         else:\\\\\\\\n-            raise ValueError(\\\\\\\\\\\\\\\"Unknown role.\\\\\\\\\\\\\\\")\\\\\\\\n+            raise ValueError(f\\\\\\\\\\\\\\\"Unknown role {role}.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         if self._is_actor or self._is_critic or self._is_ref:\\\\\\\\n             self._build_model_optimizer(\\\\\\\\n@@ -373,11 +339,13 @@ def init_model(self):\\\\\\\\n                 optim_config=optim_config,\\\\\\\\n                 padding_free=padding_free,\\\\\\\\n             )\\\\\\\\n-            # get the original unwrapped module\\\\\\\\n-            self.unwrapped_model = self.fsdp_module._fsdp_wrapped_module\\\\\\\\n+            if self._use_param_offload:\\\\\\\\n+                offload_fsdp_model(self.fsdp_module)\\\\\\\\n+                print_gpu_memory_usage(f\\\\\\\\\\\\\\\"After offload {role} model during init\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n             if self._use_optimizer_offload:\\\\\\\\n                 offload_fsdp_optimizer(optimizer=self.optimizer)\\\\\\\\n-                print_gpu_memory_usage(\\\\\\\\\\\\\\\"After offload actor optimizer during init\\\\\\\\\\\\\\\")\\\\\\\\n+                print_gpu_memory_usage(f\\\\\\\\\\\\\\\"After offload {role} optimizer during init\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         if self._is_actor:\\\\\\\\n             self.actor = DataParallelPPOActor(\\\\\\\\n@@ -435,8 +403,6 @@ def load_checkpoint(self, path: str):\\\\\\\\n         if self._use_optimizer_offload:\\\\\\\\n             offload_fsdp_optimizer(self.optimizer)\\\\\\\\n \\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"ActorRolloutRefWorker\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-\\\\\\\\n     @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\\\\\\\\n     def update_actor(self, data: DataProto):\\\\\\\\n         assert self._is_actor\\\\\\\\n@@ -459,8 +425,12 @@ def update_actor(self, data: DataProto):\\\\\\\\n             metrics[\\\\\\\\\\\\\\\"perf/mfu/actor\\\\\\\\\\\\\\\"] = (\\\\\\\\n                 estimated_flops * self.config.actor.ppo_epochs / (promised_flops * self.world_size)\\\\\\\\n             )\\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"perf/max_memory_allocated_gb\\\\\\\\\\\\\\\"] = torch.cuda.max_memory_allocated() / (1024**3)\\\\\\\\n-            metrics[\\\\\\\\\\\\\\\"perf/max_memory_reserved_gb\\\\\\\\\\\\\\\"] = torch.cuda.max_memory_reserved() / (1024**3)\\\\\\\\n+            metrics[\\\\\\\\\\\\\\\"perf/max_memory_allocated_gb\\\\\\\\\\\\\\\"] = (\\\\\\\\n+                torch.cuda.max_memory_allocated() - self.rollout_sharding_manager.freed_bytes\\\\\\\\n+            ) / (1024**3)\\\\\\\\n+            metrics[\\\\\\\\\\\\\\\"perf/max_memory_reserved_gb\\\\\\\\\\\\\\\"] = (\\\\\\\\n+                torch.cuda.max_memory_reserved() - self.rollout_sharding_manager.freed_bytes\\\\\\\\n+            ) / (1024**3)\\\\\\\\n             metrics[\\\\\\\\\\\\\\\"perf/cpu_memory_used_gb\\\\\\\\\\\\\\\"] = psutil.virtual_memory().used / (1024**3)\\\\\\\\n \\\\\\\\n             self.lr_scheduler.step()\\\\\\\\n@@ -470,7 +440,7 @@ def update_actor(self, data: DataProto):\\\\\\\\n             # Metrics should be in non_tensor_batch instead of meta_info, as DataProto not concat meta_info.\\\\\\\\n             output = DataProto(\\\\\\\\n                 non_tensor_batch={\\\\\\\\n-                    metric: np.array([value] if np.isscalar(value) else value) for metric, value in metrics.items()\\\\\\\\n+                    key: np.array([value] if np.isscalar(value) else value) for key, value in metrics.items()\\\\\\\\n                 }\\\\\\\\n             )\\\\\\\\n \\\\\\\\n@@ -568,8 +538,6 @@ def compute_ref_log_probs(self, data: DataProto):\\\\\\\\n         output = output.to(\\\\\\\\\\\\\\\"cpu\\\\\\\\\\\\\\\")\\\\\\\\n         return output\\\\\\\\n \\\\\\\\n-    \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"CriticWorker\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-\\\\\\\\n     @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\\\\\\\\n     def compute_values(self, data: DataProto):\\\\\\\\n         assert self._is_critic\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"d5b3a9d2b9da393a0fdd1a1c960052f97c57f902\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -172,7 +172,7 @@ def generate_sequences(self, prompts: DataProto) -> DataProto:\\\\\\\\n         response_position_ids = position_ids[..., -1:] + delta_position_id\\\\\\\\n         position_ids = torch.cat([position_ids, response_position_ids], dim=-1)\\\\\\\\n         response_attention_mask = VF.get_eos_mask(\\\\\\\\n-            response_ids=response_ids, eos_token=eos_token_id, dtype=attention_mask.dtype\\\\\\\\n+            response_ids=response_ids, eos_token_id=eos_token_id, dtype=attention_mask.dtype\\\\\\\\n         )\\\\\\\\n         attention_mask = torch.cat((attention_mask, response_attention_mask), dim=-1)\\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"6202826aa3d62d7b4895d4cdebebd30ad46e2289\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/sharding_manager/fsdp_vllm.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":8,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":8,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Fworkers%2Fsharding_manager%2Ffsdp_vllm.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/26c2181ad8c7286e822216c778177a5f3a93bc14/verl%2Fworkers%2Fsharding_manager%2Ffsdp_vllm.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Fsharding_manager%2Ffsdp_vllm.py?ref=26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -52,6 +52,10 @@ def __init__(\\\\\\\\n         self.tp_rank = vllm_ps.get_tensor_model_parallel_rank()\\\\\\\\n         self.tp_group = vllm_ps.get_tensor_model_parallel_group().device_group\\\\\\\\n \\\\\\\\n+        # Record freed bytes to estimate memory usage correctly\\\\\\\\n+        # https://github.com/vllm-project/vllm/pull/11743#issuecomment-2754338119\\\\\\\\n+        self.freed_bytes = 0\\\\\\\\n+\\\\\\\\n         # Note that torch_random_states may be different on each dp rank\\\\\\\\n         self.torch_random_states = torch.cuda.get_rng_state()\\\\\\\\n         # get a random rng states\\\\\\\\n@@ -88,6 +92,7 @@ def __enter__(self):\\\\\\\\n         print_gpu_memory_usage(\\\\\\\\\\\\\\\"After sync model weights in sharding manager\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         del actor_weights\\\\\\\\n+        torch.cuda.empty_cache()\\\\\\\\n         print_gpu_memory_usage(\\\\\\\\\\\\\\\"After del state_dict and empty_cache in sharding manager\\\\\\\\\\\\\\\")\\\\\\\\n         # important: need to manually set the random states of each tp to be identical.\\\\\\\\n         if self.device_mesh is not None:\\\\\\\\n@@ -96,7 +101,10 @@ def __enter__(self):\\\\\\\\n \\\\\\\\n     def __exit__(self, exc_type, exc_value, traceback):\\\\\\\\n         print_gpu_memory_usage(\\\\\\\\\\\\\\\"Before vllm offload in sharding manager\\\\\\\\\\\\\\\")\\\\\\\\n+        free_bytes_before_sleep = torch.cuda.mem_get_info()[0]\\\\\\\\n         self.inference_engine.sleep(level=1)\\\\\\\\n+        free_bytes_after_sleep = torch.cuda.mem_get_info()[0]\\\\\\\\n+        self.freed_bytes = free_bytes_after_sleep - free_bytes_before_sleep\\\\\\\\n         print_gpu_memory_usage(\\\\\\\\\\\\\\\"After vllm offload in sharding manager\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         self.module.train()\\\\\\\"}],\\\\\\\"sha\\\\\\\":\\\\\\\"26c2181ad8c7286e822216c778177a5f3a93bc14\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"committer\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"noreply@github.com\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-26T17:13:32Z\\\\\\\"},\\\\\\\"author\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"hiyouga@buaa.edu.cn\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-26T17:13:32Z\\\\\\\"}}},\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "Perfect! Now I've found evidence of the values I'm looking for! Looking at the patch in file `examples/qwen2_5_vl_32b_geo3k_grpo.sh`, I can see:\n\n```\nworker.actor.micro_batch_size_per_device_for_update=1 \nworker.actor.micro_batch_size_per_device_for_experience=8\n```\n\nBut the task is asking specifically for changes to `examples/config.yaml`. I need to find a commit that shows the change from `micro_batch_size_per_device_for_update: 4` to `micro_batch_size_per_device_for_update: 1` in that specific file. Let me check more recent commits. Based on my earlier finding, I know that commit `6c0bba7ebc1dcec2101ca6a620db503c108c6f9e` showed the values as 4 and 16. Let me check if there's a commit after that which changed them to 1 and 2.\n\nActually, let me step back and think about this systematically. The task asks for the commit where `micro_batch_size_per_device_for_update` was changed from 4 to 1, and `micro_batch_size_per_device_for_experience` was changed. Let me search for the newer commits that haven't been examined yet, since the commit I found showed them as 4 and 16, and I need to find when they changed to 1 and 2 (or other values).",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"ref\\\": \\\"ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\"}\", \"include_output_fields\": [\"sha\", \"commit.author\", \"commit.committer\", \"files\"]}",
    "call_id": "toolu_01J3DumMdbyAQEycuD6uyVTU",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01J3DumMdbyAQEycuD6uyVTU",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"files\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"c738f4b4a49aec9827958938486f1f7b6759af34\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"README.md\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":9,\\\\\\\"deletions\\\\\\\":6,\\\\\\\"changes\\\\\\\":15,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/README.md\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/README.md\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/README.md?ref=ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -17,7 +17,7 @@ EasyR1 is efficient and scalable due to the design of **[HybirdEngine](https://a\\\\\\\\n - Supported algorithms\\\\\\\\n   - GRPO\\\\\\\\n   - Reinforce++\\\\\\\\n-  - Remax\\\\\\\\n+  - ReMax\\\\\\\\n   - RLOO\\\\\\\\n \\\\\\\\n - Supported datasets\\\\\\\\n@@ -26,7 +26,7 @@ EasyR1 is efficient and scalable due to the design of **[HybirdEngine](https://a\\\\\\\\n - Supported tricks\\\\\\\\n   - Padding-free training\\\\\\\\n   - Resuming from checkpoint\\\\\\\\n-  - Wandb & SwanLab tracking\\\\\\\\n+  - Wandb & SwanLab & Mlflow & Tensorboard tracking\\\\\\\\n \\\\\\\\n ## Requirements\\\\\\\\n \\\\\\\\n@@ -52,11 +52,14 @@ docker pull hiyouga/verl:ngc-th2.6.0-cu120-vllm0.8.2\\\\\\\\n \\\\\\\\n \\\\\\\\\\\\\\\\* *estimated*\\\\\\\\n \\\\\\\\n-| Method                   | Bits |  1.5B  |   3B   |   7B   |   32B  |\\\\\\\\n-| ------------------------ | ---- | ------ | ------ | ------ | ------ |\\\\\\\\n-| GRPO Full Fine-Tuning    |  AMP | 2*24GB | 4*40GB | 8*40GB | 8*80GB |\\\\\\\\n+| Method                   | Bits |  1.5B  |   3B   |   7B   |   32B   |\\\\\\\\n+| ------------------------ | ---- | ------ | ------ | ------ | ------- |\\\\\\\\n+| GRPO Full Fine-Tuning    |  AMP | 2*24GB | 4*40GB | 8*40GB | 16*80GB |\\\\\\\\n+| GRPO Full Fine-Tuning    | BF16 | 1*24GB | 1*40GB | 4*40GB |  8*80GB |\\\\\\\\n \\\\\\\\n > [!NOTE]\\\\\\\\n+> Use `worker.actor.fsdp.torch_dtype=bf16` and `worker.actor.optim.strategy=adamw_bf16` to enable bf16 training.\\\\\\\\n+>\\\\\\\\n > We are working hard to reduce the VRAM in RL training, LoRA support will be integrated in next updates.\\\\\\\\n \\\\\\\\n ## Tutorial: Run Qwen2.5-VL GRPO on [Geometry3K](https://huggingface.co/datasets/hiyouga/geometry3k) Dataset in Just 3 Steps\\\\\\\\n@@ -80,7 +83,7 @@ bash examples/qwen2_5_vl_7b_geo3k_grpo.sh\\\\\\\\n ### Merge Checkpoint in Hugging Face Format\\\\\\\\n \\\\\\\\n ```bash\\\\\\\\n-python3 scripts/model_merger.py --local_dir path_to_your_last_actor_checkpoint\\\\\\\\n+python3 scripts/model_merger.py --local_dir checkpoints/easy_r1/exp_name/global_step_1/actor\\\\\\\\n ```\\\\\\\\n \\\\\\\\n > [!TIP]\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"f177da7a7244e8a42262db1033032ed30b6d1ac7\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/config.yaml\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":4,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":7,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fconfig.yaml?ref=ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -36,14 +36,15 @@ worker:\\\\\\\\n     optim:\\\\\\\\n       lr: 1.0e-6\\\\\\\\n       weight_decay: 1.0e-2\\\\\\\\n+      strategy: adamw  # {adamw, adamw_bf16}\\\\\\\\n       lr_warmup_ratio: 0.0\\\\\\\\n     fsdp:\\\\\\\\n       enable_full_shard: true\\\\\\\\n       enable_cpu_offload: false\\\\\\\\n       enable_rank0_init: true\\\\\\\\n     offload:\\\\\\\\n-      offload_params: true\\\\\\\\n-      offload_optimizer: true\\\\\\\\n+      offload_params: true  # true: more CPU memory; false: more GPU memory\\\\\\\\n+      offload_optimizer: true  # true: more CPU memory; false: more GPU memory\\\\\\\\n \\\\\\\\n   rollout:\\\\\\\\n     temperature: 1.0\\\\\\\\n@@ -60,7 +61,7 @@ worker:\\\\\\\\n   ref:\\\\\\\\n     fsdp:\\\\\\\\n       enable_full_shard: true\\\\\\\\n-      enable_cpu_offload: true\\\\\\\\n+      enable_cpu_offload: true  # true: more CPU memory; false: more GPU memory\\\\\\\\n       enable_rank0_init: true\\\\\\\\n     offload:\\\\\\\\n       offload_params: false\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"682f78bfb88c028d360bd5bfdab6e4e561e0a559\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh?ref=ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -14,6 +14,7 @@ python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.micro_batch_size_per_device_for_update=1 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.micro_batch_size_per_device_for_experience=8 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.fsdp.torch_dtype=bf16 \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.actor.optim.strategy=adamw_bf16 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.tensor_parallel_size=8 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.enable_chunked_prefill=false \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.experiment_name=qwen2_5_vl_32b_geo_grpo \\\\\\\\\\\\\\\\\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"285f4ade221f7d47144ced9df61d8644f5d7e822\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/metrics.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":6,\\\\\\\"deletions\\\\\\\":23,\\\\\\\"changes\\\\\\\":29,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Ftrainer%2Fmetrics.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Ftrainer%2Fmetrics.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fmetrics.py?ref=ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -20,19 +20,6 @@\\\\\\\\n from ..protocol import DataProto\\\\\\\\n \\\\\\\\n \\\\\\\\n-def _compute_response_info(batch: DataProto) -> Dict[str, Any]:\\\\\\\\n-    response_length = batch.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].shape[-1]\\\\\\\\n-    prompt_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, :-response_length]\\\\\\\\n-    response_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -response_length:]\\\\\\\\n-    prompt_length = prompt_mask.sum(-1).float()\\\\\\\\n-    response_length = response_mask.sum(-1).float()  # (batch_size,)\\\\\\\\n-    return dict(\\\\\\\\n-        response_mask=response_mask,\\\\\\\\n-        prompt_length=prompt_length,\\\\\\\\n-        response_length=response_length,\\\\\\\\n-    )\\\\\\\\n-\\\\\\\\n-\\\\\\\\n def reduce_metrics(metrics: Dict[str, List[Any]]) -> Dict[str, Any]:\\\\\\\\n     return {key: np.mean(value) for key, value in metrics.items()}\\\\\\\\n \\\\\\\\n@@ -46,14 +33,12 @@ def compute_data_metrics(batch: DataProto, use_critic: bool = False) -> Dict[str\\\\\\\\n \\\\\\\\n     max_response_length = batch.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"].size(-1)\\\\\\\\n \\\\\\\\n-    prompt_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, :-max_response_length].bool()\\\\\\\\n-    response_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -max_response_length:].bool()\\\\\\\\n+    prompt_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, :-max_response_length]\\\\\\\\n+    response_mask = batch.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"][:, -max_response_length:]\\\\\\\\n \\\\\\\\n     max_prompt_length = prompt_mask.size(-1)\\\\\\\\n-\\\\\\\\n-    response_info = _compute_response_info(batch)\\\\\\\\n-    prompt_length = response_info[\\\\\\\\\\\\\\\"prompt_length\\\\\\\\\\\\\\\"]\\\\\\\\n-    response_length = response_info[\\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\"]\\\\\\\\n+    prompt_length = prompt_mask.sum(-1).float()\\\\\\\\n+    response_length = response_mask.sum(-1).float()\\\\\\\\n \\\\\\\\n     valid_adv = torch.masked_select(advantages, response_mask)\\\\\\\\n     valid_returns = torch.masked_select(returns, response_mask)\\\\\\\\n@@ -110,10 +95,8 @@ def compute_data_metrics(batch: DataProto, use_critic: bool = False) -> Dict[str\\\\\\\\n \\\\\\\\n \\\\\\\\n def compute_timing_metrics(batch: DataProto, timing_raw: Dict[str, float]) -> Dict[str, Any]:\\\\\\\\n-    response_info = _compute_response_info(batch)\\\\\\\\n-    num_prompt_tokens = torch.sum(response_info[\\\\\\\\\\\\\\\"prompt_length\\\\\\\\\\\\\\\"]).item()\\\\\\\\n-    num_response_tokens = torch.sum(response_info[\\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\"]).item()\\\\\\\\n-    num_overall_tokens = num_prompt_tokens + num_response_tokens\\\\\\\\n+    num_response_tokens = torch.sum(batch.batch[\\\\\\\\\\\\\\\"response_mask\\\\\\\\\\\\\\\"]).item()\\\\\\\\n+    num_overall_tokens = sum(batch.meta_info[\\\\\\\\\\\\\\\"global_token_num\\\\\\\\\\\\\\\"])\\\\\\\\n     num_tokens_of_section = {\\\\\\\\n         **dict.fromkeys([\\\\\\\\\\\\\\\"gen\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"reward\\\\\\\\\\\\\\\"], num_response_tokens),\\\\\\\\n         **dict.fromkeys([\\\\\\\\\\\\\\\"ref\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"old\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"values\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"adv\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"update_critic\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"update_actor\\\\\\\\\\\\\\\"], num_overall_tokens),\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"135bfc246c619e5803e12770b43d42fbbb5976a2\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/ray_trainer.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":8,\\\\\\\"changes\\\\\\\":10,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fray_trainer.py?ref=ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -126,12 +126,9 @@ def _check_resource_available(self):\\\\\\\\n \\\\\\\\n \\\\\\\\n def apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.KLController, kl_penalty=\\\\\\\\\\\\\\\"kl\\\\\\\\\\\\\\\"):\\\\\\\\n-    responses = data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-    response_length = responses.size(1)\\\\\\\\n     token_level_scores = data.batch[\\\\\\\\\\\\\\\"token_level_scores\\\\\\\\\\\\\\\"]\\\\\\\\n     batch_size = data.batch.batch_size[0]\\\\\\\\n-    attention_mask = data.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n-    response_mask = attention_mask[:, -response_length:]\\\\\\\\n+    response_mask = data.batch[\\\\\\\\\\\\\\\"response_mask\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n     # compute kl between ref_policy and current policy\\\\\\\\n     if \\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\" in data.batch.keys():\\\\\\\\n@@ -158,11 +155,8 @@ def apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.KLController, kl_penal\\\\\\\\n \\\\\\\\n \\\\\\\\n def compute_advantage(data: DataProto, adv_estimator: AdvantageEstimator, gamma: float = 1.0, lam: float = 1.0):\\\\\\\\n-    responses = data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n-    response_length = responses.size(-1)\\\\\\\\n-    attention_mask = data.batch[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n-    response_mask = attention_mask[:, -response_length:]\\\\\\\\n     token_level_rewards = data.batch[\\\\\\\\\\\\\\\"token_level_rewards\\\\\\\\\\\\\\\"]\\\\\\\\n+    response_mask = data.batch[\\\\\\\\\\\\\\\"response_mask\\\\\\\\\\\\\\\"]\\\\\\\\n     index = data.non_tensor_batch[\\\\\\\\\\\\\\\"uid\\\\\\\\\\\\\\\"]\\\\\\\\n     if adv_estimator == AdvantageEstimator.GAE:\\\\\\\\n         values = data.batch[\\\\\\\\\\\\\\\"values\\\\\\\\\\\\\\\"]\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"8f11d0ae0f4b18d4b8202289a5f15953da4d69e0\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/py_functional.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":16,\\\\\\\"deletions\\\\\\\":4,\\\\\\\"changes\\\\\\\":20,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Futils%2Fpy_functional.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Futils%2Fpy_functional.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fpy_functional.py?ref=ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -16,6 +16,7 @@\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n import importlib.util\\\\\\\\n+import re\\\\\\\\n from functools import lru_cache\\\\\\\\n from typing import Any, Dict, List, Union\\\\\\\\n \\\\\\\\n@@ -24,13 +25,24 @@\\\\\\\\n from yaml import Dumper\\\\\\\\n \\\\\\\\n \\\\\\\\n-def numpy_representer(dumper: Dumper, value: Union[np.float32, np.float64]):\\\\\\\\n-    value = str(round(value, 3))\\\\\\\\n+def is_sci_notation(number: float) -> bool:\\\\\\\\n+    pattern = re.compile(r\\\\\\\\\\\\\\\"^[+-]?\\\\\\\\\\\\\\\\d+(\\\\\\\\\\\\\\\\.\\\\\\\\\\\\\\\\d*)?[eE][+-]?\\\\\\\\\\\\\\\\d+$\\\\\\\\\\\\\\\")\\\\\\\\n+    return bool(pattern.match(str(number)))\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+def float_representer(dumper: Dumper, value: Union[float, np.float32, np.float64]):\\\\\\\\n+    if is_sci_notation(value):\\\\\\\\n+        if \\\\\\\\\\\\\\\".\\\\\\\\\\\\\\\" not in value and \\\\\\\\\\\\\\\"e\\\\\\\\\\\\\\\" in value:\\\\\\\\n+            value = str(value).replace(\\\\\\\\\\\\\\\"e\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\".0e\\\\\\\\\\\\\\\", 1)\\\\\\\\n+    else:\\\\\\\\n+        value = str(round(value, 3))\\\\\\\\n+\\\\\\\\n     return dumper.represent_scalar(\\\\\\\\\\\\\\\"tag:yaml.org,2002:float\\\\\\\\\\\\\\\", value)\\\\\\\\n \\\\\\\\n \\\\\\\\n-yaml.add_representer(np.float32, numpy_representer)\\\\\\\\n-yaml.add_representer(np.float64, numpy_representer)\\\\\\\\n+yaml.add_representer(float, float_representer)\\\\\\\\n+yaml.add_representer(np.float32, float_representer)\\\\\\\\n+yaml.add_representer(np.float64, float_representer)\\\\\\\\n \\\\\\\\n \\\\\\\\n @lru_cache\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"6be71127087bf0992116e14867952899f58aaa62\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/torch_functional.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":133,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":134,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Futils%2Ftorch_functional.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Futils%2Ftorch_functional.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Ftorch_functional.py?ref=ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -15,7 +15,7 @@\\\\\\\\n Contain small torch utilities\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n-from typing import List, Literal, Optional, Union\\\\\\\\n+from typing import List, Literal, Optional, Tuple, Union\\\\\\\\n \\\\\\\\n import torch\\\\\\\\n import torch.distributed\\\\\\\\n@@ -193,3 +193,135 @@ def lr_lambda(current_step: int) -> float:\\\\\\\\n         return min(1.0, float(current_step) / float(max(1, num_warmup_steps)))\\\\\\\\n \\\\\\\\n     return LambdaLR(optimizer, lr_lambda, last_epoch)\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+# https://github.com/meta-llama/llama-cookbook/blob/v0.0.5/src/llama_cookbook/policies/anyprecision_optimizer.py\\\\\\\\n+class AnyPrecisionAdamW(torch.optim.Optimizer):\\\\\\\\n+    def __init__(\\\\\\\\n+        self,\\\\\\\\n+        params: List[torch.Tensor],\\\\\\\\n+        lr: float = 1e-3,\\\\\\\\n+        betas: Tuple[float, float] = (0.9, 0.999),\\\\\\\\n+        eps: float = 1e-8,\\\\\\\\n+        weight_decay: float = 0.0,\\\\\\\\n+        use_kahan_summation: bool = True,\\\\\\\\n+        momentum_dtype: torch.dtype = torch.bfloat16,\\\\\\\\n+        variance_dtype: torch.dtype = torch.bfloat16,\\\\\\\\n+        compensation_buffer_dtype: torch.dtype = torch.bfloat16,\\\\\\\\n+    ):\\\\\\\\n+        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+        Args:\\\\\\\\n+                params (iterable): iterable of parameters to optimize or dicts defining parameter groups\\\\\\\\n+                lr (float, optional): learning rate (default: 1e-3)\\\\\\\\n+                betas (Tuple[float, float], optional): coefficients used for computing\\\\\\\\n+                    running averages of gradient and its square (default: (0.9, 0.999))\\\\\\\\n+                eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-8)\\\\\\\\n+                weight_decay (float, optional): weight decay coefficient (default: 1e-2)\\\\\\\\n+\\\\\\\\n+                # Any Precision specific\\\\\\\\n+                use_kahan_summation = creates auxiliary buffer to ensure high precision\\\\\\\\n+                model param updates (default: False)\\\\\\\\n+                momentum_dtype = dtype for momentum  (default: bfloat16)\\\\\\\\n+                variance_dtype = dtype for uncentered variance (default: bfloat16)\\\\\\\\n+                compensation_buffer_dtype  = dtype for Kahan summation buffer (default: bfloat16)\\\\\\\\n+\\\\\\\\n+                # Usage\\\\\\\\n+                This optimizer implements optimizer states, and Kahan summation\\\\\\\\n+                for high precision updates, all in user controlled dtypes.\\\\\\\\n+                Defaults are variance in BF16, Momentum in FP32.\\\\\\\\n+                This can be run in FSDP mixed precision, amp, or full precision,\\\\\\\\n+                depending on what training pipeline you wish to work with.\\\\\\\\n+\\\\\\\\n+                Setting to use_kahan_summation = False, and changing momentum and\\\\\\\\n+                variance dtypes to FP32, reverts this to a standard AdamW optimizer.\\\\\\\\n+\\\\\\\\n+        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+        defaults = {\\\\\\\\n+            \\\\\\\\\\\\\\\"lr\\\\\\\\\\\\\\\": lr,\\\\\\\\n+            \\\\\\\\\\\\\\\"betas\\\\\\\\\\\\\\\": betas,\\\\\\\\n+            \\\\\\\\\\\\\\\"eps\\\\\\\\\\\\\\\": eps,\\\\\\\\n+            \\\\\\\\\\\\\\\"weight_decay\\\\\\\\\\\\\\\": weight_decay,\\\\\\\\n+            \\\\\\\\\\\\\\\"use_kahan_summation\\\\\\\\\\\\\\\": use_kahan_summation,\\\\\\\\n+            \\\\\\\\\\\\\\\"momentum_dtype\\\\\\\\\\\\\\\": momentum_dtype,\\\\\\\\n+            \\\\\\\\\\\\\\\"variance_dtype\\\\\\\\\\\\\\\": variance_dtype,\\\\\\\\n+            \\\\\\\\\\\\\\\"compensation_buffer_dtype\\\\\\\\\\\\\\\": compensation_buffer_dtype,\\\\\\\\n+        }\\\\\\\\n+        super().__init__(params, defaults)\\\\\\\\n+\\\\\\\\n+    @torch.no_grad()\\\\\\\\n+    def step(self, closure=None):\\\\\\\\n+        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+        Performs a single optimization step.\\\\\\\\n+\\\\\\\\n+        Args:\\\\\\\\n+            closure (callable, optional): A closure that reevaluates the model and returns the loss.\\\\\\\\n+        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+\\\\\\\\n+        if closure is not None:\\\\\\\\n+            with torch.enable_grad():\\\\\\\\n+                closure()\\\\\\\\n+\\\\\\\\n+        for group in self.param_groups:\\\\\\\\n+            beta1, beta2 = group[\\\\\\\\\\\\\\\"betas\\\\\\\\\\\\\\\"]\\\\\\\\n+            lr = group[\\\\\\\\\\\\\\\"lr\\\\\\\\\\\\\\\"]\\\\\\\\n+            weight_decay = group[\\\\\\\\\\\\\\\"weight_decay\\\\\\\\\\\\\\\"]\\\\\\\\n+            eps = group[\\\\\\\\\\\\\\\"eps\\\\\\\\\\\\\\\"]\\\\\\\\n+            use_kahan_summation = group[\\\\\\\\\\\\\\\"use_kahan_summation\\\\\\\\\\\\\\\"]\\\\\\\\n+\\\\\\\\n+            momentum_dtype = group[\\\\\\\\\\\\\\\"momentum_dtype\\\\\\\\\\\\\\\"]\\\\\\\\n+            variance_dtype = group[\\\\\\\\\\\\\\\"variance_dtype\\\\\\\\\\\\\\\"]\\\\\\\\n+            compensation_buffer_dtype = group[\\\\\\\\\\\\\\\"compensation_buffer_dtype\\\\\\\\\\\\\\\"]\\\\\\\\n+            for p in group[\\\\\\\\\\\\\\\"params\\\\\\\\\\\\\\\"]:\\\\\\\\n+                if p.grad is None:\\\\\\\\n+                    continue\\\\\\\\n+\\\\\\\\n+                if p.grad.is_sparse:\\\\\\\\n+                    raise RuntimeError(\\\\\\\\\\\\\\\"AnyPrecisionAdamW does not support sparse gradients.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+                state = self.state[p]\\\\\\\\n+                # State initialization\\\\\\\\n+                if len(state) == 0:\\\\\\\\n+                    state[\\\\\\\\\\\\\\\"step\\\\\\\\\\\\\\\"] = torch.tensor(0.0)\\\\\\\\n+\\\\\\\\n+                    # momentum - EMA of gradient values\\\\\\\\n+                    state[\\\\\\\\\\\\\\\"exp_avg\\\\\\\\\\\\\\\"] = torch.zeros_like(p, dtype=momentum_dtype)\\\\\\\\n+\\\\\\\\n+                    # variance uncentered - EMA of squared gradient values\\\\\\\\n+                    state[\\\\\\\\\\\\\\\"exp_avg_sq\\\\\\\\\\\\\\\"] = torch.zeros_like(p, dtype=variance_dtype)\\\\\\\\n+\\\\\\\\n+                    # optional Kahan summation - accumulated error tracker\\\\\\\\n+                    if use_kahan_summation:\\\\\\\\n+                        state[\\\\\\\\\\\\\\\"compensation\\\\\\\\\\\\\\\"] = torch.zeros_like(p, dtype=compensation_buffer_dtype)\\\\\\\\n+\\\\\\\\n+                # Main processing\\\\\\\\n+                # update the steps for each param group update\\\\\\\\n+                state[\\\\\\\\\\\\\\\"step\\\\\\\\\\\\\\\"] += 1\\\\\\\\n+                step = state[\\\\\\\\\\\\\\\"step\\\\\\\\\\\\\\\"]\\\\\\\\n+\\\\\\\\n+                exp_avg = state[\\\\\\\\\\\\\\\"exp_avg\\\\\\\\\\\\\\\"]\\\\\\\\n+                exp_avg_sq = state[\\\\\\\\\\\\\\\"exp_avg_sq\\\\\\\\\\\\\\\"]\\\\\\\\n+                grad = p.grad\\\\\\\\n+\\\\\\\\n+                if weight_decay:  # weight decay, AdamW style\\\\\\\\n+                    p.data.mul_(1 - lr * weight_decay)\\\\\\\\n+\\\\\\\\n+                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # update momentum\\\\\\\\n+                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)  # update uncentered variance\\\\\\\\n+\\\\\\\\n+                bias_correction1 = 1 - beta1**step  # adjust using bias1\\\\\\\\n+                step_size = lr / bias_correction1\\\\\\\\n+\\\\\\\\n+                denom_correction = (1 - beta2**step) ** 0.5  # adjust using bias2 and avoids math import\\\\\\\\n+                centered_variance = (exp_avg_sq.sqrt() / denom_correction).add_(eps, alpha=1)\\\\\\\\n+\\\\\\\\n+                if use_kahan_summation:  # lr update to compensation\\\\\\\\n+                    compensation = state[\\\\\\\\\\\\\\\"compensation\\\\\\\\\\\\\\\"]\\\\\\\\n+                    compensation.addcdiv_(exp_avg, centered_variance, value=-step_size)\\\\\\\\n+\\\\\\\\n+                    # update weights with compensation (Kahan summation)\\\\\\\\n+                    # save error back to compensation for next iteration\\\\\\\\n+                    temp_buffer = p.detach().clone()\\\\\\\\n+                    p.data.add_(compensation)\\\\\\\\n+                    compensation.add_(temp_buffer.sub_(p.data))\\\\\\\\n+                else:  # usual AdamW updates\\\\\\\\n+                    p.data.addcdiv_(exp_avg, centered_variance, value=-step_size)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"1f367fc9370af0d7cb3ac0f69d1c31f045819b35\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/actor/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Fworkers%2Factor%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Fworkers%2Factor%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Factor%2Fconfig.py?ref=ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -38,6 +38,7 @@ class OptimConfig:\\\\\\\\n     lr: float = 1e-6\\\\\\\\n     betas: Tuple[float, float] = (0.9, 0.999)\\\\\\\\n     weight_decay: float = 1e-2\\\\\\\\n+    strategy: str = \\\\\\\\\\\\\\\"adamw\\\\\\\\\\\\\\\"\\\\\\\\n     lr_warmup_ratio: float = 0.0\\\\\\\\n     min_lr_ratio: Optional[float] = None\\\\\\\\n     warmup_style: str = \\\\\\\\\\\\\\\"constant\\\\\\\\\\\\\\\"\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"e27f965112974b87ccf87b4d66e712821b940d9c\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/actor/dp_actor.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":5,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":6,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Fworkers%2Factor%2Fdp_actor.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Fworkers%2Factor%2Fdp_actor.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Factor%2Fdp_actor.py?ref=ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -158,7 +158,11 @@ def _optimizer_step(self) -> torch.Tensor:\\\\\\\\n         else:\\\\\\\\n             grad_norm = nn.utils.clip_grad_norm_(self.actor_module.parameters(), max_norm=self.config.max_grad_norm)\\\\\\\\n \\\\\\\\n-        self.actor_optimizer.step()\\\\\\\\n+        if not torch.isfinite(grad_norm):\\\\\\\\n+            print(\\\\\\\\\\\\\\\"Gradient norm is not finite. Skip update.\\\\\\\\\\\\\\\")\\\\\\\\n+        else:\\\\\\\\n+            self.actor_optimizer.step()\\\\\\\\n+\\\\\\\\n         self.actor_optimizer.zero_grad()\\\\\\\\n         return grad_norm\\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"0b754e4e6094ffd9946c280fa55e14567af49d0d\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/critic/dp_critic.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":22,\\\\\\\"deletions\\\\\\\":11,\\\\\\\"changes\\\\\\\":33,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Fworkers%2Fcritic%2Fdp_critic.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Fworkers%2Fcritic%2Fdp_critic.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Fcritic%2Fdp_critic.py?ref=ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -20,9 +20,9 @@\\\\\\\\n from typing import Any, Dict\\\\\\\\n \\\\\\\\n import torch\\\\\\\\n+from ray.experimental.tqdm_ray import tqdm\\\\\\\\n from torch import nn\\\\\\\\n from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\\\\\\\\n-from tqdm import tqdm\\\\\\\\n \\\\\\\\n from ...protocol import DataProto\\\\\\\\n from ...trainer import core_algos\\\\\\\\n@@ -129,7 +129,12 @@ def _optimizer_step(self) -> torch.Tensor:\\\\\\\\n                 self.critic_module.parameters(), max_norm=self.config.max_grad_norm\\\\\\\\n             )\\\\\\\\n \\\\\\\\n-        self.critic_optimizer.step()\\\\\\\\n+        if not torch.isfinite(grad_norm):\\\\\\\\n+            print(\\\\\\\\\\\\\\\"Gradient norm is not finite. Skip update.\\\\\\\\\\\\\\\")\\\\\\\\n+        else:\\\\\\\\n+            self.critic_optimizer.step()\\\\\\\\n+\\\\\\\\n+        self.critic_optimizer.zero_grad()\\\\\\\\n         return grad_norm\\\\\\\\n \\\\\\\\n     @torch.no_grad()\\\\\\\\n@@ -146,8 +151,12 @@ def compute_values(self, data: DataProto) -> torch.Tensor:\\\\\\\\n             self.config.micro_batch_size_per_device_for_experience\\\\\\\\n         )\\\\\\\\n         values_lst = []\\\\\\\\n-        for micro_batch in tqdm(micro_batches, \\\\\\\\\\\\\\\"Compute values\\\\\\\\\\\\\\\", disable=(self.rank != 0)):\\\\\\\\n-            values = self._forward_micro_batch(micro_batch)\\\\\\\\n+        if self.rank == 0:\\\\\\\\n+            micro_batches = tqdm(micro_batches, desc=\\\\\\\\\\\\\\\"Compute values\\\\\\\\\\\\\\\", position=2)\\\\\\\\n+\\\\\\\\n+        for micro_batch in micro_batches:\\\\\\\\n+            model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}\\\\\\\\n+            values = self._forward_micro_batch(model_inputs)\\\\\\\\n             values_lst.append(values)\\\\\\\\n \\\\\\\\n         values = torch.concat(values_lst, dim=0)\\\\\\\\n@@ -171,25 +180,28 @@ def update_critic(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n         mini_batches = data.select(select_keys, non_tensor_select_keys).split(self.config.global_batch_size_per_device)\\\\\\\\n \\\\\\\\n         metrics = defaultdict(list)\\\\\\\\n-        n = len(mini_batches)\\\\\\\\n         for _ in range(self.config.ppo_epochs):\\\\\\\\n-            for i, mini_batch in enumerate(mini_batches):\\\\\\\\n+            if self.rank == 0:\\\\\\\\n+                mini_batches = tqdm(mini_batches, desc=\\\\\\\\\\\\\\\"Train mini-batches\\\\\\\\\\\\\\\", position=2)\\\\\\\\n+\\\\\\\\n+            for mini_batch in mini_batches:\\\\\\\\n                 gradient_accumulation = (\\\\\\\\n                     self.config.global_batch_size_per_device // self.config.micro_batch_size_per_device_for_update\\\\\\\\n                 )\\\\\\\\n                 micro_batches = mini_batch.split(self.config.micro_batch_size_per_device_for_update)\\\\\\\\n+                if self.rank == 0:\\\\\\\\n+                    micro_batches = tqdm(micro_batches, desc=\\\\\\\\\\\\\\\"Update critic\\\\\\\\\\\\\\\", position=3)\\\\\\\\n \\\\\\\\n-                self.critic_optimizer.zero_grad()\\\\\\\\n-                for micro_batch in tqdm(micro_batches, desc=f\\\\\\\\\\\\\\\"Update critic [{i + 1}/{n}]\\\\\\\\\\\\\\\", disable=(self.rank != 0)):\\\\\\\\n+                for micro_batch in micro_batches:\\\\\\\\n                     model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}\\\\\\\\n                     responses = model_inputs[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n                     attention_mask = model_inputs[\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\"]\\\\\\\\n                     values = model_inputs[\\\\\\\\\\\\\\\"values\\\\\\\\\\\\\\\"]\\\\\\\\n                     returns = model_inputs[\\\\\\\\\\\\\\\"returns\\\\\\\\\\\\\\\"]\\\\\\\\n                     response_length = responses.size(1)\\\\\\\\n-                    eos_mask = attention_mask[:, -response_length - 1 : -1]\\\\\\\\n+                    eos_mask = attention_mask[:, -response_length - 1 : -1]  # shift left for value computation\\\\\\\\n \\\\\\\\n-                    vpreds = self._forward_micro_batch(data)\\\\\\\\n+                    vpreds = self._forward_micro_batch(model_inputs)\\\\\\\\n                     vf_loss, vf_clipfrac = core_algos.compute_value_loss(\\\\\\\\n                         vpreds=vpreds,\\\\\\\\n                         returns=returns,\\\\\\\\n@@ -210,5 +222,4 @@ def update_critic(self, data: DataProto) -> Dict[str, Any]:\\\\\\\\n                 grad_norm = self._optimizer_step()\\\\\\\\n                 append_to_dict(metrics, {\\\\\\\\\\\\\\\"critic/grad_norm\\\\\\\\\\\\\\\": grad_norm.detach().item()})\\\\\\\\n \\\\\\\\n-        self.critic_optimizer.zero_grad()\\\\\\\\n         return metrics\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"a1651bb9390003c3fb323d1c15c8145d3d13eff7\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/fsdp_workers.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":19,\\\\\\\"deletions\\\\\\\":8,\\\\\\\"changes\\\\\\\":27,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Ffsdp_workers.py?ref=ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -53,7 +53,7 @@\\\\\\\\n from ..utils.model_utils import print_gpu_memory_usage, print_model_size\\\\\\\\n from ..utils.tokenizer import get_processor, get_tokenizer\\\\\\\\n from ..utils.torch_dtypes import PrecisionType\\\\\\\\n-from ..utils.torch_functional import get_constant_schedule_with_warmup\\\\\\\\n+from ..utils.torch_functional import AnyPrecisionAdamW, get_constant_schedule_with_warmup\\\\\\\\n from .actor import DataParallelPPOActor\\\\\\\\n from .config import ActorConfig, CriticConfig, FSDPConfig, ModelConfig, OptimConfig, RefConfig, WorkerConfig\\\\\\\\n from .critic import DataParallelPPOCritic\\\\\\\\n@@ -277,13 +277,24 @@ def _build_model_optimizer(\\\\\\\\n         print_gpu_memory_usage(\\\\\\\\\\\\\\\"After FSDP module init\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         if self._is_actor or self._is_critic:\\\\\\\\n-            self.optimizer = torch.optim.AdamW(\\\\\\\\n-                self.fsdp_module.parameters(),\\\\\\\\n-                lr=optim_config.lr,\\\\\\\\n-                betas=optim_config.betas,\\\\\\\\n-                weight_decay=optim_config.weight_decay,\\\\\\\\n-                fused=True,\\\\\\\\n-            )\\\\\\\\n+            if optim_config.strategy == \\\\\\\\\\\\\\\"adamw\\\\\\\\\\\\\\\":\\\\\\\\n+                self.optimizer = torch.optim.AdamW(\\\\\\\\n+                    self.fsdp_module.parameters(),\\\\\\\\n+                    lr=optim_config.lr,\\\\\\\\n+                    betas=optim_config.betas,\\\\\\\\n+                    weight_decay=optim_config.weight_decay,\\\\\\\\n+                    fused=True,\\\\\\\\n+                )\\\\\\\\n+            elif optim_config.strategy == \\\\\\\\\\\\\\\"adamw_bf16\\\\\\\\\\\\\\\":\\\\\\\\n+                self.optimizer = AnyPrecisionAdamW(\\\\\\\\n+                    self.fsdp_module.parameters(),\\\\\\\\n+                    lr=optim_config.lr,\\\\\\\\n+                    betas=optim_config.betas,\\\\\\\\n+                    weight_decay=optim_config.weight_decay,\\\\\\\\n+                )\\\\\\\\n+            else:\\\\\\\\n+                raise NotImplementedError(f\\\\\\\\\\\\\\\"Optimizer {optim_config.strategy} not supported.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n             num_warmup_steps = int(optim_config.lr_warmup_ratio * optim_config.training_steps)\\\\\\\\n             self.lr_scheduler = get_constant_schedule_with_warmup(\\\\\\\\n                 optimizer=self.optimizer, num_warmup_steps=num_warmup_steps\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"8916bbb656fb471d05bb8674df8c53eee1b2ecb7\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":3,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":5,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/ac0bc0494ad6b39fe4c1773aff0de5d6663d83af/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Frollout%2Fvllm_rollout%2Fvllm_rollout_spmd.py?ref=ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -171,10 +171,10 @@ def generate_sequences(self, prompts: DataProto) -> DataProto:\\\\\\\\n         # position_ids:   [0,0,0,0,0,1,2,3 | 4,5,6,7,8,9,10,11]\\\\\\\\n         response_position_ids = position_ids[..., -1:] + delta_position_id\\\\\\\\n         position_ids = torch.cat([position_ids, response_position_ids], dim=-1)\\\\\\\\n-        response_attention_mask = VF.get_eos_mask(\\\\\\\\n+        response_mask = VF.get_eos_mask(\\\\\\\\n             response_ids=response_ids, eos_token_id=eos_token_id, dtype=attention_mask.dtype\\\\\\\\n         )\\\\\\\\n-        attention_mask = torch.cat((attention_mask, response_attention_mask), dim=-1)\\\\\\\\n+        attention_mask = torch.cat((attention_mask, response_mask), dim=-1)\\\\\\\\n \\\\\\\\n         # all the tp ranks should contain the same data here. data in all ranks are valid\\\\\\\\n         batch = TensorDict(\\\\\\\\n@@ -183,6 +183,7 @@ def generate_sequences(self, prompts: DataProto) -> DataProto:\\\\\\\\n                 \\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\": response_ids,\\\\\\\\n                 \\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\": sequence_ids,  # here input_ids become the whole sentences\\\\\\\\n                 \\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\": attention_mask,\\\\\\\\n+                \\\\\\\\\\\\\\\"response_mask\\\\\\\\\\\\\\\": response_mask,\\\\\\\\n                 \\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\": position_ids,\\\\\\\\n             },\\\\\\\\n             batch_size=batch_size,\\\\\\\"}],\\\\\\\"sha\\\\\\\":\\\\\\\"ac0bc0494ad6b39fe4c1773aff0de5d6663d83af\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"committer\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"noreply@github.com\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-27T15:44:01Z\\\\\\\"},\\\\\\\"author\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"hiyouga@buaa.edu.cn\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-03-27T15:44:01Z\\\\\\\"}}},\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "I still haven't found the specific commit that changes `micro_batch_size_per_device_for_update` from 4 to 1 in the `examples/config.yaml` file. Let me search through the rest of the commits more efficiently. Let me check a commit that should be more recent.\n\nActually, let me step back and look for the commit by searching backwards from the most recent commits. Since earlier I saw that commit `6c0bba7ebc1dcec2101ca6a620db503c108c6f9e` had the values as 4 and 16, I need to find a commit that comes after that where they were changed to 1 and 2 (or other values). But based on the task description, I suspect the commit I'm looking for changes the values FROM 4 to 1.\n\nLet me search a bit more systematically:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"ref\\\": \\\"4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\"}\", \"include_output_fields\": [\"sha\", \"commit.author\", \"commit.committer\", \"files\"]}",
    "call_id": "toolu_01H3SeViUYNBeFSb6ZsaniQp",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01H3SeViUYNBeFSb6ZsaniQp",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"files\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"37d76214a0cecff48ccc21c9b94c42162f922745\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/baselines/qwen2_5_vl_3b_clevr.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":3,\\\\\\\"deletions\\\\\\\":4,\\\\\\\"changes\\\\\\\":7,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fbaselines%2Fqwen2_5_vl_3b_clevr.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fbaselines%2Fqwen2_5_vl_3b_clevr.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fbaselines%2Fqwen2_5_vl_3b_clevr.sh?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -2,7 +2,7 @@ set -x\\\\\\\\n \\\\\\\\n MODEL_PATH=Qwen/Qwen2.5-VL-3B-Instruct  # replace it with your local file path\\\\\\\\n \\\\\\\\n-SYSTEM_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant\\\\\\\\n+FORMAT_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant\\\\\\\\n  first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning\\\\\\\\n  process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e.,\\\\\\\\n  <think> reasoning process here </think><answer> answer here </answer>\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n@@ -11,10 +11,9 @@ python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n     data.train_files=BUAADreamer/clevr_count_70k@train \\\\\\\\\\\\\\\\\\\\\\\\n     data.val_files=BUAADreamer/clevr_count_70k@test \\\\\\\\\\\\\\\\\\\\\\\\n-    data.system_prompt=\\\\\\\\\\\\\\\"${SYSTEM_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n+    data.format_prompt=\\\\\\\\\\\\\\\"${FORMAT_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.tensor_parallel_size=1 \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.rollout.enable_chunked_prefill=false \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.reward.compute_score=r1v \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.reward.score_function=r1v \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.experiment_name=qwen2_5_vl_3b_clevr \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.n_gpus_per_node=2\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"7b4473515d0e6839836193c0aa4179952b90483d\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/baselines/qwen2_5_vl_3b_geoqa8k.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":3,\\\\\\\"deletions\\\\\\\":4,\\\\\\\"changes\\\\\\\":7,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fbaselines%2Fqwen2_5_vl_3b_geoqa8k.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fbaselines%2Fqwen2_5_vl_3b_geoqa8k.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fbaselines%2Fqwen2_5_vl_3b_geoqa8k.sh?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -2,7 +2,7 @@ set -x\\\\\\\\n \\\\\\\\n MODEL_PATH=Qwen/Qwen2.5-VL-3B-Instruct  # replace it with your local file path\\\\\\\\n \\\\\\\\n-SYSTEM_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant\\\\\\\\n+FORMAT_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant\\\\\\\\n  first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning\\\\\\\\n  process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e.,\\\\\\\\n  <think> reasoning process here </think><answer> answer here </answer>\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n@@ -11,10 +11,9 @@ python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n     data.train_files=leonardPKU/GEOQA_8K_R1V@train \\\\\\\\\\\\\\\\\\\\\\\\n     data.val_files=leonardPKU/GEOQA_8K_R1V@test \\\\\\\\\\\\\\\\\\\\\\\\n-    data.system_prompt=\\\\\\\\\\\\\\\"${SYSTEM_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n+    data.format_prompt=\\\\\\\\\\\\\\\"${FORMAT_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.tensor_parallel_size=1 \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.rollout.enable_chunked_prefill=false \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.reward.compute_score=r1v \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.reward.score_function=r1v \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.experiment_name=qwen2_5_vl_3b_geoqa8k \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.n_gpus_per_node=8\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"98bddb9c05460c540419f5f0570b39a976b99344\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/config.yaml\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":3,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":5,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fconfig.yaml?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -68,7 +68,8 @@ worker:\\\\\\\\n \\\\\\\\n   reward:\\\\\\\\n     reward_type: function\\\\\\\\n-    compute_score: math\\\\\\\\n+    score_function: math\\\\\\\\n+    skip_special_tokens: true\\\\\\\\n \\\\\\\\n trainer:\\\\\\\\n   total_episodes: 15\\\\\\\\n@@ -80,7 +81,7 @@ trainer:\\\\\\\\n   val_freq: 5  # -1 to disable\\\\\\\\n   val_before_train: true\\\\\\\\n   val_only: false\\\\\\\\n-  val_generations_to_log: 1\\\\\\\\n+  val_generations_to_log: 3\\\\\\\\n   save_freq: 5  # -1 to disable\\\\\\\\n   save_limit: 3  # -1 to disable\\\\\\\\n   save_checkpoint_path: null\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"3e1eca7aeb041761eea3572c66d3150fcef2702a\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_7b_math_grpo.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fqwen2_5_7b_math_grpo.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fqwen2_5_7b_math_grpo.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_7b_math_grpo.sh?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -2,14 +2,14 @@ set -x\\\\\\\\n \\\\\\\\n MODEL_PATH=Qwen/Qwen2.5-7B-Instruct  # replace it with your local file path\\\\\\\\n \\\\\\\\n-SYSTEM_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal monologue and then provide the final answer.\\\\\\\\n+FORMAT_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal monologue and then provide the final answer.\\\\\\\\n  The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in \\\\\\\\\\\\\\\\boxed{}.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n     data.train_files=hiyouga/math12k@train \\\\\\\\\\\\\\\\\\\\\\\\n     data.val_files=hiyouga/math12k@test \\\\\\\\\\\\\\\\\\\\\\\\n-    data.system_prompt=\\\\\\\\\\\\\\\"${SYSTEM_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n+    data.format_prompt=\\\\\\\\\\\\\\\"${FORMAT_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.experiment_name=qwen2_5_7b_math_grpo \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.n_gpus_per_node=8\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"af9102c48a12f93d505622b462082776f79e0e3c\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":5,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_vl_32b_geo3k_grpo.sh?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -2,20 +2,19 @@ set -x\\\\\\\\n \\\\\\\\n MODEL_PATH=Qwen/Qwen2.5-VL-32B-Instruct  # replace it with your local file path\\\\\\\\n \\\\\\\\n-SYSTEM_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal monologue and then provide the final answer.\\\\\\\\n+FORMAT_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal monologue and then provide the final answer.\\\\\\\\n  The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in \\\\\\\\\\\\\\\\boxed{}.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n     data.train_files=hiyouga/geometry3k@train \\\\\\\\\\\\\\\\\\\\\\\\n     data.val_files=hiyouga/geometry3k@test \\\\\\\\\\\\\\\\\\\\\\\\n-    data.system_prompt=\\\\\\\\\\\\\\\"${SYSTEM_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n+    data.format_prompt=\\\\\\\\\\\\\\\"${FORMAT_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.micro_batch_size_per_device_for_update=1 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.micro_batch_size_per_device_for_experience=8 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.fsdp.torch_dtype=bf16 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.optim.strategy=adamw_bf16 \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.tensor_parallel_size=8 \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.rollout.enable_chunked_prefill=false \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.experiment_name=qwen2_5_vl_32b_geo_grpo \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.n_gpus_per_node=8\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"99774766b6653d1802584bf213e6ed6a09c7cf24\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_vl_3b_geo3k_grpo.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":5,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fqwen2_5_vl_3b_geo3k_grpo.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fqwen2_5_vl_3b_geo3k_grpo.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_vl_3b_geo3k_grpo.sh?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -2,16 +2,15 @@ set -x\\\\\\\\n \\\\\\\\n MODEL_PATH=Qwen/Qwen2.5-VL-3B-Instruct  # replace it with your local file path\\\\\\\\n \\\\\\\\n-SYSTEM_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal monologue and then provide the final answer.\\\\\\\\n+FORMAT_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal monologue and then provide the final answer.\\\\\\\\n  The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in \\\\\\\\\\\\\\\\boxed{}.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n     data.train_files=hiyouga/geometry3k@train \\\\\\\\\\\\\\\\\\\\\\\\n     data.val_files=hiyouga/geometry3k@test \\\\\\\\\\\\\\\\\\\\\\\\n-    data.system_prompt=\\\\\\\\\\\\\\\"${SYSTEM_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n+    data.format_prompt=\\\\\\\\\\\\\\\"${FORMAT_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.tensor_parallel_size=1 \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.rollout.enable_chunked_prefill=false \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.experiment_name=qwen2_5_vl_3b_geo_grpo \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.n_gpus_per_node=2\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"8c15f5243b295e9d09244c7831247d920d943343\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_vl_7b_geo3k_grpo.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":5,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fqwen2_5_vl_7b_geo3k_grpo.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fqwen2_5_vl_7b_geo3k_grpo.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_vl_7b_geo3k_grpo.sh?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -2,15 +2,14 @@ set -x\\\\\\\\n \\\\\\\\n MODEL_PATH=Qwen/Qwen2.5-VL-7B-Instruct  # replace it with your local file path\\\\\\\\n \\\\\\\\n-SYSTEM_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal monologue and then provide the final answer.\\\\\\\\n+FORMAT_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal monologue and then provide the final answer.\\\\\\\\n  The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in \\\\\\\\\\\\\\\\boxed{}.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n     data.train_files=hiyouga/geometry3k@train \\\\\\\\\\\\\\\\\\\\\\\\n     data.val_files=hiyouga/geometry3k@test \\\\\\\\\\\\\\\\\\\\\\\\n-    data.system_prompt=\\\\\\\\\\\\\\\"${SYSTEM_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n+    data.format_prompt=\\\\\\\\\\\\\\\"${FORMAT_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.rollout.enable_chunked_prefill=false \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.experiment_name=qwen2_5_vl_7b_geo_grpo \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.n_gpus_per_node=8\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"970e63f8722a8a1055bad5156f07669082c846ca\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_vl_7b_geo3k_reinforce.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":5,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fqwen2_5_vl_7b_geo3k_reinforce.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fqwen2_5_vl_7b_geo3k_reinforce.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_vl_7b_geo3k_reinforce.sh?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -2,16 +2,15 @@ set -x\\\\\\\\n \\\\\\\\n MODEL_PATH=Qwen/Qwen2.5-VL-7B-Instruct  # replace it with your local file path\\\\\\\\n \\\\\\\\n-SYSTEM_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal monologue and then provide the final answer.\\\\\\\\n+FORMAT_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal monologue and then provide the final answer.\\\\\\\\n  The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in \\\\\\\\\\\\\\\\boxed{}.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n     data.train_files=hiyouga/geometry3k@train \\\\\\\\\\\\\\\\\\\\\\\\n     data.val_files=hiyouga/geometry3k@test \\\\\\\\\\\\\\\\\\\\\\\\n-    data.system_prompt=\\\\\\\\\\\\\\\"${SYSTEM_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n+    data.format_prompt=\\\\\\\\\\\\\\\"${FORMAT_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.rollout.enable_chunked_prefill=false \\\\\\\\\\\\\\\\\\\\\\\\n     algorithm.adv_estimator=reinforce_plus_plus \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.experiment_name=qwen2_5_vl_7b_geo_reinforce_pp \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.n_gpus_per_node=8\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"7244ae30be559d9f98c939032460aa754dba3263\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/qwen2_5_vl_7b_geo3k_swanlab.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":5,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fqwen2_5_vl_7b_geo3k_swanlab.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/examples%2Fqwen2_5_vl_7b_geo3k_swanlab.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fqwen2_5_vl_7b_geo3k_swanlab.sh?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -2,16 +2,15 @@ set -x\\\\\\\\n \\\\\\\\n MODEL_PATH=Qwen/Qwen2.5-VL-7B-Instruct  # replace it with your local file path\\\\\\\\n \\\\\\\\n-SYSTEM_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal monologue and then provide the final answer.\\\\\\\\n+FORMAT_PROMPT=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"You FIRST think about the reasoning process as an internal monologue and then provide the final answer.\\\\\\\\n  The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in \\\\\\\\\\\\\\\\boxed{}.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n     data.train_files=hiyouga/geometry3k@train \\\\\\\\\\\\\\\\\\\\\\\\n     data.val_files=hiyouga/geometry3k@test \\\\\\\\\\\\\\\\\\\\\\\\n-    data.system_prompt=\\\\\\\\\\\\\\\"${SYSTEM_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n+    data.format_prompt=\\\\\\\\\\\\\\\"${FORMAT_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.rollout.enable_chunked_prefill=false \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.experiment_name=qwen2_5_vl_7b_geo_grpo \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.logger=['console','swanlab'] \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.n_gpus_per_node=8\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"f7cce74b94fd3bf945269e0e3bb8a5e44ae30f0c\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/protocol.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":54,\\\\\\\"deletions\\\\\\\":32,\\\\\\\"changes\\\\\\\":86,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Fprotocol.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Fprotocol.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fprotocol.py?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -122,7 +122,7 @@ def batch_collate(features: List[Dict[str, Any]]) -> Dict[str, List[Any]]:\\\\\\\\n     return batch_features\\\\\\\\n \\\\\\\\n \\\\\\\\n-def fold_batch_dim(data: \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\", new_batch_size):\\\\\\\\n+def fold_batch_dim(data: \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\", new_batch_size: int):\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n     Fold a batch dim from [bsz, xxx] into [new_bsz, bsz // new_bsz, xxx]\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n@@ -158,8 +158,8 @@ def collate_fn(data_items: list[\\\\\\\\\\\\\\\"DataProtoItem\\\\\\\\\\\\\\\"]):\\\\\\\\n @dataclass\\\\\\\\n class DataProtoItem:\\\\\\\\n     batch: Optional[TensorDict] = None\\\\\\\\n-    non_tensor_batch: Dict = field(default_factory=dict)\\\\\\\\n-    meta_info: Dict = field(default_factory=dict)\\\\\\\\n+    non_tensor_batch: Dict[str, NDArray] = field(default_factory=dict)\\\\\\\\n+    meta_info: Dict[str, Any] = field(default_factory=dict)\\\\\\\\n \\\\\\\\n \\\\\\\\n @dataclass\\\\\\\\n@@ -172,13 +172,13 @@ class DataProto:\\\\\\\\n     \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n     batch: Optional[TensorDict] = None\\\\\\\\n-    non_tensor_batch: Dict[str, Any] = field(default_factory=dict)\\\\\\\\n+    non_tensor_batch: Dict[str, NDArray] = field(default_factory=dict)\\\\\\\\n     meta_info: Dict[str, Any] = field(default_factory=dict)\\\\\\\\n \\\\\\\\n     def __post_init__(self):\\\\\\\\n         self.check_consistency()  # perform necessary checking\\\\\\\\n \\\\\\\\n-    def __len__(self):\\\\\\\\n+    def __len__(self) -> int:\\\\\\\\n         if self.batch is not None:\\\\\\\\n             return self.batch.batch_size[0]\\\\\\\\n         elif self.non_tensor_batch is not None and len(self.non_tensor_batch) > 0:\\\\\\\\n@@ -187,43 +187,41 @@ def __len__(self):\\\\\\\\n         else:\\\\\\\\n             return 0\\\\\\\\n \\\\\\\\n-    def __getitem__(self, item):\\\\\\\\n+    def __getitem__(self, item: Union[int, slice]) -> Union[\\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"DataProtoItem\\\\\\\\\\\\\\\"]:\\\\\\\\n         tensor_data = self.batch[item]\\\\\\\\n         non_tensor_data = {key: val[item] for key, val in self.non_tensor_batch.items()}\\\\\\\\n         return_type = DataProto if isinstance(item, slice) else DataProtoItem\\\\\\\\n         return return_type(batch=tensor_data, non_tensor_batch=non_tensor_data, meta_info=self.meta_info)\\\\\\\\n \\\\\\\\n-    def __getstate__(self):\\\\\\\\n+    def __getstate__(self) -> Tuple[bytes, Dict[str, NDArray], Dict[str, Any]]:\\\\\\\\n         buffer = io.BytesIO()\\\\\\\\n         if self.batch is not None:\\\\\\\\n-            self.batch = self.batch.contiguous()\\\\\\\\n-            self.batch = self.batch.consolidate()\\\\\\\\n+            self.batch: TensorDict = self.batch.contiguous()\\\\\\\\n+            self.batch: TensorDict = self.batch.consolidate()\\\\\\\\n \\\\\\\\n         torch.save(self.batch, buffer)\\\\\\\\n         buffer_bytes = buffer.getvalue()\\\\\\\\n         return buffer_bytes, self.non_tensor_batch, self.meta_info\\\\\\\\n \\\\\\\\n-    def __setstate__(self, data):\\\\\\\\n+    def __setstate__(self, data: Tuple[bytes, Dict[str, NDArray], Dict[str, Any]]) -> None:\\\\\\\\n         batch_deserialized_bytes, non_tensor_batch, meta_info = data\\\\\\\\n-        batch_deserialized = io.BytesIO(initial_bytes=batch_deserialized_bytes)\\\\\\\\n-        batch = torch.load(\\\\\\\\n-            batch_deserialized, weights_only=False, map_location=\\\\\\\\\\\\\\\"cpu\\\\\\\\\\\\\\\" if not torch.cuda.is_available() else None\\\\\\\\n-        )\\\\\\\\n+        batch_deserialized = io.BytesIO(batch_deserialized_bytes)\\\\\\\\n+        batch = torch.load(batch_deserialized, weights_only=False, map_location=\\\\\\\\\\\\\\\"cpu\\\\\\\\\\\\\\\")\\\\\\\\n         self.batch = batch\\\\\\\\n         self.non_tensor_batch = non_tensor_batch\\\\\\\\n         self.meta_info = meta_info\\\\\\\\n \\\\\\\\n-    def save_to_disk(self, filepath):\\\\\\\\n+    def save_to_disk(self, filepath: str) -> None:\\\\\\\\n         with open(filepath, \\\\\\\\\\\\\\\"wb\\\\\\\\\\\\\\\") as f:\\\\\\\\n             pickle.dump(self, f)\\\\\\\\n \\\\\\\\n     @staticmethod\\\\\\\\n-    def load_from_disk(filepath) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n+    def load_from_disk(filepath: str) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n         with open(filepath, \\\\\\\\\\\\\\\"rb\\\\\\\\\\\\\\\") as f:\\\\\\\\n             data = pickle.load(f)\\\\\\\\n             return data\\\\\\\\n \\\\\\\\n-    def print_size(self, prefix=\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"):\\\\\\\\n+    def print_size(self, prefix: str = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\") -> None:\\\\\\\\n         size_of_tensordict = 0\\\\\\\\n         for tensor in self.batch.values():\\\\\\\\n             if isinstance(tensor, torch.Tensor):\\\\\\\\n@@ -255,7 +253,11 @@ def check_consistency(self):\\\\\\\\n                 assert len(val) == batch_size, f\\\\\\\\\\\\\\\"key {key} length {len(val)} is not equal to batch size {batch_size}.\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n     @classmethod\\\\\\\\n-    def from_single_dict(cls, data: Dict[str, Union[torch.Tensor, NDArray]], meta_info=None):\\\\\\\\n+    def from_single_dict(\\\\\\\\n+        cls,\\\\\\\\n+        data: Dict[str, Union[torch.Tensor, NDArray]],\\\\\\\\n+        meta_info: Optional[Dict[str, Any]] = None,\\\\\\\\n+    ) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n         tensors = {}\\\\\\\\n         non_tensors = {}\\\\\\\\n         for key, value in data.items():\\\\\\\\n@@ -269,7 +271,13 @@ def from_single_dict(cls, data: Dict[str, Union[torch.Tensor, NDArray]], meta_in\\\\\\\\n         return DataProto.from_dict(tensors=tensors, non_tensors=non_tensors, meta_info=meta_info)\\\\\\\\n \\\\\\\\n     @classmethod\\\\\\\\n-    def from_dict(cls, tensors: Dict[str, torch.Tensor], non_tensors=None, meta_info=None, num_batch_dims=1):\\\\\\\\n+    def from_dict(\\\\\\\\n+        cls,\\\\\\\\n+        tensors: Dict[str, torch.Tensor],\\\\\\\\n+        non_tensors: Dict[str, NDArray] = None,\\\\\\\\n+        meta_info: Optional[Dict[str, Any]] = None,\\\\\\\\n+        num_batch_dims: int = 1,\\\\\\\\n+    ) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Create a DataProto from a dict of tensors. This assumes that\\\\\\\\n         1. All the tensor in tensors have the same dim0\\\\\\\\n         2. Only dim0 is the batch dim\\\\\\\\n@@ -293,13 +301,14 @@ def from_dict(cls, tensors: Dict[str, torch.Tensor], non_tensors=None, meta_info\\\\\\\\n             else:\\\\\\\\n                 current_batch = tensor.shape[:num_batch_dims]\\\\\\\\n                 assert batch_size == current_batch, (\\\\\\\\n-                    f\\\\\\\\\\\\\\\"Not all the tensor in tensors have the same batch size with batch_dims={num_batch_dims}. Got {pivot_key} has {batch_size}, {key} has {current_batch}\\\\\\\\\\\\\\\"\\\\\\\\n+                    f\\\\\\\\\\\\\\\"Not all the tensor in tensors have the same batch size with batch_dims={num_batch_dims}. \\\\\\\\\\\\\\\"\\\\\\\\n+                    f\\\\\\\\\\\\\\\"Got {pivot_key} has {batch_size}, {key} has {current_batch}\\\\\\\\\\\\\\\"\\\\\\\\n                 )\\\\\\\\n \\\\\\\\n         tensor_dict = TensorDict(source=tensors, batch_size=batch_size)\\\\\\\\n         return cls(batch=tensor_dict, non_tensor_batch=non_tensors, meta_info=meta_info)\\\\\\\\n \\\\\\\\n-    def to(self, device) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n+    def to(self, device: torch.device) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"move the batch to device\\\\\\\\n \\\\\\\\n         Args:\\\\\\\\n@@ -314,7 +323,13 @@ def to(self, device) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n \\\\\\\\n         return self\\\\\\\\n \\\\\\\\n-    def select(self, batch_keys=None, non_tensor_batch_keys=None, meta_info_keys=None, deepcopy=False) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n+    def select(\\\\\\\\n+        self,\\\\\\\\n+        batch_keys: Optional[List[str]] = None,\\\\\\\\n+        non_tensor_batch_keys: Optional[List[str]] = None,\\\\\\\\n+        meta_info_keys: Optional[List[str]] = None,\\\\\\\\n+        deepcopy: bool = False,\\\\\\\\n+    ) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Select a subset of the DataProto via batch_keys and meta_info_keys\\\\\\\\n \\\\\\\\n         Args:\\\\\\\\n@@ -332,15 +347,15 @@ def select(self, batch_keys=None, non_tensor_batch_keys=None, meta_info_keys=Non\\\\\\\\n             sub_batch = self.batch\\\\\\\\n \\\\\\\\n         if non_tensor_batch_keys is not None:\\\\\\\\n-            non_tensor_batch = {key: val for key, val in self.non_tensor_batch.items() if key in non_tensor_batch_keys}\\\\\\\\n+            non_tensor_batch = {k: v for k, v in self.non_tensor_batch.items() if k in non_tensor_batch_keys}\\\\\\\\n         else:\\\\\\\\n             non_tensor_batch = self.non_tensor_batch\\\\\\\\n \\\\\\\\n         if deepcopy:\\\\\\\\n             non_tensor_batch = copy.deepcopy(non_tensor_batch)\\\\\\\\n \\\\\\\\n         if meta_info_keys is not None:\\\\\\\\n-            sub_meta_info = {key: val for key, val in self.meta_info.items() if key in meta_info_keys}\\\\\\\\n+            sub_meta_info = {k: v for k, v in self.meta_info.items() if k in meta_info_keys}\\\\\\\\n         else:\\\\\\\\n             sub_meta_info = self.meta_info\\\\\\\\n \\\\\\\\n@@ -349,7 +364,12 @@ def select(self, batch_keys=None, non_tensor_batch_keys=None, meta_info_keys=Non\\\\\\\\n \\\\\\\\n         return DataProto(batch=sub_batch, non_tensor_batch=non_tensor_batch, meta_info=sub_meta_info)\\\\\\\\n \\\\\\\\n-    def pop(self, batch_keys=None, non_tensor_batch_keys=None, meta_info_keys=None) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n+    def pop(\\\\\\\\n+        self,\\\\\\\\n+        batch_keys: Optional[List[str]] = None,\\\\\\\\n+        non_tensor_batch_keys: Optional[List[str]] = None,\\\\\\\\n+        meta_info_keys: Optional[List[str]] = None,\\\\\\\\n+    ) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Pop a subset of the DataProto via `batch_keys` and `meta_info_keys`\\\\\\\\n \\\\\\\\n         Args:\\\\\\\\n@@ -377,7 +397,9 @@ def pop(self, batch_keys=None, non_tensor_batch_keys=None, meta_info_keys=None)\\\\\\\\n \\\\\\\\n         return DataProto.from_dict(tensors=tensors, non_tensors=non_tensors, meta_info=meta_info)\\\\\\\\n \\\\\\\\n-    def rename(self, old_keys=None, new_keys=None) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n+    def rename(\\\\\\\\n+        self, old_keys: Optional[Union[str, List[str]]] = None, new_keys: Optional[Union[str, List[str]]] = None\\\\\\\\n+    ) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n         Note that this function only rename the key in the batch\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n@@ -422,7 +444,9 @@ def union(self, other: \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\") -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n         self.meta_info = union_two_dict(self.meta_info, other.meta_info)\\\\\\\\n         return self\\\\\\\\n \\\\\\\\n-    def make_iterator(self, mini_batch_size, epochs, seed=None, dataloader_kwargs=None):\\\\\\\\n+    def make_iterator(\\\\\\\\n+        self, mini_batch_size: int, epochs: int, seed: int = None, dataloader_kwargs: Dict[str, Any] = None\\\\\\\\n+    ):\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Make an iterator from the DataProto. This is built upon that TensorDict can be used as a normal Pytorch\\\\\\\\n         dataset. See https://pytorch.org/tensordict/tutorials/data_fashion for more details.\\\\\\\\n \\\\\\\\n@@ -509,9 +533,7 @@ def concat(data: List[\\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\"]) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n         Returns:\\\\\\\\n             DataProto: concatenated DataProto\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        batch_lst = []\\\\\\\\n-        for batch in data:\\\\\\\\n-            batch_lst.append(batch.batch)\\\\\\\\n+        batch_lst = [batch.batch for batch in data]\\\\\\\\n         if batch_lst[0] is not None:\\\\\\\\n             new_batch = torch.cat(batch_lst, dim=0)\\\\\\\\n         else:\\\\\\\\n@@ -523,15 +545,15 @@ def concat(data: List[\\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\"]) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n \\\\\\\\n         return DataProto(batch=new_batch, non_tensor_batch=non_tensor_batch, meta_info=data[0].meta_info)\\\\\\\\n \\\\\\\\n-    def reorder(self, indices):\\\\\\\\n+    def reorder(self, indices: torch.Tensor) -> None:\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n         Note that this operation is in-place\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n         indices_np = indices.detach().numpy()\\\\\\\\n         self.batch = self.batch[indices]\\\\\\\\n         self.non_tensor_batch = {key: val[indices_np] for key, val in self.non_tensor_batch.items()}\\\\\\\\n \\\\\\\\n-    def repeat(self, repeat_times=2, interleave=True):\\\\\\\\n+    def repeat(self, repeat_times: int = 2, interleave: bool = True) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n         Repeat the batch data a specified number of times.\\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"f57c3fe7512dd054f6b6f2277fc219c58a46544a\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Ftrainer%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Ftrainer%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fconfig.py?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -42,7 +42,7 @@ class DataConfig:\\\\\\\\n     max_response_length: int = 512\\\\\\\\n     rollout_batch_size: int = 512\\\\\\\\n     val_batch_size: int = -1\\\\\\\\n-    system_prompt: Optional[str] = None\\\\\\\\n+    format_prompt: Optional[str] = None\\\\\\\\n     shuffle: bool = True\\\\\\\\n     seed: int = 1\\\\\\\\n     max_pixels: int = 4194304\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"63afcbc6f59c217d9b1679b11fcd9037dabd4ea9\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/main.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Ftrainer%2Fmain.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Ftrainer%2Fmain.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fmain.py?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -68,8 +68,8 @@ def run(self, config: PPOConfig):\\\\\\\\n         }\\\\\\\\n         resource_pool_manager = ResourcePoolManager(resource_pool_spec=resource_pool_spec, mapping=mapping)\\\\\\\\n \\\\\\\\n-        reward_fn = CustomRewardManager(tokenizer=tokenizer, compute_score=config.worker.reward.compute_score)\\\\\\\\n-        val_reward_fn = CustomRewardManager(tokenizer=tokenizer, compute_score=config.worker.reward.compute_score)\\\\\\\\n+        reward_fn = CustomRewardManager(tokenizer=tokenizer, config=config.worker.reward)\\\\\\\\n+        val_reward_fn = CustomRewardManager(tokenizer=tokenizer, config=config.worker.reward)\\\\\\\\n \\\\\\\\n         trainer = RayPPOTrainer(\\\\\\\\n             config=config,\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"72e97595ef35540e8752c27fe8a9737659a20925\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/ray_trainer.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":29,\\\\\\\"deletions\\\\\\\":23,\\\\\\\"changes\\\\\\\":52,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fray_trainer.py?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -129,25 +129,19 @@ def apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.KLController, kl_penal\\\\\\\\n \\\\\\\\n     # compute kl between ref_policy and current policy\\\\\\\\n     if \\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\" in data.batch.keys():\\\\\\\\n-        kld = core_algos.compute_kl(\\\\\\\\n-            data.batch[\\\\\\\\\\\\\\\"old_log_probs\\\\\\\\\\\\\\\"], data.batch[\\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\"], kl_penalty=kl_penalty\\\\\\\\n-        )  # (batch_size, response_length)\\\\\\\\n-        kld = kld * response_mask\\\\\\\\n-        beta = kl_ctrl.kl_coef\\\\\\\\n+        kld = core_algos.compute_kl(data.batch[\\\\\\\\\\\\\\\"old_log_probs\\\\\\\\\\\\\\\"], data.batch[\\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\"], kl_penalty=kl_penalty)\\\\\\\\n+        kld = kld * response_mask  # (batch_size, response_length)\\\\\\\\n     else:\\\\\\\\n-        beta = 0\\\\\\\\n         kld = torch.zeros_like(response_mask, dtype=torch.float32)\\\\\\\\n \\\\\\\\n-    token_level_rewards = token_level_scores - beta * kld\\\\\\\\n+    data.batch[\\\\\\\\\\\\\\\"token_level_rewards\\\\\\\\\\\\\\\"] = token_level_scores - kl_ctrl.kl_coef * kld\\\\\\\\n \\\\\\\\n     current_kl = VF.masked_mean(kld, mask=response_mask, dim=-1)  # average over sequence\\\\\\\\n     current_kl = torch.mean(current_kl, dim=0).item()\\\\\\\\n+    metrics = {\\\\\\\\\\\\\\\"critic/kl\\\\\\\\\\\\\\\": current_kl, \\\\\\\\\\\\\\\"critic/kl_coef\\\\\\\\\\\\\\\": kl_ctrl.kl_coef}\\\\\\\\n \\\\\\\\n     # According to https://github.com/huggingface/trl/blob/v0.11.0/trl/trainer/ppo_trainer.py#L880\\\\\\\\n     kl_ctrl.update(current_kl=current_kl, n_steps=batch_size)\\\\\\\\n-    data.batch[\\\\\\\\\\\\\\\"token_level_rewards\\\\\\\\\\\\\\\"] = token_level_rewards\\\\\\\\n-\\\\\\\\n-    metrics = {\\\\\\\\\\\\\\\"critic/kl\\\\\\\\\\\\\\\": current_kl, \\\\\\\\\\\\\\\"critic/kl_coef\\\\\\\\\\\\\\\": beta}\\\\\\\\n     return data, metrics\\\\\\\\n \\\\\\\\n \\\\\\\\n@@ -242,12 +236,22 @@ def __init__(\\\\\\\\n             raise NotImplementedError(f\\\\\\\\\\\\\\\"Unknown advantage estimator: {config.algorithm.adv_estimator}.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         if config.data.rollout_batch_size % config.worker.actor.global_batch_size != 0:\\\\\\\\n-            raise ValueError(\\\\\\\\\\\\\\\"Rollout batch size must be divisible by global batch size.\\\\\\\\\\\\\\\")\\\\\\\\n+            raise ValueError(\\\\\\\\\\\\\\\"Rollout batch size must be divisible by actor global batch size.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+        if config.data.rollout_batch_size % config.worker.actor.micro_batch_size_per_device_for_experience != 0:\\\\\\\\n+            raise ValueError(\\\\\\\\\\\\\\\"Rollout batch size must be divisible by actor micro batch size for experience.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n-        if self.use_critic and config.data.rollout_batch_size % config.worker.critic.global_batch_size != 0:\\\\\\\\n-            raise ValueError(\\\\\\\\\\\\\\\"Rollout batch size must be divisible by global batch size.\\\\\\\\\\\\\\\")\\\\\\\\n+        if self.use_critic:\\\\\\\\n+            if config.data.rollout_batch_size % config.worker.critic.global_batch_size != 0:\\\\\\\\n+                raise ValueError(\\\\\\\\\\\\\\\"Rollout batch size must be divisible by critic global batch size.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n-        if config.algorithm.adv_estimator in (AdvantageEstimator.GRPO, AdvantageEstimator.RLOO) and config.worker.rollout.n == 1:\\\\\\\\n+            if config.data.rollout_batch_size % config.worker.critic.micro_batch_size_per_device_for_experience != 0:\\\\\\\\n+                raise ValueError(\\\\\\\\\\\\\\\"Rollout batch size must be divisible by critic micro batch size for experience.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+        if (\\\\\\\\n+            config.algorithm.adv_estimator in (AdvantageEstimator.GRPO, AdvantageEstimator.RLOO)\\\\\\\\n+            and config.worker.rollout.n == 1\\\\\\\\n+        ):\\\\\\\\n             raise ValueError(\\\\\\\\\\\\\\\"GRPO and RLOO algorithm need `config.worker.rollout.n > 1`.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         self._create_dataloader()\\\\\\\\n@@ -262,7 +266,7 @@ def _create_dataloader(self) -> None:\\\\\\\\n             image_key=self.config.data.image_key,\\\\\\\\n             max_prompt_length=self.config.data.max_prompt_length,\\\\\\\\n             truncation=\\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\",\\\\\\\\n-            system_prompt=self.config.data.system_prompt,\\\\\\\\n+            format_prompt=self.config.data.format_prompt,\\\\\\\\n             min_pixels=self.config.data.min_pixels,\\\\\\\\n             max_pixels=self.config.data.max_pixels,\\\\\\\\n         )\\\\\\\\n@@ -293,7 +297,7 @@ def _create_dataloader(self) -> None:\\\\\\\\n             image_key=self.config.data.image_key,\\\\\\\\n             max_prompt_length=self.config.data.max_prompt_length,\\\\\\\\n             truncation=\\\\\\\\\\\\\\\"right\\\\\\\\\\\\\\\",\\\\\\\\n-            system_prompt=self.config.data.system_prompt,\\\\\\\\n+            format_prompt=self.config.data.format_prompt,\\\\\\\\n             min_pixels=self.config.data.min_pixels,\\\\\\\\n             max_pixels=self.config.data.max_pixels,\\\\\\\\n         )\\\\\\\\n@@ -324,13 +328,15 @@ def _create_dataloader(self) -> None:\\\\\\\\n         self.config.worker.critic.optim.training_steps = training_steps\\\\\\\\n         print(f\\\\\\\\\\\\\\\"Total training steps: {self.training_steps}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n-    def _maybe_log_val_generations(self, inputs: List[str], outputs: List[str], scores: List[float]) -> None:\\\\\\\\n+    def _maybe_log_val_generations(\\\\\\\\n+        self, inputs: List[str], outputs: List[str], labels: List[str], scores: List[float]\\\\\\\\n+    ) -> None:\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Log a table of validation samples\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n         if self.config.trainer.val_generations_to_log <= 0:\\\\\\\\n             return\\\\\\\\n \\\\\\\\n         # Create tuples of (input, output, score) and sort by input text\\\\\\\\n-        samples = list(zip(inputs, outputs, scores))\\\\\\\\n+        samples = list(zip(inputs, outputs, labels, scores))\\\\\\\\n         samples.sort(key=lambda x: x[0])  # Sort by input text\\\\\\\\n \\\\\\\\n         # Use fixed random seed for deterministic shuffling\\\\\\\\n@@ -343,10 +349,10 @@ def _maybe_log_val_generations(self, inputs: List[str], outputs: List[str], scor\\\\\\\\n     def _validate(self) -> Dict[str, Any]:\\\\\\\\n         reward_tensor_lst = []\\\\\\\\n         # Lists to collect samples for the table\\\\\\\\n-        sample_inputs, sample_outputs, sample_scores = [], [], []\\\\\\\\n+        sample_inputs, sample_outputs, sample_labels, sample_scores = [], [], [], []\\\\\\\\n         reward_metrics_lst = defaultdict(list)\\\\\\\\n-        for test_data in self.val_dataloader:\\\\\\\\n-            test_batch = DataProto.from_single_dict(test_data)\\\\\\\\n+        for batch_dict in self.val_dataloader:\\\\\\\\n+            test_batch = DataProto.from_single_dict(batch_dict)\\\\\\\\n             # Store original inputs\\\\\\\\n             input_ids = test_batch.batch[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\"]\\\\\\\\n             input_texts = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\\\\\\\\n@@ -373,7 +379,7 @@ def _validate(self) -> Dict[str, Any]:\\\\\\\\n             output_ids = test_output_gen_batch.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"]\\\\\\\\n             output_texts = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]\\\\\\\\n             sample_outputs.extend(output_texts)\\\\\\\\n-\\\\\\\\n+            sample_labels.extend(test_batch.non_tensor_batch[\\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\"].tolist())\\\\\\\\n             test_batch = test_batch.union(test_output_gen_batch)\\\\\\\\n \\\\\\\\n             # evaluate using reward_function\\\\\\\\n@@ -387,7 +393,7 @@ def _validate(self) -> Dict[str, Any]:\\\\\\\\n             for key, value in reward_metrics.items():\\\\\\\\n                 reward_metrics_lst[key].extend(value)\\\\\\\\n \\\\\\\\n-        self._maybe_log_val_generations(inputs=sample_inputs, outputs=sample_outputs, scores=sample_scores)\\\\\\\\n+        self._maybe_log_val_generations(sample_inputs, sample_outputs, sample_labels, sample_scores)\\\\\\\\n         reward_score = torch.cat(reward_tensor_lst, dim=0).sum(-1).mean().item()\\\\\\\\n         val_reward_metrics = {f\\\\\\\\\\\\\\\"val/{key}_reward\\\\\\\\\\\\\\\": value for key, value in reduce_metrics(reward_metrics_lst).items()}\\\\\\\\n         return {\\\\\\\\\\\\\\\"val/reward_score\\\\\\\\\\\\\\\": reward_score, **val_reward_metrics}\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"aa7a673b1ca31855b6f52d3bc04bbea42ddeb2a2\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/dataset.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":4,\\\\\\\"deletions\\\\\\\":4,\\\\\\\"changes\\\\\\\":8,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Futils%2Fdataset.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Futils%2Fdataset.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fdataset.py?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -90,7 +90,7 @@ def __init__(\\\\\\\\n         image_key: str = \\\\\\\\\\\\\\\"images\\\\\\\\\\\\\\\",\\\\\\\\n         max_prompt_length: int = 1024,\\\\\\\\n         truncation: str = \\\\\\\\\\\\\\\"error\\\\\\\\\\\\\\\",\\\\\\\\n-        system_prompt: str = None,\\\\\\\\n+        format_prompt: str = None,\\\\\\\\n         max_pixels: int = None,\\\\\\\\n         min_pixels: int = None,\\\\\\\\n     ):\\\\\\\\n@@ -101,7 +101,7 @@ def __init__(\\\\\\\\n         self.image_key = image_key\\\\\\\\n         self.max_prompt_length = max_prompt_length\\\\\\\\n         self.truncation = truncation\\\\\\\\n-        self.system_prompt = system_prompt\\\\\\\\n+        self.format_prompt = format_prompt\\\\\\\\n         self.max_pixels = max_pixels\\\\\\\\n         self.min_pixels = min_pixels\\\\\\\\n \\\\\\\\n@@ -123,8 +123,8 @@ def __len__(self):\\\\\\\\n     def __getitem__(self, index):\\\\\\\\n         row_dict: dict = self.dataset[index]\\\\\\\\n         prompt_str: str = row_dict[self.prompt_key]\\\\\\\\n-        if self.system_prompt:\\\\\\\\n-            prompt_str = \\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\".join((self.system_prompt.strip(), prompt_str))\\\\\\\\n+        if self.format_prompt:\\\\\\\\n+            prompt_str = prompt_str + \\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\" + self.format_prompt.strip()\\\\\\\\n \\\\\\\\n         if self.image_key in row_dict:\\\\\\\\n             # https://huggingface.co/docs/transformers/en/tasks/image_text_to_text\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"b62cde6cf52af087585a92441db0fcfdc27bccb0\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/logger/gen_logger.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":12,\\\\\\\"deletions\\\\\\\":9,\\\\\\\"changes\\\\\\\":21,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Futils%2Flogger%2Fgen_logger.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Futils%2Flogger%2Fgen_logger.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Flogger%2Fgen_logger.py?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -31,22 +31,23 @@\\\\\\\\n @dataclass\\\\\\\\n class GenerationLogger(ABC):\\\\\\\\n     @abstractmethod\\\\\\\\n-    def log(self, samples: List[Tuple[str, str, float]], step: int) -> None: ...\\\\\\\\n+    def log(self, samples: List[Tuple[str, str, str, float]], step: int) -> None: ...\\\\\\\\n \\\\\\\\n \\\\\\\\n @dataclass\\\\\\\\n class ConsoleGenerationLogger(GenerationLogger):\\\\\\\\n-    def log(self, samples: List[Tuple[str, str, float]], step: int) -> None:\\\\\\\\n-        for inp, out, score in samples:\\\\\\\\n-            print(f\\\\\\\\\\\\\\\"[prompt] {inp}\\\\\\\\\\\\\\\\n[output] {out}\\\\\\\\\\\\\\\\n[score] {score}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n+    def log(self, samples: List[Tuple[str, str, str, float]], step: int) -> None:\\\\\\\\n+        for inp, out, lab, score in samples:\\\\\\\\n+            print(f\\\\\\\\\\\\\\\"[prompt] {inp}\\\\\\\\\\\\\\\\n[output] {out}\\\\\\\\\\\\\\\\n[ground_truth] {lab}\\\\\\\\\\\\\\\\n[score] {score}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n \\\\\\\\n @dataclass\\\\\\\\n class WandbGenerationLogger(GenerationLogger):\\\\\\\\n-    def log(self, samples: List[Tuple[str, str, float]], step: int) -> None:\\\\\\\\n+    def log(self, samples: List[Tuple[str, str, str, float]], step: int) -> None:\\\\\\\\n         # Create column names for all samples\\\\\\\\n         columns = [\\\\\\\\\\\\\\\"step\\\\\\\\\\\\\\\"] + sum(\\\\\\\\n-            [[f\\\\\\\\\\\\\\\"input_{i + 1}\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"output_{i + 1}\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"score_{i + 1}\\\\\\\\\\\\\\\"] for i in range(len(samples))], []\\\\\\\\n+            [[f\\\\\\\\\\\\\\\"input_{i + 1}\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"output_{i + 1}\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"label_{i + 1}\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"score_{i + 1}\\\\\\\\\\\\\\\"] for i in range(len(samples))],\\\\\\\\n+            [],\\\\\\\\n         )\\\\\\\\n \\\\\\\\n         if not hasattr(self, \\\\\\\\\\\\\\\"validation_table\\\\\\\\\\\\\\\"):\\\\\\\\n@@ -69,10 +70,12 @@ def log(self, samples: List[Tuple[str, str, float]], step: int) -> None:\\\\\\\\n \\\\\\\\n @dataclass\\\\\\\\n class SwanlabGenerationLogger(GenerationLogger):\\\\\\\\n-    def log(self, samples: List[Tuple[str, str, float]], step: int) -> None:\\\\\\\\n+    def log(self, samples: List[Tuple[str, str, str, float]], step: int) -> None:\\\\\\\\n         swanlab_text_list = []\\\\\\\\n         for i, sample in enumerate(samples):\\\\\\\\n-            row_text = f\\\\\\\\\\\\\\\"input: {sample[0]}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n---\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\noutput: {sample[1]}\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n---\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\nscore: {sample[2]}\\\\\\\\\\\\\\\"\\\\\\\\n+            row_text = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n---\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\".join(\\\\\\\\n+                (f\\\\\\\\\\\\\\\"input: {sample[0]}\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"output: {sample[1]}\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"label: {sample[2]}\\\\\\\\\\\\\\\", f\\\\\\\\\\\\\\\"score: {sample[3]}\\\\\\\\\\\\\\\")\\\\\\\\n+            )\\\\\\\\n             swanlab_text_list.append(swanlab.Text(row_text, caption=f\\\\\\\\\\\\\\\"sample {i + 1}\\\\\\\\\\\\\\\"))\\\\\\\\n \\\\\\\\n         swanlab.log({\\\\\\\\\\\\\\\"val/generations\\\\\\\\\\\\\\\": swanlab_text_list}, step=step)\\\\\\\\n@@ -94,6 +97,6 @@ def __init__(self, loggers: List[str]):\\\\\\\\n             if logger in GEN_LOGGERS:\\\\\\\\n                 self.loggers.append(GEN_LOGGERS[logger]())\\\\\\\\n \\\\\\\\n-    def log(self, samples: List[Tuple[str, str, float]], step: int) -> None:\\\\\\\\n+    def log(self, samples: List[Tuple[str, str, str, float]], step: int) -> None:\\\\\\\\n         for logger in self.loggers:\\\\\\\\n             logger.log(samples, step)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"b306d1fe5aab89a27b7fd9d99e885683e4b6764a\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/logger/logger.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Futils%2Flogger%2Flogger.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Futils%2Flogger%2Flogger.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Flogger%2Flogger.py?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -146,7 +146,7 @@ def log(self, data: Dict[str, Any], step: int) -> None:\\\\\\\\n         for logger in self.loggers:\\\\\\\\n             logger.log(data=data, step=step)\\\\\\\\n \\\\\\\\n-    def log_generation(self, samples: List[Tuple[str, str, float]], step: int) -> None:\\\\\\\\n+    def log_generation(self, samples: List[Tuple[str, str, str, float]], step: int) -> None:\\\\\\\\n         self.gen_logger.log(samples, step)\\\\\\\\n \\\\\\\\n     def __del__(self):\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"f4968fc4621822e3f9b4a8db34cc145cbb2ac50e\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/fsdp_workers.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":7,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":10,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Ffsdp_workers.py?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -57,7 +57,7 @@\\\\\\\\n from .actor import DataParallelPPOActor\\\\\\\\n from .config import ActorConfig, CriticConfig, FSDPConfig, ModelConfig, OptimConfig, RefConfig, WorkerConfig\\\\\\\\n from .critic import DataParallelPPOCritic\\\\\\\\n-from .rollout.vllm_rollout import vLLMRollout\\\\\\\\n+from .rollout import vLLMRollout\\\\\\\\n from .sharding_manager import FSDPVLLMShardingManager\\\\\\\\n from .sharding_manager.fsdp_ulysses import FSDPUlyssesShardingManager\\\\\\\\n \\\\\\\\n@@ -75,6 +75,10 @@ def __init__(\\\\\\\\n         if not dist.is_initialized():\\\\\\\\n             dist.init_process_group(backend=\\\\\\\\\\\\\\\"nccl\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n+        # improve numerical stability\\\\\\\\n+        torch.backends.cuda.matmul.allow_tf32 = False\\\\\\\\n+        torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\\\\\\\\n+\\\\\\\\n         self._is_actor = self.role in [\\\\\\\\\\\\\\\"actor\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"actor_rollout\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"actor_rollout_ref\\\\\\\\\\\\\\\"]\\\\\\\\n         self._is_critic = self.role == \\\\\\\\\\\\\\\"critic\\\\\\\\\\\\\\\"\\\\\\\\n         self._is_rollout = self.role in [\\\\\\\\\\\\\\\"rollout\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"actor_rollout\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"actor_rollout_ref\\\\\\\\\\\\\\\"]\\\\\\\\n@@ -131,7 +135,7 @@ def _init_config(\\\\\\\\n             config.global_batch_size // self.device_mesh.size() * config.ulysses_sequence_parallel_size\\\\\\\\n         )\\\\\\\\n         if config.global_batch_size_per_device == 0:\\\\\\\\n-            raise ValueError(f\\\\\\\\\\\\\\\"{role} global batch size must be larger than num gpus.\\\\\\\\\\\\\\\")\\\\\\\\n+            raise ValueError(f\\\\\\\\\\\\\\\"{role} global batch size * ulysses size must be larger than num gpus.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         if config.global_batch_size_per_device % config.micro_batch_size_per_device_for_update != 0:\\\\\\\\n             raise ValueError(f\\\\\\\\\\\\\\\"{role} global batch size per device must be divisible by the micro batch size.\\\\\\\\\\\\\\\")\\\\\\\\n@@ -413,7 +417,7 @@ def load_checkpoint(self, path: str):\\\\\\\\n         if self._use_param_offload:\\\\\\\\n             offload_fsdp_model(self.fsdp_module)\\\\\\\\n \\\\\\\\n-        if self._use_optimizer_offload:\\\\\\\\n+        if self._use_optimizer_offload:  # avoid OOM in resuming\\\\\\\\n             offload_fsdp_optimizer(self.optimizer)\\\\\\\\n \\\\\\\\n     @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"e7d5c616db2dfbccd7720ec4be1bf80930e0232f\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/reward/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":3,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Fworkers%2Freward%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Fworkers%2Freward%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Freward%2Fconfig.py?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -21,4 +21,5 @@\\\\\\\\n @dataclass\\\\\\\\n class RewardConfig:\\\\\\\\n     reward_type: str = \\\\\\\\\\\\\\\"function\\\\\\\\\\\\\\\"\\\\\\\\n-    compute_score: str = \\\\\\\\\\\\\\\"math\\\\\\\\\\\\\\\"\\\\\\\\n+    score_function: str = \\\\\\\\\\\\\\\"math\\\\\\\\\\\\\\\"\\\\\\\\n+    skip_special_tokens: bool = True\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"43d8ea9cab708130c16746cc71162fcf36b195b3\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/reward/custom.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":9,\\\\\\\"deletions\\\\\\\":5,\\\\\\\"changes\\\\\\\":14,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Fworkers%2Freward%2Fcustom.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Fworkers%2Freward%2Fcustom.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Freward%2Fcustom.py?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -21,6 +21,7 @@\\\\\\\\n \\\\\\\\n from ...protocol import DataProto\\\\\\\\n from ...utils.reward_score import math_compute_score, r1v_compute_score\\\\\\\\n+from .config import RewardConfig\\\\\\\\n \\\\\\\\n \\\\\\\\n class RewardScore(TypedDict):\\\\\\\\n@@ -30,14 +31,15 @@ class RewardScore(TypedDict):\\\\\\\\n \\\\\\\\n \\\\\\\\n class CustomRewardManager:\\\\\\\\n-    def __init__(self, tokenizer: PreTrainedTokenizer, compute_score: str):\\\\\\\\n+    def __init__(self, tokenizer: PreTrainedTokenizer, config: RewardConfig):\\\\\\\\n+        self.config = config\\\\\\\\n         self.tokenizer = tokenizer\\\\\\\\n-        if compute_score == \\\\\\\\\\\\\\\"math\\\\\\\\\\\\\\\":\\\\\\\\n+        if config.score_function == \\\\\\\\\\\\\\\"math\\\\\\\\\\\\\\\":\\\\\\\\n             self.compute_score: Callable[[str, str], RewardScore] = math_compute_score\\\\\\\\n-        elif compute_score == \\\\\\\\\\\\\\\"r1v\\\\\\\\\\\\\\\":\\\\\\\\n+        elif config.score_function == \\\\\\\\\\\\\\\"r1v\\\\\\\\\\\\\\\":\\\\\\\\n             self.compute_score: Callable[[str, str], RewardScore] = r1v_compute_score\\\\\\\\n         else:\\\\\\\\n-            raise NotImplementedError()\\\\\\\\n+            raise NotImplementedError(f\\\\\\\\\\\\\\\"Unknown score function {config.score_function}.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n     def __call__(self, data: DataProto) -> Tuple[torch.Tensor, Dict[str, List[float]]]:\\\\\\\\n         reward_tensor = torch.zeros_like(data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"], dtype=torch.float32)\\\\\\\\n@@ -49,7 +51,9 @@ def __call__(self, data: DataProto) -> Tuple[torch.Tensor, Dict[str, List[float]\\\\\\\\n             valid_response_length = response_mask.sum()\\\\\\\\n             valid_response_ids = response_ids[:valid_response_length]\\\\\\\\n \\\\\\\\n-            response_str = self.tokenizer.decode(valid_response_ids, skip_special_tokens=False)\\\\\\\\n+            response_str = self.tokenizer.decode(\\\\\\\\n+                valid_response_ids, skip_special_tokens=self.config.skip_special_tokens\\\\\\\\n+            )\\\\\\\\n             ground_truth = data_item.non_tensor_batch[\\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n             score = self.compute_score(response_str, ground_truth)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"b294bcea67d0caae8e4cba6c99dcfd8221440f7a\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/rollout/__init__.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":3,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Fworkers%2Frollout%2F__init__.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Fworkers%2Frollout%2F__init__.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Frollout%2F__init__.py?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -14,6 +14,7 @@\\\\\\\\n \\\\\\\\n \\\\\\\\n from .config import RolloutConfig\\\\\\\\n+from .vllm_rollout_spmd import vLLMRollout\\\\\\\\n \\\\\\\\n \\\\\\\\n-__all__ = [\\\\\\\\\\\\\\\"RolloutConfig\\\\\\\\\\\\\\\"]\\\\\\\\n+__all__ = [\\\\\\\\\\\\\\\"RolloutConfig\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"vLLMRollout\\\\\\\\\\\\\\\"]\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"a9060b4513170d7f68027233c89489a1022c7652\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/rollout/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":5,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Fworkers%2Frollout%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Fworkers%2Frollout%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Frollout%2Fconfig.py?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -28,11 +28,10 @@ class RolloutConfig:\\\\\\\\n     top_k: int = -1\\\\\\\\n     limit_images: int = 0\\\\\\\\n     dtype: str = \\\\\\\\\\\\\\\"bf16\\\\\\\\\\\\\\\"\\\\\\\\n-    gpu_memory_utilization: float = 0.5\\\\\\\\n+    gpu_memory_utilization: float = 0.6\\\\\\\\n     ignore_eos: bool = False\\\\\\\\n     enforce_eager: bool = False\\\\\\\\n-    free_cache_engine: bool = False\\\\\\\\n-    enable_chunked_prefill: bool = False\\\\\\\\n+    enable_chunked_prefill: bool = False  # only for v0 engine\\\\\\\\n     tensor_parallel_size: int = 2\\\\\\\\n     max_num_batched_tokens: int = 8192\\\\\\\\n     max_num_seqs: int = 1024\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"7fc316182cb7eb3e92238a20fa063bfd1a8ae7b3\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/rollout/vllm_rollout/__init__.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"removed\\\\\\\",\\\\\\\"additions\\\\\\\":0,\\\\\\\"deletions\\\\\\\":18,\\\\\\\"changes\\\\\\\":18,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/19cb1b7315a7012e4fdf357e64707263c21aeb44/verl%2Fworkers%2Frollout%2Fvllm_rollout%2F__init__.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/19cb1b7315a7012e4fdf357e64707263c21aeb44/verl%2Fworkers%2Frollout%2Fvllm_rollout%2F__init__.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Frollout%2Fvllm_rollout%2F__init__.py?ref=19cb1b7315a7012e4fdf357e64707263c21aeb44\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -1,18 +0,0 @@\\\\\\\\n-# Copyright 2024 Bytedance Ltd. and/or its affiliates\\\\\\\\n-#\\\\\\\\n-# Licensed under the Apache License, Version 2.0 (the \\\\\\\\\\\\\\\"License\\\\\\\\\\\\\\\");\\\\\\\\n-# you may not use this file except in compliance with the License.\\\\\\\\n-# You may obtain a copy of the License at\\\\\\\\n-#\\\\\\\\n-#     http://www.apache.org/licenses/LICENSE-2.0\\\\\\\\n-#\\\\\\\\n-# Unless required by applicable law or agreed to in writing, software\\\\\\\\n-# distributed under the License is distributed on an \\\\\\\\\\\\\\\"AS IS\\\\\\\\\\\\\\\" BASIS,\\\\\\\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\\\\\\\n-# See the License for the specific language governing permissions and\\\\\\\\n-# limitations under the License.\\\\\\\\n-\\\\\\\\n-from .vllm_rollout_spmd import vLLMRollout\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-__all__ = [\\\\\\\\\\\\\\\"vLLMRollout\\\\\\\\\\\\\\\"]\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"30225161b6a002a620518e27b07109e18bf2c17d\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"renamed\\\\\\\",\\\\\\\"additions\\\\\\\":5,\\\\\\\"deletions\\\\\\\":8,\\\\\\\"changes\\\\\\\":13,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Fworkers%2Frollout%2Fvllm_rollout_spmd.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5/verl%2Fworkers%2Frollout%2Fvllm_rollout_spmd.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Frollout%2Fvllm_rollout_spmd.py?ref=4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -29,11 +29,11 @@\\\\\\\\n from transformers import PreTrainedTokenizer\\\\\\\\n from vllm import LLM, RequestOutput, SamplingParams\\\\\\\\n \\\\\\\\n-from ....protocol import DataProto\\\\\\\\n-from ....utils import torch_functional as VF\\\\\\\\n-from ....utils.torch_dtypes import PrecisionType\\\\\\\\n-from ..base import BaseRollout\\\\\\\\n-from ..config import RolloutConfig\\\\\\\\n+from ...protocol import DataProto\\\\\\\\n+from ...utils import torch_functional as VF\\\\\\\\n+from ...utils.torch_dtypes import PrecisionType\\\\\\\\n+from .base import BaseRollout\\\\\\\\n+from .config import RolloutConfig\\\\\\\\n \\\\\\\\n \\\\\\\\n def _repeat_interleave(value: Union[torch.Tensor, np.ndarray], repeats: int) -> Union[torch.Tensor, List[Any]]:\\\\\\\\n@@ -59,9 +59,6 @@ def __init__(self, model_path: str, config: RolloutConfig, tokenizer: PreTrained\\\\\\\\n         if config.tensor_parallel_size > torch.distributed.get_world_size():\\\\\\\\n             raise ValueError(\\\\\\\\\\\\\\\"Tensor parallelism size should be less than world size.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n-        if not config.enforce_eager and config.free_cache_engine:\\\\\\\\n-            raise ValueError(\\\\\\\\\\\\\\\"CUDA graph should be disabled when `free_cache_engine` is True.\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n         if config.max_num_batched_tokens < config.prompt_length + config.response_length:\\\\\\\\n             raise ValueError(\\\\\\\\\\\\\\\"max_num_batched_tokens should be greater than prompt_length + response_length.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\",\\\\\\\"previous_filename\\\\\\\":\\\\\\\"verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\\\\\\\"}],\\\\\\\"sha\\\\\\\":\\\\\\\"4ab7a324c4395d796f245ddba3ecc81fd8dbc6a5\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"committer\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"noreply@github.com\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-03T20:26:53Z\\\\\\\"},\\\\\\\"author\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"hiyouga@buaa.edu.cn\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-03T20:26:53Z\\\\\\\"}}},\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "I've been searching through many commits, but I haven't found the specific commit that shows the change from `micro_batch_size_per_device_for_update: 4` to `micro_batch_size_per_device_for_update: 1` in the `examples/config.yaml` file. From all my analysis so far, I can see that:\n\n1. Commit `6c0bba7ebc1dcec2101ca6a620db503c108c6f9e` shows these values as `micro_batch_size_per_device_for_update: 4` and `micro_batch_size_per_device_for_experience: 16`\n2. Many shell scripts use override values like `worker.actor.micro_batch_size_per_device_for_update=1` and `worker.actor.micro_batch_size_per_device_for_experience=8` or `worker.actor.micro_batch_size_per_device_for_experience=2`\n\nBased on the task requirements, I need to find a commit that actually changes the base config file. Let me check the current state of the config file to see if it has changed to have the value `1`:",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_REPOS\", \"action_name\": \"get_commit\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\", \\\"ref\\\": \\\"4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\"}\", \"include_output_fields\": [\"sha\", \"commit.author\", \"commit.committer\", \"files\"]}",
    "call_id": "toolu_01JzrtdLYKVXCUSWSKZqLupj",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01JzrtdLYKVXCUSWSKZqLupj",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"get_commit\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":{\\\\\\\"files\\\\\\\":[{\\\\\\\"sha\\\\\\\":\\\\\\\"6cbcc829ea595214fdaba9f09b18bc39cc666cb8\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"README.md\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/README.md\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/README.md\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/README.md?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -33,7 +33,7 @@ EasyR1 is efficient and scalable due to the design of **[HybirdEngine](https://a\\\\\\\\n ### Software Requirements\\\\\\\\n \\\\\\\\n - Python 3.9+\\\\\\\\n-- transformers>=4.49.0\\\\\\\\n+- transformers>=4.51.0\\\\\\\\n - flash-attn>=2.4.3\\\\\\\\n - vllm>=0.8.3\\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"25d3cb8a1852516afcb9e77bf6ac99217a8e39b1\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/baselines/qwen2_5_vl_3b_clevr.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/examples%2Fbaselines%2Fqwen2_5_vl_3b_clevr.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/examples%2Fbaselines%2Fqwen2_5_vl_3b_clevr.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fbaselines%2Fqwen2_5_vl_3b_clevr.sh?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -14,6 +14,6 @@ python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     data.format_prompt=\\\\\\\\\\\\\\\"${FORMAT_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.tensor_parallel_size=1 \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.reward.score_function=r1v \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.reward.score_function=./examples/score_function/r1v.py:compute_score \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.experiment_name=qwen2_5_vl_3b_clevr \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.n_gpus_per_node=2\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"2aa6dcd6d6f3c8d7ceda79d2adde3574743b2b9e\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/baselines/qwen2_5_vl_3b_geoqa8k.sh\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/examples%2Fbaselines%2Fqwen2_5_vl_3b_geoqa8k.sh\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/examples%2Fbaselines%2Fqwen2_5_vl_3b_geoqa8k.sh\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fbaselines%2Fqwen2_5_vl_3b_geoqa8k.sh?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -14,6 +14,6 @@ python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n     data.format_prompt=\\\\\\\\\\\\\\\"${FORMAT_PROMPT}\\\\\\\\\\\\\\\" \\\\\\\\\\\\\\\\\\\\\\\\n     worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n     worker.rollout.tensor_parallel_size=1 \\\\\\\\\\\\\\\\\\\\\\\\n-    worker.reward.score_function=r1v \\\\\\\\\\\\\\\\\\\\\\\\n+    worker.reward.score_function=./examples/score_function/r1v.py:compute_score \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.experiment_name=qwen2_5_vl_3b_geoqa8k \\\\\\\\\\\\\\\\\\\\\\\\n     trainer.n_gpus_per_node=8\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"7e3d0df454acc120cf54fe4ca8811eac7974ef7c\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/config.yaml\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":3,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":6,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/examples%2Fconfig.yaml\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fconfig.yaml?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -47,8 +47,9 @@ worker:\\\\\\\\n       offload_optimizer: true  # true: more CPU memory; false: more GPU memory\\\\\\\\n \\\\\\\\n   rollout:\\\\\\\\n-    temperature: 1.0\\\\\\\\n     n: 5\\\\\\\\n+    temperature: 1.0\\\\\\\\n+    top_p: 0.99\\\\\\\\n     gpu_memory_utilization: 0.6\\\\\\\\n     enforce_eager: false\\\\\\\\n     enable_chunked_prefill: false\\\\\\\\n@@ -68,8 +69,7 @@ worker:\\\\\\\\n \\\\\\\\n   reward:\\\\\\\\n     reward_type: function\\\\\\\\n-    score_function: math\\\\\\\\n-    skip_special_tokens: true\\\\\\\\n+    score_function: ./examples/score_function/math.py:compute_score\\\\\\\\n \\\\\\\\n trainer:\\\\\\\\n   total_episodes: 15\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"0e13511ab5ad0561b955dc276e810f6f8e729bab\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/score_function/math.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"renamed\\\\\\\",\\\\\\\"additions\\\\\\\":8,\\\\\\\"deletions\\\\\\\":8,\\\\\\\"changes\\\\\\\":16,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/examples%2Fscore_function%2Fmath.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/examples%2Fscore_function%2Fmath.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fscore_function%2Fmath.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -18,23 +18,23 @@\\\\\\\\n from mathruler.grader import extract_boxed_content, grade_answer\\\\\\\\n \\\\\\\\n \\\\\\\\n-def math_format_reward(predict_str: str) -> float:\\\\\\\\n+def format_reward(predict_str: str) -> float:\\\\\\\\n     pattern = re.compile(r\\\\\\\\\\\\\\\"<think>.*</think>.*\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\boxed\\\\\\\\\\\\\\\\{.*\\\\\\\\\\\\\\\\}.*\\\\\\\\\\\\\\\", re.DOTALL)\\\\\\\\n     format_match = re.fullmatch(pattern, predict_str)\\\\\\\\n     return 1.0 if format_match else 0.0\\\\\\\\n \\\\\\\\n \\\\\\\\n-def math_acc_reward(predict_str: str, ground_truth: str) -> float:\\\\\\\\n+def accuracy_reward(predict_str: str, ground_truth: str) -> float:\\\\\\\\n     answer = extract_boxed_content(predict_str)\\\\\\\\n     return 1.0 if grade_answer(answer, ground_truth) else 0.0\\\\\\\\n \\\\\\\\n \\\\\\\\n-def math_compute_score(predict_str: str, ground_truth: str) -> Dict[str, float]:\\\\\\\\n+def compute_score(predict_str: str, ground_truth: str, format_weight: float = 0.1) -> Dict[str, float]:\\\\\\\\n     predict_str = re.sub(r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\s*(<|>|/)\\\\\\\\\\\\\\\\s*\\\\\\\\\\\\\\\", r\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\1\\\\\\\\\\\\\\\", predict_str)  # handle qwen2.5vl-32b format\\\\\\\\n-    format = math_format_reward(predict_str)\\\\\\\\n-    accuracy = math_acc_reward(predict_str, ground_truth)\\\\\\\\n+    format_score = format_reward(predict_str)\\\\\\\\n+    accuracy_score = accuracy_reward(predict_str, ground_truth)\\\\\\\\n     return {\\\\\\\\n-        \\\\\\\\\\\\\\\"overall\\\\\\\\\\\\\\\": 0.9 * accuracy + 0.1 * format,\\\\\\\\n-        \\\\\\\\\\\\\\\"format\\\\\\\\\\\\\\\": format,\\\\\\\\n-        \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\": accuracy,\\\\\\\\n+        \\\\\\\\\\\\\\\"overall\\\\\\\\\\\\\\\": (1 - format_weight) * accuracy_score + format_weight * format_score,\\\\\\\\n+        \\\\\\\\\\\\\\\"format\\\\\\\\\\\\\\\": format_score,\\\\\\\\n+        \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\": accuracy_score,\\\\\\\\n     }\\\\\\\",\\\\\\\"previous_filename\\\\\\\":\\\\\\\"verl/utils/reward_score/math.py\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"3db4feece8b2a10ff1500101ecebdc5da05e4e7b\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"examples/score_function/r1v.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"renamed\\\\\\\",\\\\\\\"additions\\\\\\\":9,\\\\\\\"deletions\\\\\\\":10,\\\\\\\"changes\\\\\\\":19,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/examples%2Fscore_function%2Fr1v.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/examples%2Fscore_function%2Fr1v.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/examples%2Fscore_function%2Fr1v.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -18,18 +18,17 @@\\\\\\\\n from mathruler.grader import grade_answer\\\\\\\\n \\\\\\\\n \\\\\\\\n-def r1v_format_reward(predict_str: str) -> float:\\\\\\\\n+def format_reward(predict_str: str) -> float:\\\\\\\\n     pattern = re.compile(r\\\\\\\\\\\\\\\"<think>.*?</think>\\\\\\\\\\\\\\\\s*<answer>.*?</answer>\\\\\\\\\\\\\\\", re.DOTALL)\\\\\\\\n     format_match = re.fullmatch(pattern, predict_str)\\\\\\\\n     return 1.0 if format_match else 0.0\\\\\\\\n \\\\\\\\n \\\\\\\\n-def r1v_accuracy_reward(predict_str: str, ground_truth: str) -> float:\\\\\\\\n+def accuracy_reward(predict_str: str, ground_truth: str) -> float:\\\\\\\\n     try:\\\\\\\\n-        ground_truth = ground_truth.strip()\\\\\\\\n         content_match = re.search(r\\\\\\\\\\\\\\\"<answer>(.*?)</answer>\\\\\\\\\\\\\\\", predict_str)\\\\\\\\n         given_answer = content_match.group(1).strip() if content_match else predict_str.strip()\\\\\\\\n-        if grade_answer(given_answer, ground_truth):\\\\\\\\n+        if grade_answer(given_answer, ground_truth.strip()):\\\\\\\\n             return 1.0\\\\\\\\n \\\\\\\\n     except Exception:\\\\\\\\n@@ -38,11 +37,11 @@ def r1v_accuracy_reward(predict_str: str, ground_truth: str) -> float:\\\\\\\\n     return 0.0\\\\\\\\n \\\\\\\\n \\\\\\\\n-def r1v_compute_score(predict_str: str, ground_truth: str) -> Dict[str, float]:\\\\\\\\n-    format = r1v_format_reward(predict_str)\\\\\\\\n-    accuracy = r1v_accuracy_reward(predict_str, ground_truth)\\\\\\\\n+def compute_score(predict_str: str, ground_truth: str, format_weight: float = 0.5) -> Dict[str, float]:\\\\\\\\n+    format_score = format_reward(predict_str)\\\\\\\\n+    accuracy_score = accuracy_reward(predict_str, ground_truth)\\\\\\\\n     return {\\\\\\\\n-        \\\\\\\\\\\\\\\"overall\\\\\\\\\\\\\\\": 0.5 * accuracy + 0.5 * format,\\\\\\\\n-        \\\\\\\\\\\\\\\"format\\\\\\\\\\\\\\\": format,\\\\\\\\n-        \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\": accuracy,\\\\\\\\n+        \\\\\\\\\\\\\\\"overall\\\\\\\\\\\\\\\": (1 - format_weight) * accuracy_score + format_weight * format_score,\\\\\\\\n+        \\\\\\\\\\\\\\\"format\\\\\\\\\\\\\\\": format_score,\\\\\\\\n+        \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\": accuracy_score,\\\\\\\\n     }\\\\\\\",\\\\\\\"previous_filename\\\\\\\":\\\\\\\"verl/utils/reward_score/r1v.py\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"1a77edd37b98b37c6007600b1e06b20a1494b357\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"requirements.txt\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/requirements.txt\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/requirements.txt\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/requirements.txt?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -15,6 +15,6 @@ qwen-vl-utils\\\\\\\\n ray[default]\\\\\\\\n tensordict\\\\\\\\n torchdata\\\\\\\\n-transformers>=4.49.0\\\\\\\\n+transformers>=4.51.0\\\\\\\\n vllm>=0.7.3\\\\\\\\n wandb\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"65d48be0d82279099b2bddb85f8d810856325c8a\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/protocol.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":16,\\\\\\\"deletions\\\\\\\":16,\\\\\\\"changes\\\\\\\":32,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fprotocol.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fprotocol.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fprotocol.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -136,8 +136,8 @@ def fold_batch_dim(data: \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\", new_batch_size: int):\\\\\\\\n     tensor = tensor.view(new_batch_size, -1)\\\\\\\\n     tensor.auto_batch_size_(batch_dims=1)\\\\\\\\n \\\\\\\\n-    for key, val in non_tensor.items():\\\\\\\\n-        non_tensor[key] = np.reshape(val, newshape=(new_batch_size, -1, *val.shape[1:]))\\\\\\\\n+    for key, value in non_tensor.items():\\\\\\\\n+        non_tensor[key] = np.reshape(value, newshape=(new_batch_size, -1, *value.shape[1:]))\\\\\\\\n \\\\\\\\n     return DataProto(batch=tensor, non_tensor_batch=non_tensor, meta_info=data.meta_info)\\\\\\\\n \\\\\\\\n@@ -182,14 +182,14 @@ def __len__(self) -> int:\\\\\\\\n         if self.batch is not None:\\\\\\\\n             return self.batch.batch_size[0]\\\\\\\\n         elif self.non_tensor_batch is not None and len(self.non_tensor_batch) > 0:\\\\\\\\n-            random_key = list(self.non_tensor_batch.keys())[0]\\\\\\\\n-            return self.non_tensor_batch[random_key].shape[0]\\\\\\\\n+            pivot_key = list(self.non_tensor_batch.keys())[0]\\\\\\\\n+            return self.non_tensor_batch[pivot_key].shape[0]\\\\\\\\n         else:\\\\\\\\n             return 0\\\\\\\\n \\\\\\\\n     def __getitem__(self, item: Union[int, slice]) -> Union[\\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"DataProtoItem\\\\\\\\\\\\\\\"]:\\\\\\\\n         tensor_data = self.batch[item]\\\\\\\\n-        non_tensor_data = {key: val[item] for key, val in self.non_tensor_batch.items()}\\\\\\\\n+        non_tensor_data = {key: value[item] for key, value in self.non_tensor_batch.items()}\\\\\\\\n         return_type = DataProto if isinstance(item, slice) else DataProtoItem\\\\\\\\n         return return_type(batch=tensor_data, non_tensor_batch=non_tensor_data, meta_info=self.meta_info)\\\\\\\\n \\\\\\\\n@@ -223,9 +223,10 @@ def load_from_disk(filepath: str) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n \\\\\\\\n     def print_size(self, prefix: str = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\") -> None:\\\\\\\\n         size_of_tensordict = 0\\\\\\\\n-        for tensor in self.batch.values():\\\\\\\\n-            if isinstance(tensor, torch.Tensor):\\\\\\\\n-                size_of_tensordict += tensor.element_size() * tensor.numel()\\\\\\\\n+        if self.batch is not None:\\\\\\\\n+            for tensor in self.batch.values():\\\\\\\\n+                if isinstance(tensor, torch.Tensor):\\\\\\\\n+                    size_of_tensordict += tensor.element_size() * tensor.numel()\\\\\\\\n \\\\\\\\n         size_of_numpy_array = 0\\\\\\\\n         for value in self.non_tensor_batch.values():\\\\\\\\n@@ -249,17 +250,16 @@ def check_consistency(self):\\\\\\\\n             assert len(self.batch.batch_size) == 1, \\\\\\\\\\\\\\\"only support num_batch_dims=1 when non_tensor_batch is not empty.\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n             batch_size = self.batch.batch_size[0]\\\\\\\\n-            for key, val in self.non_tensor_batch.items():\\\\\\\\n-                assert len(val) == batch_size, f\\\\\\\\\\\\\\\"key {key} length {len(val)} is not equal to batch size {batch_size}.\\\\\\\\\\\\\\\"\\\\\\\\n+            for key, value in self.non_tensor_batch.items():\\\\\\\\n+                assert len(value) == batch_size, f\\\\\\\\\\\\\\\"key {key} length {len(value)} is not equal to bsz {batch_size}.\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n     @classmethod\\\\\\\\n     def from_single_dict(\\\\\\\\n         cls,\\\\\\\\n         data: Dict[str, Union[torch.Tensor, NDArray]],\\\\\\\\n         meta_info: Optional[Dict[str, Any]] = None,\\\\\\\\n     ) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n-        tensors = {}\\\\\\\\n-        non_tensors = {}\\\\\\\\n+        tensors, non_tensors = {}, {}\\\\\\\\n         for key, value in data.items():\\\\\\\\n             if isinstance(value, torch.Tensor):\\\\\\\\n                 tensors[key] = value\\\\\\\\n@@ -551,7 +551,7 @@ def reorder(self, indices: torch.Tensor) -> None:\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n         indices_np = indices.detach().numpy()\\\\\\\\n         self.batch = self.batch[indices]\\\\\\\\n-        self.non_tensor_batch = {key: val[indices_np] for key, val in self.non_tensor_batch.items()}\\\\\\\\n+        self.non_tensor_batch = {key: value[indices_np] for key, value in self.non_tensor_batch.items()}\\\\\\\\n \\\\\\\\n     def repeat(self, repeat_times: int = 2, interleave: bool = True) -> \\\\\\\\\\\\\\\"DataProto\\\\\\\\\\\\\\\":\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n@@ -666,9 +666,9 @@ def allgather_dict_tensors(\\\\\\\\n     output = {}\\\\\\\\n     sorted_keys = sorted(tensors_as_dict.keys())\\\\\\\\n     for key in sorted_keys:\\\\\\\\n-        val = tensors_as_dict[key]\\\\\\\\n-        output[key] = [torch.empty_like(val) for _ in range(size)]\\\\\\\\n-        torch.distributed.all_gather(output[key], val, group=group, async_op=False)\\\\\\\\n+        value = tensors_as_dict[key]\\\\\\\\n+        output[key] = [torch.empty_like(value) for _ in range(size)]\\\\\\\\n+        torch.distributed.all_gather(output[key], value, group=group, async_op=False)\\\\\\\\n         output[key] = torch.cat(output[key], dim=dim)\\\\\\\\n \\\\\\\\n     if is_tensor_dict:\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"5f9560d049eff4e8df3d1e3de306d9f79329a21c\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":4,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Ftrainer%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Ftrainer%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fconfig.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -86,6 +86,10 @@ def post_init(self):\\\\\\\\n         if self.save_checkpoint_path is None:\\\\\\\\n             self.save_checkpoint_path = os.path.join(\\\\\\\\\\\\\\\"checkpoints\\\\\\\\\\\\\\\", self.project_name, self.experiment_name)\\\\\\\\n \\\\\\\\n+        self.save_checkpoint_path = os.path.abspath(self.save_checkpoint_path)\\\\\\\\n+        if self.load_checkpoint_path is not None:\\\\\\\\n+            self.load_checkpoint_path = os.path.abspath(self.load_checkpoint_path)\\\\\\\\n+\\\\\\\\n \\\\\\\\n @dataclass\\\\\\\\n class PPOConfig:\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"fe64118137e8eb0199d3636dd3985b4466f51dc4\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/main.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":13,\\\\\\\"deletions\\\\\\\":10,\\\\\\\"changes\\\\\\\":23,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Ftrainer%2Fmain.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Ftrainer%2Fmain.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fmain.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -11,9 +11,6 @@\\\\\\\\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\\\\\\\n # See the License for the specific language governing permissions and\\\\\\\\n # limitations under the License.\\\\\\\\n-\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-Note that we don't combine the main with ray_trainer as ray_trainer is used by other main.\\\\\\\\n-\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n import json\\\\\\\\n \\\\\\\\n@@ -23,7 +20,7 @@\\\\\\\\n from ..single_controller.ray import RayWorkerGroup\\\\\\\\n from ..utils.tokenizer import get_processor, get_tokenizer\\\\\\\\n from ..workers.fsdp_workers import FSDPWorker\\\\\\\\n-from ..workers.reward import CustomRewardManager\\\\\\\\n+from ..workers.reward import FunctionRewardManager\\\\\\\\n from .config import PPOConfig\\\\\\\\n from .ray_trainer import RayPPOTrainer, ResourcePoolManager, Role\\\\\\\\n \\\\\\\\n@@ -35,7 +32,6 @@ class Runner:\\\\\\\\n \\\\\\\\n     def run(self, config: PPOConfig):\\\\\\\\n         # print config\\\\\\\\n-        config.deep_post_init()\\\\\\\\n         print(json.dumps(config.to_dict(), indent=2))\\\\\\\\n \\\\\\\\n         # instantiate tokenizer\\\\\\\\n@@ -68,8 +64,8 @@ def run(self, config: PPOConfig):\\\\\\\\n         }\\\\\\\\n         resource_pool_manager = ResourcePoolManager(resource_pool_spec=resource_pool_spec, mapping=mapping)\\\\\\\\n \\\\\\\\n-        reward_fn = CustomRewardManager(tokenizer=tokenizer, config=config.worker.reward)\\\\\\\\n-        val_reward_fn = CustomRewardManager(tokenizer=tokenizer, config=config.worker.reward)\\\\\\\\n+        reward_fn = FunctionRewardManager(config=config.worker.reward, tokenizer=tokenizer)\\\\\\\\n+        val_reward_fn = FunctionRewardManager(config=config.worker.reward, tokenizer=tokenizer)\\\\\\\\n \\\\\\\\n         trainer = RayPPOTrainer(\\\\\\\\n             config=config,\\\\\\\\n@@ -95,11 +91,18 @@ def main():\\\\\\\\n         default_config = OmegaConf.merge(default_config, file_config)\\\\\\\\n \\\\\\\\n     ppo_config = OmegaConf.merge(default_config, cli_args)\\\\\\\\n-    ppo_config = OmegaConf.to_object(ppo_config)\\\\\\\\n+    ppo_config: PPOConfig = OmegaConf.to_object(ppo_config)\\\\\\\\n+    ppo_config.deep_post_init()\\\\\\\\n \\\\\\\\n     if not ray.is_initialized():\\\\\\\\n-        # this is for local ray cluster\\\\\\\\n-        ray.init(runtime_env={\\\\\\\\\\\\\\\"env_vars\\\\\\\\\\\\\\\": {\\\\\\\\\\\\\\\"TOKENIZERS_PARALLELISM\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"true\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"NCCL_DEBUG\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"WARN\\\\\\\\\\\\\\\"}})\\\\\\\\n+        runtime_env = {\\\\\\\\n+            \\\\\\\\\\\\\\\"env_vars\\\\\\\\\\\\\\\": {\\\\\\\\n+                \\\\\\\\\\\\\\\"TOKENIZERS_PARALLELISM\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"true\\\\\\\\\\\\\\\",\\\\\\\\n+                \\\\\\\\\\\\\\\"NCCL_DEBUG\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"WARN\\\\\\\\\\\\\\\",\\\\\\\\n+                \\\\\\\\\\\\\\\"VLLM_LOGGING_LEVEL\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"INFO\\\\\\\\\\\\\\\",\\\\\\\\n+            }\\\\\\\\n+        }\\\\\\\\n+        ray.init(runtime_env=runtime_env)\\\\\\\\n \\\\\\\\n     runner = Runner.remote()\\\\\\\\n     ray.get(runner.run.remote(ppo_config))\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"02cd2335f9ac1aada4fcbe795940d94d5b99281b\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/metrics.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Ftrainer%2Fmetrics.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Ftrainer%2Fmetrics.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fmetrics.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -110,11 +110,11 @@ def compute_timing_metrics(batch: DataProto, timing_raw: Dict[str, float]) -> Di\\\\\\\\n     }\\\\\\\\n \\\\\\\\n \\\\\\\\n-def compute_throughout_metrics(batch: DataProto, timing_raw: Dict[str, float], n_gpus: int) -> Dict[str, Any]:\\\\\\\\n+def compute_throughout_metrics(batch: DataProto, timing_raw: Dict[str, float], num_gpus: int) -> Dict[str, Any]:\\\\\\\\n     total_num_tokens = sum(batch.meta_info[\\\\\\\\\\\\\\\"global_token_num\\\\\\\\\\\\\\\"])\\\\\\\\n     time = timing_raw[\\\\\\\\\\\\\\\"step\\\\\\\\\\\\\\\"]\\\\\\\\n     return {\\\\\\\\n         \\\\\\\\\\\\\\\"perf/total_num_tokens\\\\\\\\\\\\\\\": total_num_tokens,\\\\\\\\n         \\\\\\\\\\\\\\\"perf/time_per_step\\\\\\\\\\\\\\\": time,\\\\\\\\n-        \\\\\\\\\\\\\\\"perf/throughput\\\\\\\\\\\\\\\": total_num_tokens / (time * n_gpus),\\\\\\\\n+        \\\\\\\\\\\\\\\"perf/throughput\\\\\\\\\\\\\\\": total_num_tokens / (time * num_gpus),\\\\\\\\n     }\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"8903416029cf20871b03236530540ad834768b4a\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/trainer/ray_trainer.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":14,\\\\\\\"deletions\\\\\\\":24,\\\\\\\"changes\\\\\\\":38,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Ftrainer%2Fray_trainer.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Ftrainer%2Fray_trainer.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -102,24 +102,16 @@ def get_resource_pool(self, role: Role) -> RayResourcePool:\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Get the resource pool of the worker.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n         return self.resource_pool_dict[self.mapping[role]]\\\\\\\\n \\\\\\\\n-    def get_n_gpus(self) -> int:\\\\\\\\n+    def get_num_gpus(self) -> int:\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Get the number of gpus in this cluster.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n         return sum([n_gpus for process_on_nodes in self.resource_pool_spec.values() for n_gpus in process_on_nodes])\\\\\\\\n \\\\\\\\n     def _check_resource_available(self):\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Check if the resource pool can be satisfied in this ray cluster.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n-        node_available_resources = ray.state.available_resources_per_node()\\\\\\\\n-        node_available_gpus = {node: node_info.get(\\\\\\\\\\\\\\\"GPU\\\\\\\\\\\\\\\", 0) for node, node_info in node_available_resources.items()}\\\\\\\\n-\\\\\\\\n-        # check total required gpus can be satisfied\\\\\\\\n-        total_available_gpus = sum(node_available_gpus.values())\\\\\\\\n-        total_required_gpus = sum(\\\\\\\\n-            [n_gpus for process_on_nodes in self.resource_pool_spec.values() for n_gpus in process_on_nodes]\\\\\\\\n-        )\\\\\\\\n-        if total_available_gpus < total_required_gpus:\\\\\\\\n-            raise ValueError(\\\\\\\\n-                f\\\\\\\\\\\\\\\"Total available GPUs {total_available_gpus} is less than total desired GPUs {total_required_gpus}.\\\\\\\\\\\\\\\"\\\\\\\\n-            )\\\\\\\\n+        gpus_available = ray.available_resources().get(\\\\\\\\\\\\\\\"GPU\\\\\\\\\\\\\\\", 0)\\\\\\\\n+        gpus_required = self.get_num_gpus()\\\\\\\\n+        if gpus_available < gpus_required:\\\\\\\\n+            raise ValueError(f\\\\\\\\\\\\\\\"Total available GPUs {gpus_available} is less than total desired GPUs {gpus_required}.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n \\\\\\\\n def apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.KLController, kl_penalty=\\\\\\\\\\\\\\\"kl\\\\\\\\\\\\\\\"):\\\\\\\\n@@ -128,11 +120,8 @@ def apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.KLController, kl_penal\\\\\\\\n     response_mask = data.batch[\\\\\\\\\\\\\\\"response_mask\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n     # compute kl between ref_policy and current policy\\\\\\\\n-    if \\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\" in data.batch.keys():\\\\\\\\n-        kld = core_algos.compute_kl(data.batch[\\\\\\\\\\\\\\\"old_log_probs\\\\\\\\\\\\\\\"], data.batch[\\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\"], kl_penalty=kl_penalty)\\\\\\\\n-        kld = kld * response_mask  # (batch_size, response_length)\\\\\\\\n-    else:\\\\\\\\n-        kld = torch.zeros_like(response_mask, dtype=torch.float32)\\\\\\\\n+    kld = core_algos.compute_kl(data.batch[\\\\\\\\\\\\\\\"old_log_probs\\\\\\\\\\\\\\\"], data.batch[\\\\\\\\\\\\\\\"ref_log_probs\\\\\\\\\\\\\\\"], kl_penalty=kl_penalty)\\\\\\\\n+    kld = kld * response_mask  # (batch_size, response_length)\\\\\\\\n \\\\\\\\n     data.batch[\\\\\\\\\\\\\\\"token_level_rewards\\\\\\\\\\\\\\\"] = token_level_scores - kl_ctrl.kl_coef * kld\\\\\\\\n \\\\\\\\n@@ -366,10 +355,10 @@ def _validate(self) -> Dict[str, Any]:\\\\\\\\n             input_texts = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\\\\\\\\n             sample_inputs.extend(input_texts)\\\\\\\\n \\\\\\\\n-            if \\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\" in test_batch.non_tensor_batch.keys():\\\\\\\\n+            if \\\\\\\\\\\\\\\"multi_modal_data\\\\\\\\\\\\\\\" in test_batch.non_tensor_batch.keys():\\\\\\\\n                 test_gen_batch = test_batch.pop(\\\\\\\\n                     batch_keys=[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\"],\\\\\\\\n-                    non_tensor_batch_keys=[\\\\\\\\\\\\\\\"raw_prompt_ids\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"multi_modal_data\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\"],\\\\\\\\n+                    non_tensor_batch_keys=[\\\\\\\\\\\\\\\"raw_prompt_ids\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"multi_modal_data\\\\\\\\\\\\\\\"],\\\\\\\\n                 )\\\\\\\\n             else:\\\\\\\\n                 test_gen_batch = test_batch.pop(\\\\\\\\n@@ -567,10 +556,10 @@ def fit(self):\\\\\\\\n                 batch: DataProto = DataProto.from_single_dict(batch_dict)\\\\\\\\n \\\\\\\\n                 # pop those keys for generation\\\\\\\\n-                if \\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\" in batch.non_tensor_batch.keys():\\\\\\\\n+                if \\\\\\\\\\\\\\\"multi_modal_data\\\\\\\\\\\\\\\" in batch.non_tensor_batch.keys():\\\\\\\\n                     gen_batch = batch.pop(\\\\\\\\n                         batch_keys=[\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"position_ids\\\\\\\\\\\\\\\"],\\\\\\\\n-                        non_tensor_batch_keys=[\\\\\\\\\\\\\\\"raw_prompt_ids\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"multi_modal_data\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\"],\\\\\\\\n+                        non_tensor_batch_keys=[\\\\\\\\\\\\\\\"raw_prompt_ids\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"multi_modal_data\\\\\\\\\\\\\\\"],\\\\\\\\n                     )\\\\\\\\n                 else:\\\\\\\\n                     gen_batch = batch.pop(\\\\\\\\n@@ -604,6 +593,7 @@ def fit(self):\\\\\\\\n                     # repeat to align with repeated responses in rollout\\\\\\\\n                     batch = batch.repeat(repeat_times=self.config.worker.rollout.n, interleave=True)\\\\\\\\n                     batch = batch.union(gen_batch_output)\\\\\\\\n+                    batch.non_tensor_batch.pop(\\\\\\\\\\\\\\\"multi_modal_data\\\\\\\\\\\\\\\", None)\\\\\\\\n \\\\\\\\n                     # compute reward\\\\\\\\n                     with _timer(\\\\\\\\\\\\\\\"reward\\\\\\\\\\\\\\\", timing_raw):\\\\\\\\n@@ -694,10 +684,10 @@ def fit(self):\\\\\\\\n                             self._save_checkpoint()\\\\\\\\n \\\\\\\\n                 # collect metrics\\\\\\\\n-                n_gpus = self.resource_pool_manager.get_n_gpus()\\\\\\\\n+                num_gpus = self.resource_pool_manager.get_num_gpus()\\\\\\\\n                 metrics.update(compute_data_metrics(batch=batch, use_critic=self.use_critic))\\\\\\\\n                 metrics.update(compute_timing_metrics(batch=batch, timing_raw=timing_raw))\\\\\\\\n-                metrics.update(compute_throughout_metrics(batch=batch, timing_raw=timing_raw, n_gpus=n_gpus))\\\\\\\\n+                metrics.update(compute_throughout_metrics(batch=batch, timing_raw=timing_raw, num_gpus=num_gpus))\\\\\\\\n \\\\\\\\n                 self.logger.log(data=metrics, step=self.global_step)\\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"158190a6cd15e598021b13d13fbbb74e3366af7f\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/checkpoint/fsdp_checkpoint_manager.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":27,\\\\\\\"deletions\\\\\\\":47,\\\\\\\"changes\\\\\\\":74,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Futils%2Fcheckpoint%2Ffsdp_checkpoint_manager.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Futils%2Fcheckpoint%2Ffsdp_checkpoint_manager.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fcheckpoint%2Ffsdp_checkpoint_manager.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -13,13 +13,12 @@\\\\\\\\n # limitations under the License.\\\\\\\\n \\\\\\\\n import os\\\\\\\\n-import warnings\\\\\\\\n from typing import Optional, Union\\\\\\\\n \\\\\\\\n import torch\\\\\\\\n import torch.distributed as dist\\\\\\\\n+from torch.distributed.checkpoint.state_dict import StateDictOptions, get_state_dict, set_state_dict\\\\\\\\n from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\\\\\\\\n-from torch.distributed.fsdp import ShardedOptimStateDictConfig, ShardedStateDictConfig, StateDictType\\\\\\\\n from transformers import PreTrainedModel, PreTrainedTokenizer, ProcessorMixin\\\\\\\\n \\\\\\\\n from .checkpoint_manager import BaseCheckpointManager\\\\\\\\n@@ -59,21 +58,18 @@ def load_checkpoint(self, path: Optional[str] = None):\\\\\\\\n         extra_state_path = os.path.join(path, f\\\\\\\\\\\\\\\"extra_state_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n         print(f\\\\\\\\\\\\\\\"[rank-{self.rank}]: Loading from {model_path} and {optim_path} and {extra_state_path}.\\\\\\\\\\\\\\\")\\\\\\\\n         model_state_dict = torch.load(model_path, weights_only=False)\\\\\\\\n-        optimizer_state_dict = torch.load(optim_path, weights_only=False)\\\\\\\\n+        optim_state_dict = torch.load(optim_path, weights_only=False)\\\\\\\\n         extra_state_dict = torch.load(extra_state_path, weights_only=False)\\\\\\\\n-        lr_scheduler_state_dict = extra_state_dict[\\\\\\\\\\\\\\\"lr_scheduler\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n-        state_dict_config = ShardedStateDictConfig(offload_to_cpu=True)\\\\\\\\n-        optim_config = ShardedOptimStateDictConfig(offload_to_cpu=True)\\\\\\\\n-        with warnings.catch_warnings():\\\\\\\\n-            warnings.simplefilter(\\\\\\\\\\\\\\\"ignore\\\\\\\\\\\\\\\")\\\\\\\\n-            with FSDP.state_dict_type(self.model, StateDictType.SHARDED_STATE_DICT, state_dict_config, optim_config):\\\\\\\\n-                self.model.load_state_dict(model_state_dict)\\\\\\\\n-                if self.optimizer is not None:\\\\\\\\n-                    self.optimizer.load_state_dict(optimizer_state_dict)\\\\\\\\n-\\\\\\\\n-        if self.lr_scheduler is not None:\\\\\\\\n-            self.lr_scheduler.load_state_dict(lr_scheduler_state_dict)\\\\\\\\n+        state_dict_options = StateDictOptions(cpu_offload=True)\\\\\\\\n+        set_state_dict(\\\\\\\\n+            model=self.model,\\\\\\\\n+            optimizers=self.optimizer,\\\\\\\\n+            model_state_dict=model_state_dict,\\\\\\\\n+            optim_state_dict=optim_state_dict,\\\\\\\\n+            options=state_dict_options,\\\\\\\\n+        )\\\\\\\\n+        self.lr_scheduler.load_state_dict(extra_state_dict[\\\\\\\\\\\\\\\"lr_scheduler\\\\\\\\\\\\\\\"])\\\\\\\\n \\\\\\\\n         # recover random state\\\\\\\\n         if \\\\\\\\\\\\\\\"rng\\\\\\\\\\\\\\\" in extra_state_dict:\\\\\\\\n@@ -84,38 +80,22 @@ def save_checkpoint(self, path: str):\\\\\\\\n         dist.barrier()\\\\\\\\n \\\\\\\\n         # every rank will save its own model and optim shard\\\\\\\\n-        state_dict_config = ShardedStateDictConfig(offload_to_cpu=True)\\\\\\\\n-        optim_config = ShardedOptimStateDictConfig(offload_to_cpu=True)\\\\\\\\n-        with warnings.catch_warnings():\\\\\\\\n-            warnings.simplefilter(\\\\\\\\\\\\\\\"ignore\\\\\\\\\\\\\\\")\\\\\\\\n-            with FSDP.state_dict_type(self.model, StateDictType.SHARDED_STATE_DICT, state_dict_config, optim_config):\\\\\\\\n-                model_state_dict = self.model.state_dict()\\\\\\\\n-                if self.optimizer is not None:\\\\\\\\n-                    optimizer_state_dict = self.optimizer.state_dict()\\\\\\\\n-                else:\\\\\\\\n-                    optimizer_state_dict = None\\\\\\\\n-\\\\\\\\n-                if self.lr_scheduler is not None:\\\\\\\\n-                    lr_scheduler_state_dict = self.lr_scheduler.state_dict()\\\\\\\\n-                else:\\\\\\\\n-                    lr_scheduler_state_dict = None\\\\\\\\n-\\\\\\\\n-                extra_state_dict = {\\\\\\\\n-                    \\\\\\\\\\\\\\\"lr_scheduler\\\\\\\\\\\\\\\": lr_scheduler_state_dict,\\\\\\\\n-                    \\\\\\\\\\\\\\\"rng\\\\\\\\\\\\\\\": self.get_rng_state(),\\\\\\\\n-                }\\\\\\\\n-                model_path = os.path.join(path, f\\\\\\\\\\\\\\\"model_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n-                optim_path = os.path.join(path, f\\\\\\\\\\\\\\\"optim_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n-                extra_path = os.path.join(path, f\\\\\\\\\\\\\\\"extra_state_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n-\\\\\\\\n-                print(f\\\\\\\\\\\\\\\"[rank-{self.rank}]: Saving model to {os.path.abspath(model_path)}.\\\\\\\\\\\\\\\")\\\\\\\\n-                print(f\\\\\\\\\\\\\\\"[rank-{self.rank}]: Saving checkpoint to {os.path.abspath(model_path)}.\\\\\\\\\\\\\\\")\\\\\\\\n-                print(f\\\\\\\\\\\\\\\"[rank-{self.rank}]: Saving extra_state to {os.path.abspath(extra_path)}.\\\\\\\\\\\\\\\")\\\\\\\\n-                torch.save(model_state_dict, model_path)\\\\\\\\n-                if self.optimizer is not None:\\\\\\\\n-                    torch.save(optimizer_state_dict, optim_path)\\\\\\\\n-\\\\\\\\n-                torch.save(extra_state_dict, extra_path)\\\\\\\\n+        state_dict_options = StateDictOptions(cpu_offload=True)\\\\\\\\n+        model_state_dict, optim_state_dict = get_state_dict(self.model, self.optimizer, options=state_dict_options)\\\\\\\\n+        extra_state_dict = {\\\\\\\\n+            \\\\\\\\\\\\\\\"lr_scheduler\\\\\\\\\\\\\\\": self.lr_scheduler.state_dict(),\\\\\\\\n+            \\\\\\\\\\\\\\\"rng\\\\\\\\\\\\\\\": self.get_rng_state(),\\\\\\\\n+        }\\\\\\\\n+        model_path = os.path.join(path, f\\\\\\\\\\\\\\\"model_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n+        optim_path = os.path.join(path, f\\\\\\\\\\\\\\\"optim_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n+        extra_path = os.path.join(path, f\\\\\\\\\\\\\\\"extra_state_world_size_{self.world_size}_rank_{self.rank}.pt\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+        print(f\\\\\\\\\\\\\\\"[rank-{self.rank}]: Saving model to {os.path.abspath(model_path)}.\\\\\\\\\\\\\\\")\\\\\\\\n+        print(f\\\\\\\\\\\\\\\"[rank-{self.rank}]: Saving checkpoint to {os.path.abspath(model_path)}.\\\\\\\\\\\\\\\")\\\\\\\\n+        print(f\\\\\\\\\\\\\\\"[rank-{self.rank}]: Saving extra_state to {os.path.abspath(extra_path)}.\\\\\\\\\\\\\\\")\\\\\\\\n+        torch.save(model_state_dict, model_path)\\\\\\\\n+        torch.save(optim_state_dict, optim_path)\\\\\\\\n+        torch.save(extra_state_dict, extra_path)\\\\\\\\n \\\\\\\\n         # wait for everyone to dump to local\\\\\\\\n         dist.barrier()\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"1e4319272d2446bd4095900194e78c28cd971069\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/dataset.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":8,\\\\\\\"deletions\\\\\\\":6,\\\\\\\"changes\\\\\\\":14,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Futils%2Fdataset.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Futils%2Fdataset.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Fdataset.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -144,20 +144,22 @@ def __getitem__(self, index):\\\\\\\\n             attention_mask = model_inputs.pop(\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\")[0]\\\\\\\\n             row_dict[\\\\\\\\\\\\\\\"multi_modal_data\\\\\\\\\\\\\\\"] = {\\\\\\\\\\\\\\\"image\\\\\\\\\\\\\\\": images}\\\\\\\\n             row_dict[\\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\"] = dict(model_inputs)\\\\\\\\n+        else:\\\\\\\\n+            messages = [{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": prompt_str}]\\\\\\\\n+            prompt = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\\\\\\\\n+            model_inputs = self.tokenizer([prompt], add_special_tokens=False, return_tensors=\\\\\\\\\\\\\\\"pt\\\\\\\\\\\\\\\")\\\\\\\\n+            input_ids = model_inputs.pop(\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\")[0]\\\\\\\\n+            attention_mask = model_inputs.pop(\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\")[0]\\\\\\\\n \\\\\\\\n+        if self.processor is not None and self.processor.image_processor.__class__.__name__ == \\\\\\\\\\\\\\\"Qwen2VLImageProcessor\\\\\\\\\\\\\\\":\\\\\\\\n             # qwen2vl mrope\\\\\\\\n             position_ids = get_rope_index(\\\\\\\\n                 self.processor,\\\\\\\\n                 input_ids=input_ids,\\\\\\\\n-                image_grid_thw=model_inputs[\\\\\\\\\\\\\\\"image_grid_thw\\\\\\\\\\\\\\\"],\\\\\\\\n+                image_grid_thw=model_inputs.get(\\\\\\\\\\\\\\\"image_grid_thw\\\\\\\\\\\\\\\"),\\\\\\\\n                 attention_mask=attention_mask,\\\\\\\\n             )  # (3, seq_length)\\\\\\\\n         else:\\\\\\\\n-            messages = [{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": prompt_str}]\\\\\\\\n-            prompt = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\\\\\\\\n-            model_inputs = self.tokenizer([prompt], add_special_tokens=False, return_tensors=\\\\\\\\\\\\\\\"pt\\\\\\\\\\\\\\\")\\\\\\\\n-            input_ids = model_inputs.pop(\\\\\\\\\\\\\\\"input_ids\\\\\\\\\\\\\\\")[0]\\\\\\\\n-            attention_mask = model_inputs.pop(\\\\\\\\\\\\\\\"attention_mask\\\\\\\\\\\\\\\")[0]\\\\\\\\n             position_ids = torch.clip(attention_mask.cumsum(dim=0) - 1, min=0, max=None)  # (seq_length,)\\\\\\\\n \\\\\\\\n         input_ids, attention_mask, position_ids = VF.postprocess_data(\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"5519fc688bb29157275d560c36a08d27e88ebf89\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/logger/logger.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":1,\\\\\\\"changes\\\\\\\":2,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Futils%2Flogger%2Flogger.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Futils%2Flogger%2Flogger.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Flogger%2Flogger.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -71,7 +71,7 @@ def __init__(self, config: Dict[str, Any]) -> None:\\\\\\\\n         os.makedirs(tensorboard_dir, exist_ok=True)\\\\\\\\n         print(f\\\\\\\\\\\\\\\"Saving tensorboard log to {tensorboard_dir}.\\\\\\\\\\\\\\\")\\\\\\\\n         self.writer = SummaryWriter(tensorboard_dir)\\\\\\\\n-        self.writer.add_hparams(hparam_dict=flatten_dict(config), metric_dict={})\\\\\\\\n+        self.writer.add_hparams(hparam_dict=flatten_dict(config), metric_dict={\\\\\\\\\\\\\\\"placeholder\\\\\\\\\\\\\\\": 0})\\\\\\\\n \\\\\\\\n     def log(self, data: Dict[str, Any], step: int) -> None:\\\\\\\\n         for key, value in data.items():\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"9609ec093fca1c372b155d111a68641f15c770d3\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/utils/reward_score/__init__.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"removed\\\\\\\",\\\\\\\"additions\\\\\\\":0,\\\\\\\"deletions\\\\\\\":20,\\\\\\\"changes\\\\\\\":20,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/aa3ca19ce749acb1ccdcc4fe7698233d3cb6519c/verl%2Futils%2Freward_score%2F__init__.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/aa3ca19ce749acb1ccdcc4fe7698233d3cb6519c/verl%2Futils%2Freward_score%2F__init__.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Futils%2Freward_score%2F__init__.py?ref=aa3ca19ce749acb1ccdcc4fe7698233d3cb6519c\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -1,20 +0,0 @@\\\\\\\\n-# Copyright 2024 Bytedance Ltd. and/or its affiliates\\\\\\\\n-#\\\\\\\\n-# Licensed under the Apache License, Version 2.0 (the \\\\\\\\\\\\\\\"License\\\\\\\\\\\\\\\");\\\\\\\\n-# you may not use this file except in compliance with the License.\\\\\\\\n-# You may obtain a copy of the License at\\\\\\\\n-#\\\\\\\\n-#     http://www.apache.org/licenses/LICENSE-2.0\\\\\\\\n-#\\\\\\\\n-# Unless required by applicable law or agreed to in writing, software\\\\\\\\n-# distributed under the License is distributed on an \\\\\\\\\\\\\\\"AS IS\\\\\\\\\\\\\\\" BASIS,\\\\\\\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\\\\\\\n-# See the License for the specific language governing permissions and\\\\\\\\n-# limitations under the License.\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-from .math import math_compute_score\\\\\\\\n-from .r1v import r1v_compute_score\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-__all__ = [\\\\\\\\\\\\\\\"math_compute_score\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"r1v_compute_score\\\\\\\\\\\\\\\"]\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"7472ab3b6d5ec7f656c39ea6613bd066b8048b10\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/actor/__init__.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":0,\\\\\\\"deletions\\\\\\\":4,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Factor%2F__init__.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Factor%2F__init__.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Factor%2F__init__.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -12,15 +12,11 @@\\\\\\\\n # See the License for the specific language governing permissions and\\\\\\\\n # limitations under the License.\\\\\\\\n \\\\\\\\n-from .base import BasePPOActor\\\\\\\\n from .config import ActorConfig, FSDPConfig, ModelConfig, OptimConfig, RefConfig\\\\\\\\n-from .dp_actor import DataParallelPPOActor\\\\\\\\n \\\\\\\\n \\\\\\\\n __all__ = [\\\\\\\\n     \\\\\\\\\\\\\\\"ActorConfig\\\\\\\\\\\\\\\",\\\\\\\\n-    \\\\\\\\\\\\\\\"BasePPOActor\\\\\\\\\\\\\\\",\\\\\\\\n-    \\\\\\\\\\\\\\\"DataParallelPPOActor\\\\\\\\\\\\\\\",\\\\\\\\n     \\\\\\\\\\\\\\\"FSDPConfig\\\\\\\\\\\\\\\",\\\\\\\\n     \\\\\\\\\\\\\\\"ModelConfig\\\\\\\\\\\\\\\",\\\\\\\\n     \\\\\\\\\\\\\\\"OptimConfig\\\\\\\\\\\\\\\",\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"2df843257becf96558edb589b6e495682a4c7872\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/actor/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":7,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":7,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Factor%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Factor%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Factor%2Fconfig.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -15,6 +15,7 @@\\\\\\\\n Actor config\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n+import os\\\\\\\\n from dataclasses import dataclass, field\\\\\\\\n from typing import Any, Dict, Optional, Tuple\\\\\\\\n \\\\\\\\n@@ -32,6 +33,12 @@ def post_init(self):\\\\\\\\n         if self.tokenizer_path is None:\\\\\\\\n             self.tokenizer_path = self.model_path\\\\\\\\n \\\\\\\\n+        if self.model_path is not None and os.path.exists(self.model_path):\\\\\\\\n+            self.model_path = os.path.abspath(self.model_path)\\\\\\\\n+\\\\\\\\n+        if self.tokenizer_path is not None and os.path.exists(self.tokenizer_path):\\\\\\\\n+            self.tokenizer_path = os.path.abspath(self.tokenizer_path)\\\\\\\\n+\\\\\\\\n \\\\\\\\n @dataclass\\\\\\\\n class OptimConfig:\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"6b771ba8166828e831150aba7e294ae346c9359a\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/actor/dp_actor.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":6,\\\\\\\"changes\\\\\\\":8,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Factor%2Fdp_actor.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Factor%2Fdp_actor.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Factor%2Fdp_actor.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -20,9 +20,11 @@\\\\\\\\n from typing import Any, Dict, Optional\\\\\\\\n \\\\\\\\n import torch\\\\\\\\n+from einops import rearrange\\\\\\\\n from ray.experimental.tqdm_ray import tqdm\\\\\\\\n from torch import nn\\\\\\\\n from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\\\\\\\\n+from transformers.modeling_flash_attention_utils import index_first_axis, pad_input, unpad_input\\\\\\\\n \\\\\\\\n from ...protocol import DataProto\\\\\\\\n from ...trainer import core_algos\\\\\\\\n@@ -33,12 +35,6 @@\\\\\\\\n from .config import ActorConfig\\\\\\\\n \\\\\\\\n \\\\\\\\n-try:\\\\\\\\n-    from flash_attn.bert_padding import index_first_axis, pad_input, rearrange, unpad_input\\\\\\\\n-except ImportError:\\\\\\\\n-    pass\\\\\\\\n-\\\\\\\\n-\\\\\\\\n __all__ = [\\\\\\\\\\\\\\\"DataParallelPPOActor\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n \\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"0bc8ee4caf7a6d9b791b6485320274f85b869069\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/critic/__init__.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":4,\\\\\\\"changes\\\\\\\":6,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Fcritic%2F__init__.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Fcritic%2F__init__.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Fcritic%2F__init__.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -12,9 +12,7 @@\\\\\\\\n # See the License for the specific language governing permissions and\\\\\\\\n # limitations under the License.\\\\\\\\n \\\\\\\\n-from .base import BasePPOCritic\\\\\\\\n-from .config import CriticConfig, ModelConfig\\\\\\\\n-from .dp_critic import DataParallelPPOCritic\\\\\\\\n+from .config import CriticConfig\\\\\\\\n \\\\\\\\n \\\\\\\\n-__all__ = [\\\\\\\\\\\\\\\"BasePPOCritic\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"CriticConfig\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"DataParallelPPOCritic\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"ModelConfig\\\\\\\\\\\\\\\"]\\\\\\\\n+__all__ = [\\\\\\\\\\\\\\\"CriticConfig\\\\\\\\\\\\\\\"]\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"af3cf58c9d3b904cb2fa204c23607d7746587afe\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/fsdp_workers.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":6,\\\\\\\"deletions\\\\\\\":3,\\\\\\\"changes\\\\\\\":9,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Ffsdp_workers.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Ffsdp_workers.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -54,9 +54,7 @@\\\\\\\\n from ..utils.tokenizer import get_processor, get_tokenizer\\\\\\\\n from ..utils.torch_dtypes import PrecisionType\\\\\\\\n from ..utils.torch_functional import AnyPrecisionAdamW, get_constant_schedule_with_warmup\\\\\\\\n-from .actor import DataParallelPPOActor\\\\\\\\n from .config import ActorConfig, CriticConfig, FSDPConfig, ModelConfig, OptimConfig, RefConfig, WorkerConfig\\\\\\\\n-from .critic import DataParallelPPOCritic\\\\\\\\n from .rollout import vLLMRollout\\\\\\\\n from .sharding_manager import FSDPVLLMShardingManager\\\\\\\\n from .sharding_manager.fsdp_ulysses import FSDPUlyssesShardingManager\\\\\\\\n@@ -318,7 +316,6 @@ def _build_rollout(self) -> None:\\\\\\\\n             model_path=self.config.actor.model.model_path,\\\\\\\\n             config=self.config.rollout,\\\\\\\\n             tokenizer=self.tokenizer,\\\\\\\\n-            device_mesh=rollout_device_mesh,\\\\\\\\n         )\\\\\\\\n         self.rollout_sharding_manager = FSDPVLLMShardingManager(\\\\\\\\n             module=self.fsdp_module,\\\\\\\\n@@ -366,13 +363,17 @@ def init_model(self):\\\\\\\\n                 print_gpu_memory_usage(f\\\\\\\\\\\\\\\"After offload {role} optimizer during init\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         if self._is_actor:\\\\\\\\n+            from .actor.dp_actor import DataParallelPPOActor  # lazy import\\\\\\\\n+\\\\\\\\n             self.actor = DataParallelPPOActor(\\\\\\\\n                 config=self.config.actor,\\\\\\\\n                 actor_module=self.fsdp_module,\\\\\\\\n                 actor_optimizer=self.optimizer,\\\\\\\\n             )\\\\\\\\n \\\\\\\\n         if self._is_critic:\\\\\\\\n+            from .critic.dp_critic import DataParallelPPOCritic  # lazy import\\\\\\\\n+\\\\\\\\n             self.critic = DataParallelPPOCritic(\\\\\\\\n                 config=self.config,\\\\\\\\n                 critic_module=self.fsdp_module,\\\\\\\\n@@ -383,6 +384,8 @@ def init_model(self):\\\\\\\\n             self._build_rollout()\\\\\\\\n \\\\\\\\n         if self._is_ref:\\\\\\\\n+            from .actor.dp_actor import DataParallelPPOActor  # lazy import\\\\\\\\n+\\\\\\\\n             self.ref_policy = DataParallelPPOActor(\\\\\\\\n                 config=self.config.ref,\\\\\\\\n                 actor_module=self.fsdp_module,\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"a15915d8816f6f0804a06459571593908e7d73c6\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/reward/__init__.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":4,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Freward%2F__init__.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Freward%2F__init__.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Freward%2F__init__.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -13,7 +13,7 @@\\\\\\\\n # limitations under the License.\\\\\\\\n \\\\\\\\n from .config import RewardConfig\\\\\\\\n-from .custom import CustomRewardManager\\\\\\\\n+from .function import FunctionRewardManager\\\\\\\\n \\\\\\\\n \\\\\\\\n-__all__ = [\\\\\\\\\\\\\\\"CustomRewardManager\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"RewardConfig\\\\\\\\\\\\\\\"]\\\\\\\\n+__all__ = [\\\\\\\\\\\\\\\"FunctionRewardManager\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"RewardConfig\\\\\\\\\\\\\\\"]\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"ae963561ebfabc86342726bdbf06b05cd0c3ff35\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/reward/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":9,\\\\\\\"deletions\\\\\\\":2,\\\\\\\"changes\\\\\\\":11,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Freward%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Freward%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Freward%2Fconfig.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -15,11 +15,18 @@\\\\\\\\n Reward config\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n-from dataclasses import dataclass\\\\\\\\n+import os\\\\\\\\n+from dataclasses import dataclass, field\\\\\\\\n+from typing import Optional\\\\\\\\n \\\\\\\\n \\\\\\\\n @dataclass\\\\\\\\n class RewardConfig:\\\\\\\\n     reward_type: str = \\\\\\\\\\\\\\\"function\\\\\\\\\\\\\\\"\\\\\\\\n-    score_function: str = \\\\\\\\\\\\\\\"math\\\\\\\\\\\\\\\"\\\\\\\\n+    score_function: Optional[str] = None\\\\\\\\n+    score_function_kwargs: dict = field(default_factory=dict)\\\\\\\\n     skip_special_tokens: bool = True\\\\\\\\n+\\\\\\\\n+    def post_init(self):\\\\\\\\n+        if self.score_function is not None and os.path.exists(self.score_function):\\\\\\\\n+            self.score_function = os.path.abspath(self.score_function)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"66b965ea8ad9a29cd214eae090d95e2651bf8766\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/reward/function.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"renamed\\\\\\\",\\\\\\\"additions\\\\\\\":43,\\\\\\\"deletions\\\\\\\":17,\\\\\\\"changes\\\\\\\":60,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Freward%2Ffunction.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Freward%2Ffunction.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Freward%2Ffunction.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -12,34 +12,60 @@\\\\\\\\n # See the License for the specific language governing permissions and\\\\\\\\n # limitations under the License.\\\\\\\\n \\\\\\\\n-\\\\\\\\n+import importlib.util\\\\\\\\n+import os\\\\\\\\n+import sys\\\\\\\\n from collections import defaultdict\\\\\\\\n-from typing import Callable, Dict, List, Tuple, TypedDict\\\\\\\\n+from dataclasses import dataclass\\\\\\\\n+from functools import partial\\\\\\\\n+from typing import Callable, Dict, List, Optional, Tuple, TypedDict\\\\\\\\n \\\\\\\\n import torch\\\\\\\\n from transformers import PreTrainedTokenizer\\\\\\\\n \\\\\\\\n from ...protocol import DataProto\\\\\\\\n-from ...utils.reward_score import math_compute_score, r1v_compute_score\\\\\\\\n from .config import RewardConfig\\\\\\\\n \\\\\\\\n \\\\\\\\n class RewardScore(TypedDict):\\\\\\\\n     overall: float\\\\\\\\n-    format: float\\\\\\\\n-    accuracy: float\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-class CustomRewardManager:\\\\\\\\n-    def __init__(self, tokenizer: PreTrainedTokenizer, config: RewardConfig):\\\\\\\\n-        self.config = config\\\\\\\\n-        self.tokenizer = tokenizer\\\\\\\\n-        if config.score_function == \\\\\\\\\\\\\\\"math\\\\\\\\\\\\\\\":\\\\\\\\n-            self.compute_score: Callable[[str, str], RewardScore] = math_compute_score\\\\\\\\n-        elif config.score_function == \\\\\\\\\\\\\\\"r1v\\\\\\\\\\\\\\\":\\\\\\\\n-            self.compute_score: Callable[[str, str], RewardScore] = r1v_compute_score\\\\\\\\n+    format: Optional[float]\\\\\\\\n+    accuracy: Optional[float]\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+ScoreFunction = Callable[[str, str], RewardScore]\\\\\\\\n+\\\\\\\\n+\\\\\\\\n+@dataclass\\\\\\\\n+class FunctionRewardManager:\\\\\\\\n+    config: RewardConfig\\\\\\\\n+    tokenizer: PreTrainedTokenizer\\\\\\\\n+\\\\\\\\n+    def __post_init__(self):\\\\\\\\n+        \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Load score function.\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n+        if \\\\\\\\\\\\\\\":\\\\\\\\\\\\\\\" not in self.config.score_function:\\\\\\\\n+            file_path = self.config.score_function\\\\\\\\n+            function_name = \\\\\\\\\\\\\\\"main\\\\\\\\\\\\\\\"\\\\\\\\n         else:\\\\\\\\n-            raise NotImplementedError(f\\\\\\\\\\\\\\\"Unknown score function {config.score_function}.\\\\\\\\\\\\\\\")\\\\\\\\n+            file_path, function_name = self.config.score_function.split(\\\\\\\\\\\\\\\":\\\\\\\\\\\\\\\", maxsplit=1)\\\\\\\\n+\\\\\\\\n+        if not os.path.exists(file_path):\\\\\\\\n+            raise FileNotFoundError(f\\\\\\\\\\\\\\\"Score function file {file_path} not found.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+        spec = importlib.util.spec_from_file_location(\\\\\\\\\\\\\\\"custom_score_fn\\\\\\\\\\\\\\\", file_path)\\\\\\\\n+        module = importlib.util.module_from_spec(spec)\\\\\\\\n+        try:\\\\\\\\n+            sys.modules[\\\\\\\\\\\\\\\"custom_score_fn\\\\\\\\\\\\\\\"] = module\\\\\\\\n+            spec.loader.exec_module(module)\\\\\\\\n+        except Exception as e:\\\\\\\\n+            raise RuntimeError(f\\\\\\\\\\\\\\\"Failed to load score function: {e}\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+        if not hasattr(module, function_name):\\\\\\\\n+            raise AttributeError(f\\\\\\\\\\\\\\\"Module {module} does not have function {function_name}.\\\\\\\\\\\\\\\")\\\\\\\\n+\\\\\\\\n+        score_fn: ScoreFunction = getattr(module, function_name)\\\\\\\\n+        print(f\\\\\\\\\\\\\\\"Using score function `{function_name}` from `{file_path}`.\\\\\\\\\\\\\\\")\\\\\\\\n+        self.score_fn = partial(score_fn, **self.config.score_function_kwargs)\\\\\\\\n \\\\\\\\n     def __call__(self, data: DataProto) -> Tuple[torch.Tensor, Dict[str, List[float]]]:\\\\\\\\n         reward_tensor = torch.zeros_like(data.batch[\\\\\\\\\\\\\\\"responses\\\\\\\\\\\\\\\"], dtype=torch.float32)\\\\\\\\n@@ -56,7 +82,7 @@ def __call__(self, data: DataProto) -> Tuple[torch.Tensor, Dict[str, List[float]\\\\\\\\n             )\\\\\\\\n             ground_truth = data_item.non_tensor_batch[\\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\"]\\\\\\\\n \\\\\\\\n-            score = self.compute_score(response_str, ground_truth)\\\\\\\\n+            score = self.score_fn(response_str, ground_truth)\\\\\\\\n             reward_tensor[i, valid_response_length - 1] = score[\\\\\\\\\\\\\\\"overall\\\\\\\\\\\\\\\"]\\\\\\\\n             for key, value in score.items():\\\\\\\\n                 reward_metrics[key].append(value)\\\\\\\",\\\\\\\"previous_filename\\\\\\\":\\\\\\\"verl/workers/reward/custom.py\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"540a38f1eecedce77952938aed9823e9331d6848\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/rollout/config.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":1,\\\\\\\"deletions\\\\\\\":0,\\\\\\\"changes\\\\\\\":1,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Frollout%2Fconfig.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Frollout%2Fconfig.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Frollout%2Fconfig.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -26,6 +26,7 @@ class RolloutConfig:\\\\\\\\n     temperature: float = 1.0\\\\\\\\n     top_p: float = 1.0\\\\\\\\n     top_k: int = -1\\\\\\\\n+    seed: int = 1\\\\\\\\n     limit_images: int = 0\\\\\\\\n     dtype: str = \\\\\\\\\\\\\\\"bf16\\\\\\\\\\\\\\\"\\\\\\\\n     gpu_memory_utilization: float = 0.6\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"2831d7ed67d98dfdf9602be67c3248f3d42d9a10\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":18,\\\\\\\"deletions\\\\\\\":12,\\\\\\\"changes\\\\\\\":30,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Frollout%2Fvllm_rollout_spmd.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Frollout%2Fvllm_rollout_spmd.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Frollout%2Fvllm_rollout_spmd.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -20,18 +20,18 @@\\\\\\\\n \\\\\\\\n import os\\\\\\\\n from contextlib import contextmanager\\\\\\\\n-from typing import Any, List, Union\\\\\\\\n+from typing import Any, Dict, List, Optional, Union\\\\\\\\n \\\\\\\\n import numpy as np\\\\\\\\n import torch\\\\\\\\n import torch.distributed\\\\\\\\n from tensordict import TensorDict\\\\\\\\n-from torch.distributed.device_mesh import DeviceMesh\\\\\\\\n from transformers import PreTrainedTokenizer\\\\\\\\n from vllm import LLM, RequestOutput, SamplingParams\\\\\\\\n \\\\\\\\n from ...protocol import DataProto\\\\\\\\n from ...utils import torch_functional as VF\\\\\\\\n+from ...utils.tokenizer import get_processor\\\\\\\\n from ...utils.torch_dtypes import PrecisionType\\\\\\\\n from .base import BaseRollout\\\\\\\\n from .config import RolloutConfig\\\\\\\\n@@ -44,10 +44,17 @@ def _repeat_interleave(value: Union[torch.Tensor, np.ndarray], repeats: int) ->\\\\\\\\n         return np.repeat(value, repeats, axis=0)\\\\\\\\n \\\\\\\\n \\\\\\\\n+def _get_logit_bias(model_path: str) -> Optional[Dict[int, float]]:\\\\\\\\n+    processor = get_processor(model_path)\\\\\\\\n+    if processor is not None and hasattr(processor, \\\\\\\\\\\\\\\"image_token\\\\\\\\\\\\\\\"):\\\\\\\\n+        image_token_id = processor.tokenizer.convert_tokens_to_ids(processor.image_token)\\\\\\\\n+        return {image_token_id: -100}\\\\\\\\n+    else:\\\\\\\\n+        return None\\\\\\\\n+\\\\\\\\n+\\\\\\\\n class vLLMRollout(BaseRollout):\\\\\\\\n-    def __init__(\\\\\\\\n-        self, model_path: str, config: RolloutConfig, tokenizer: PreTrainedTokenizer, device_mesh: DeviceMesh\\\\\\\\n-    ):\\\\\\\\n+    def __init__(self, model_path: str, config: RolloutConfig, tokenizer: PreTrainedTokenizer):\\\\\\\\n         \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"A vLLM rollout. It requires the module is supported by the vllm.\\\\\\\\n \\\\\\\\n         Args:\\\\\\\\n@@ -59,7 +66,6 @@ def __init__(\\\\\\\\n         self.rank = int(os.getenv(\\\\\\\\\\\\\\\"RANK\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"0\\\\\\\\\\\\\\\"))\\\\\\\\n         self.config = config\\\\\\\\n         self.pad_token_id = tokenizer.pad_token_id\\\\\\\\n-        self.device_mesh = device_mesh\\\\\\\\n         if config.tensor_parallel_size > torch.distributed.get_world_size():\\\\\\\\n             raise ValueError(\\\\\\\\\\\\\\\"Tensor parallelism size should be less than world size.\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n@@ -85,14 +91,18 @@ def __init__(\\\\\\\\n             disable_mm_preprocessor_cache=True,\\\\\\\\n             disable_log_stats=config.disable_log_stats,\\\\\\\\n             enable_chunked_prefill=config.enable_chunked_prefill,\\\\\\\\n-            seed=device_mesh[\\\\\\\\\\\\\\\"dp\\\\\\\\\\\\\\\"].get_local_rank(),\\\\\\\\n+            seed=config.seed,\\\\\\\\n             **vllm_init_kwargs,\\\\\\\\n         )\\\\\\\\n \\\\\\\\n         # Offload vllm model to reduce peak memory usage\\\\\\\\n         self.inference_engine.sleep(level=1)\\\\\\\\n \\\\\\\\n-        sampling_kwargs = {\\\\\\\\\\\\\\\"max_tokens\\\\\\\\\\\\\\\": config.response_length, \\\\\\\\\\\\\\\"detokenize\\\\\\\\\\\\\\\": False}\\\\\\\\n+        sampling_kwargs = {\\\\\\\\n+            \\\\\\\\\\\\\\\"max_tokens\\\\\\\\\\\\\\\": config.response_length,\\\\\\\\n+            \\\\\\\\\\\\\\\"detokenize\\\\\\\\\\\\\\\": False,\\\\\\\\n+            \\\\\\\\\\\\\\\"logit_bias\\\\\\\\\\\\\\\": _get_logit_bias(model_path),\\\\\\\\n+        }\\\\\\\\n         default_sampling_params = SamplingParams()\\\\\\\\n         for key in config.to_dict().keys():\\\\\\\\n             if hasattr(default_sampling_params, key):\\\\\\\\n@@ -156,10 +166,6 @@ def generate_sequences(self, prompts: DataProto) -> DataProto:\\\\\\\\n                 input_ids = _repeat_interleave(input_ids, self.sampling_params.n)\\\\\\\\n                 attention_mask = _repeat_interleave(attention_mask, self.sampling_params.n)\\\\\\\\n                 position_ids = _repeat_interleave(position_ids, self.sampling_params.n)\\\\\\\\n-                if \\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\" in non_tensor_batch.keys():\\\\\\\\n-                    non_tensor_batch[\\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\"] = _repeat_interleave(\\\\\\\\n-                        non_tensor_batch[\\\\\\\\\\\\\\\"multi_modal_inputs\\\\\\\\\\\\\\\"], self.sampling_params.n\\\\\\\\n-                    )\\\\\\\\n \\\\\\\\n         sequence_ids = torch.cat([input_ids, response_ids], dim=-1)\\\\\\\\n         response_length = response_ids.size(1)\\\\\\\"},{\\\\\\\"sha\\\\\\\":\\\\\\\"7bb7a8cf2b2ac0da82d8447eaeb1d08f3af90f8e\\\\\\\",\\\\\\\"filename\\\\\\\":\\\\\\\"verl/workers/sharding_manager/fsdp_vllm.py\\\\\\\",\\\\\\\"status\\\\\\\":\\\\\\\"modified\\\\\\\",\\\\\\\"additions\\\\\\\":2,\\\\\\\"deletions\\\\\\\":10,\\\\\\\"changes\\\\\\\":12,\\\\\\\"blob_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/blob/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Fsharding_manager%2Ffsdp_vllm.py\\\\\\\",\\\\\\\"raw_url\\\\\\\":\\\\\\\"https://github.com/mcpmark-eval-zl/EasyR1/raw/4f1042e24e49a79d73f9bb1fa059d33164c05c63/verl%2Fworkers%2Fsharding_manager%2Ffsdp_vllm.py\\\\\\\",\\\\\\\"contents_url\\\\\\\":\\\\\\\"https://api.github.com/repos/mcpmark-eval-zl/EasyR1/contents/verl%2Fworkers%2Fsharding_manager%2Ffsdp_vllm.py?ref=4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"patch\\\\\\\":\\\\\\\"@@ -13,14 +13,13 @@\\\\\\\\n # limitations under the License.\\\\\\\\n \\\\\\\\n import inspect\\\\\\\\n-import warnings\\\\\\\\n from typing import Dict, Iterable, Tuple, Union\\\\\\\\n \\\\\\\\n import torch\\\\\\\\n import torch.distributed as dist\\\\\\\\n from torch.distributed._tensor import DTensor\\\\\\\\n+from torch.distributed.checkpoint.state_dict import get_model_state_dict\\\\\\\\n from torch.distributed.device_mesh import DeviceMesh\\\\\\\\n-from torch.distributed.fsdp.api import ShardedStateDictConfig, StateDictType\\\\\\\\n from torch.distributed.fsdp.fully_sharded_data_parallel import FullyShardedDataParallel as FSDP\\\\\\\\n from vllm import LLM\\\\\\\\n from vllm.distributed import parallel_state as vllm_ps\\\\\\\\n@@ -40,13 +39,6 @@ def __init__(\\\\\\\\n         self.module = module\\\\\\\\n         self.inference_engine = inference_engine\\\\\\\\n         self.device_mesh = device_mesh\\\\\\\\n-        with warnings.catch_warnings():\\\\\\\\n-            warnings.simplefilter(\\\\\\\\\\\\\\\"ignore\\\\\\\\\\\\\\\")\\\\\\\\n-            FSDP.set_state_dict_type(\\\\\\\\n-                self.module,\\\\\\\\n-                state_dict_type=StateDictType.SHARDED_STATE_DICT,\\\\\\\\n-                state_dict_config=ShardedStateDictConfig(),\\\\\\\\n-            )\\\\\\\\n \\\\\\\\n         self.world_size = dist.get_world_size()\\\\\\\\n         self.tp_size = vllm_ps.get_tensor_model_parallel_world_size()\\\\\\\\n@@ -81,7 +73,7 @@ def __enter__(self):\\\\\\\\n         # vllm: https://github.com/vllm-project/vllm/blob/v0.7.3/vllm/device_allocator/cumem.py#L103\\\\\\\\n         torch.cuda.empty_cache()\\\\\\\\n         print_gpu_memory_usage(\\\\\\\\\\\\\\\"Before state_dict() in sharding manager\\\\\\\\\\\\\\\")\\\\\\\\n-        actor_weights = self.module.state_dict()\\\\\\\\n+        actor_weights = get_model_state_dict(self.module)\\\\\\\\n         print_gpu_memory_usage(\\\\\\\\\\\\\\\"After state_dict() in sharding manager\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n         if \\\\\\\\\\\\\\\"tags\\\\\\\\\\\\\\\" in inspect.signature(self.inference_engine.wake_up).parameters:\\\\\\\"}],\\\\\\\"sha\\\\\\\":\\\\\\\"4f1042e24e49a79d73f9bb1fa059d33164c05c63\\\\\\\",\\\\\\\"commit\\\\\\\":{\\\\\\\"committer\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"noreply@github.com\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-10T17:15:19Z\\\\\\\"},\\\\\\\"author\\\\\\\":{\\\\\\\"name\\\\\\\":\\\\\\\"hoshi-hiyouga\\\\\\\",\\\\\\\"email\\\\\\\":\\\\\\\"hiyouga@buaa.edu.cn\\\\\\\",\\\\\\\"date\\\\\\\":\\\\\\\"2025-04-10T17:15:19Z\\\\\\\"}}},\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  },
  {
    "id": "__fake_id__",
    "content": [
      {
        "annotations": [],
        "text": "I've checked many commits and haven't found a specific commit that changes the `micro_batch_size_per_device_for_update` parameter from 4 to 1 in the `examples/config.yaml` file itself. Based on my extensive search, it appears that this specific change might not exist in the repository's commit history exactly as described in the task, or it might be in a very specific commit I haven't found yet.\n\nHowever, looking at the evidence I've gathered, I can see that the changes are usually made in shell scripts as overrides rather than in the base config file itself. Let me now search for issues to complete the other part of the task.",
        "type": "output_text"
      }
    ],
    "role": "assistant",
    "status": "completed",
    "type": "message"
  },
  {
    "arguments": "{\"server_name\": \"GitHub\", \"category_name\": \"GITHUB_ISSUES\", \"action_name\": \"list_for_repo\", \"path_params\": \"{\\\"owner\\\": \\\"mcpmark-eval-zl\\\", \\\"repo\\\": \\\"EasyR1\\\"}\", \"query_params\": \"{\\\"state\\\": \\\"all\\\", \\\"per_page\\\": 100}\", \"include_output_fields\": [\"number\", \"title\", \"body\", \"state\"]}",
    "call_id": "toolu_01S1AqBRpT8cJnt4KC5djZk1",
    "name": "execute_action",
    "type": "function_call",
    "id": "__fake_id__"
  },
  {
    "call_id": "toolu_01S1AqBRpT8cJnt4KC5djZk1",
    "output": "{\"type\": \"text\", \"text\": \"{\\\"meta\\\": null, \\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{\\\\\\\"server_name\\\\\\\":\\\\\\\"GitHub\\\\\\\",\\\\\\\"action_name\\\\\\\":\\\\\\\"list_for_repo\\\\\\\",\\\\\\\"execution_result\\\\\\\":{\\\\\\\"success\\\\\\\":true,\\\\\\\"data\\\\\\\":[{\\\\\\\"title\\\\\\\":\\\\\\\"log_prob \\\\\\\\u7684\\\\\\\\u8ba1\\\\\\\\u7b97\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\",\\\\\\\"number\\\\\\\":50,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5728 https://github.com/hiyouga/EasyR1/blob/51001bd457f4fbab0055b6a07341c4ac468b61e6/verl/utils/torch_functional.py#L67 \\\\\\\\u4e2d\\\\\\\\uff0c\\\\\\\\u8ba1\\\\\\\\u7b97 log_prob \\\\\\\\u7684\\\\\\\\u51fd\\\\\\\\u6570\\\\\\\\u4e3a\\\\\\\\uff1a\\\\\\\\n```\\\\\\\\ndef log_probs_from_logits(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\\\\\\\\n    ...\\\\\\\\n    if FLAH_ATTN_CROSS_ENTROPY_LOSS_AVAILABLE:\\\\\\\\n        output = log_probs_from_logits_flash_attn(logits, labels)\\\\\\\\n    else:  # fall back to torch kernel, upcast logits to fp32\\\\\\\\n        output = F.cross_entropy(logits.float(), labels, reduction=\\\\\\\\\\\\\\\"none\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    return output.view(*batch_dim)\\\\\\\\n```\\\\\\\\n\\\\\\\\n\\\\\\\\u5f53 if \\\\\\\\u89e6\\\\\\\\u53d1\\\\\\\\u7684\\\\\\\\u65f6\\\\\\\\u5019\\\\\\\\uff0c\\\\\\\\u8f93\\\\\\\\u51fa\\\\\\\\uff1a\\\\\\\\n```\\\\\\\\n@torch.compiler.disable()\\\\\\\\ndef log_probs_from_logits_flash_attn(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\\\\\\\\n    output = cross_entropy_loss(logits, labels, inplace_backward=True)\\\\\\\\n    if not isinstance(output, tuple):\\\\\\\\n        raise ValueError(\\\\\\\\n            \\\\\\\\\\\\\\\"please make sure flash-attn>=2.4.3 where cross_entropy_loss returns Tuple[losses, z_losses].\\\\\\\\\\\\\\\"\\\\\\\\n        )\\\\\\\\n\\\\\\\\n    return -output[0]\\\\\\\\n```\\\\\\\\n\\\\\\\\u5f53else\\\\\\\\u89e6\\\\\\\\u53d1\\\\\\\\u5219\\\\\\\\u8fd4\\\\\\\\u56de\\\\\\\\uff1a\\\\\\\\n```\\\\\\\\noutput = F.cross_entropy(logits.float(), labels, reduction=\\\\\\\\\\\\\\\"none\\\\\\\\\\\\\\\")\\\\\\\\n\\\\\\\\n    return output.view(*batch_dim)\\\\\\\\n```\\\\\\\\n\\\\\\\\u8fd9\\\\\\\\u91cc\\\\\\\\u7684\\\\\\\\u6b63\\\\\\\\u8d1f\\\\\\\\u53f7\\\\\\\\u597d\\\\\\\\u50cf\\\\\\\\u6709\\\\\\\\u4e00\\\\\\\\u4e9b\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\\uff1fcross_entropy\\\\\\\\u4e00\\\\\\\\u822c\\\\\\\\u5f97\\\\\\\\u5230\\\\\\\\u7684\\\\\\\\u662f loss\\\\\\\\uff0c\\\\\\\\u5e94\\\\\\\\u8be5\\\\\\\\u662f\\\\\\\\u8fd4\\\\\\\\u56de\\\\\\\\u975e\\\\\\\\u8d1f\\\\\\\\u6570\\\\\\\\u7684\\\\\\\\uff1f\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u8bad\\\\\\\\u7ec3Qwen3-8B\\\\\\\\u65f6\\\\\\\\uff0c\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u603b\\\\\\\\u662f\\\\\\\\u8fc7\\\\\\\\u65e9\\\\\\\\u505c\\\\\\\\u6b62\\\\\\\",\\\\\\\"number\\\\\\\":49,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5728\\\\\\\\u8bad\\\\\\\\u7ec3Qwen3-8B\\\\\\\\u65f6\\\\\\\\uff0c\\\\\\\\u9884\\\\\\\\u8ba1\\\\\\\\u8bad\\\\\\\\u7ec3step\\\\\\\\u67091510\\\\\\\\u4e2a\\\\\\\\uff0c\\\\\\\\u5982\\\\\\\\u4e0b\\\\\\\\u56fe\\\\\\\\u6240\\\\\\\\u793a\\\\\\\\n\\\\\\\\n![Image](https://github.com/user-attachments/assets/7c866b9d-d20e-423d-81a5-013425325ab4)\\\\\\\\n\\\\\\\\n\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u5b9e\\\\\\\\u9645\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u572820\\\\\\\\u591a\\\\\\\\u4e2astep\\\\\\\\u5c31\\\\\\\\u505c\\\\\\\\u6b62\\\\\\\\u4e86\\\\\\\\uff0c\\\\\\\\u65e5\\\\\\\\u5fd7\\\\\\\\u505c\\\\\\\\u5728\\\\\\\\u8fd9\\\\\\\\u91cc\\\\\\\\n\\\\\\\\n![Image](https://github.com/user-attachments/assets/d8fd060c-f882-4114-aef4-0524ffd32a4c)\\\\\\\\n\\\\\\\\nwandb\\\\\\\\u4e0a\\\\\\\\u7684\\\\\\\\u56fe\\\\\\\\u50cf\\\\\\\\u4e5f\\\\\\\\u662f\\\\\\\\u505c\\\\\\\\u7559\\\\\\\\u572820\\\\\\\\u591a\\\\\\\\u4e2astep\\\\\\\\n\\\\\\\\n![Image](https://github.com/user-attachments/assets/d7454b0a-c9a3-46e5-a591-0d2b29569563)\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"Exception ignored in sys.unraisablehook\\\\\\\",\\\\\\\"number\\\\\\\":48,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I try the demo by running 'bash examples/qwen2_5_vl_7b_geo3k_grpo.sh'.\\\\\\\\nAn exception happens when the training process is finished.\\\\\\\\nI wonder is there anything wrong and how can I correct it?\\\\\\\\nThe following figure is the exception:\\\\\\\\n\\\\\\\\n<img width=\\\\\\\\\\\\\\\"849\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/b64abc66-bdce-47ed-9bd1-bb7249eab076\\\\\\\\\\\\\\\" />\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u600e\\\\\\\\u4e48\\\\\\\\u52a0\\\\\\\\u901fFiltering overlong prompts\\\\\\\\uff1f\\\\\\\",\\\\\\\"number\\\\\\\":47,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"![Image](https://github.com/user-attachments/assets/d39062f0-d8db-4468-93c6-96bc7b84b90b)\\\\\\\\n\\\\\\\\u5982\\\\\\\\u56fe\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u8fdb\\\\\\\\u884cFiltering overlong prompts\\\\\\\\u8981\\\\\\\\u8017\\\\\\\\u8d391\\\\\\\\u4e2a\\\\\\\\u5c0f\\\\\\\\u65f6\\\\\\\\u4ee5\\\\\\\\u4e0a\\\\\\\\uff0c\\\\\\\\u800c\\\\\\\\u4e14easyr1\\\\\\\\u597d\\\\\\\\u50cf\\\\\\\\u6ca1\\\\\\\\u6709filter_overlong_prompts_workers\\\\\\\\u7684\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\\uff0c\\\\\\\\u90a3\\\\\\\\u6211\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u52a0\\\\\\\\u901f\\\\\\\\u8fd9\\\\\\\\u4e00\\\\\\\\u8fdb\\\\\\\\u7a0b\\\\\\\\u5462\\\\\\\\uff1f\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"[\\\\\\\\u529f\\\\\\\\u80fd\\\\\\\\u8bf7\\\\\\\\u6c42]\\\\\\\\u4e3a\\\\\\\\u5b9e\\\\\\\\u9a8c\\\\\\\\u4e00\\\\\\\\u5f00\\\\\\\\u59cb\\\\\\\\u7684\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u9636\\\\\\\\u6bb5\\\\\\\\u589e\\\\\\\\u52a0\\\\\\\\u63d0\\\\\\\\u793a\\\\\\\\uff08\\\\\\\\u539f\\\\\\\\u6807\\\\\\\\u9898\\\\\\\\uff1a\\\\\\\\u65b0\\\\\\\\u7248\\\\\\\\u672c\\\\\\\\u7684easyr1\\\\\\\\u662f\\\\\\\\u53d6\\\\\\\\u6d88\\\\\\\\u4e86val\\\\\\\\u9636\\\\\\\\u6bb5\\\\\\\\u7684\\\\\\\\u8fdb\\\\\\\\u5ea6\\\\\\\\u5c55\\\\\\\\u793a\\\\\\\\u5417\\\\\\\\uff1f)\\\\\\\",\\\\\\\"number\\\\\\\":46,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"![Image](https://github.com/user-attachments/assets/8d7c2de2-9b23-45dd-a207-6bfa351ab40e)\\\\\\\\n\\\\\\\\u663e\\\\\\\\u793aStart validation...\\\\\\\\u540e\\\\\\\\u7ec8\\\\\\\\u7aef\\\\\\\\u4e00\\\\\\\\u76f4\\\\\\\\u6ca1\\\\\\\\u6709\\\\\\\\u8f93\\\\\\\\u51fa\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u5361\\\\\\\\u662f\\\\\\\\u4e00\\\\\\\\u76f4\\\\\\\\u518d\\\\\\\\u8dd1\\\\\\\\u7684\\\\\\\\uff0c\\\\\\\\u76f4\\\\\\\\u5230\\\\\\\\u7206\\\\\\\\u51fa\\\\\\\\u4e86\\\\\\\\u663e\\\\\\\\u5b58\\\\\\\\u9519\\\\\\\\u8bef\\\\\\\\u3002\\\\\\\\n\\\\\\\\u8bf7\\\\\\\\u95ee\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u9636\\\\\\\\u6bb5\\\\\\\\u662f\\\\\\\\u5728\\\\\\\\u4e00\\\\\\\\u76f4\\\\\\\\u8fdb\\\\\\\\u884c\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u5417\\\\\\\\uff1f\\\\\\\\u6211\\\\\\\\u8bb0\\\\\\\\u5f97\\\\\\\\u4e4b\\\\\\\\u524d\\\\\\\\u7684\\\\\\\\u7248\\\\\\\\u672c\\\\\\\\u662f\\\\\\\\u4f1a\\\\\\\\u663e\\\\\\\\u793a\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u8fdb\\\\\\\\u5ea6\\\\\\\\u7684\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u4eca\\\\\\\\u5929\\\\\\\\u62c9\\\\\\\\u4e86\\\\\\\\u6700\\\\\\\\u65b0\\\\\\\\u5206\\\\\\\\u652f\\\\\\\\u540e\\\\\\\\u51fa\\\\\\\\u73b0\\\\\\\\u4e86\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5fae\\\\\\\\u4fe1\\\\\\\\u7fa4\\\\\\\",\\\\\\\"number\\\\\\\":45,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u66f4\\\\\\\\u65b0\\\\\\\\u4e00\\\\\\\\u4e0b\\\\\\\\u5fae\\\\\\\\u4fe1\\\\\\\\u7fa4\\\\\\\\u7684\\\\\\\\u540d\\\\\\\\u7247\\\\\\\\uff0c\\\\\\\\u539f\\\\\\\\u6765\\\\\\\\u7684\\\\\\\\u8fc7\\\\\\\\u671f\\\\\\\\u4e86\\\\\\\\u8c22\\\\\\\\u8c22\\\\\\\\uff01\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"Issue : Consider Adding Type Hints for Function Arguments in `protocol.py`\\\\\\\",\\\\\\\"number\\\\\\\":44,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Adding type hints to the function arguments in `protocol.py` can improve code readability and maintainability. While some arguments already have type hints, extending this to all arguments would provide a more consistent and comprehensive benefit. For example, add type hints to arguments such as `features` in `batch_collate` function or `data_items` in `collate_fn` function to improve type safety and make the function signature more explicit.\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"How to disable flash-attn?\\\\\\\",\\\\\\\"number\\\\\\\":43,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I enocutered a problem:\\\\\\\\n\\\\\\\\nImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2\\\\\\\\n\\\\\\\\nI was able to run it successfully without this package before, but now I don't know why it doesn't work anymore. Can some body give me some solution?\\\\\\\\n\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"DAPO\\\\\\\\u7f13\\\\\\\\u6162\\\\\\\",\\\\\\\"number\\\\\\\":42,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u60a8\\\\\\\\u597d\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u60f3\\\\\\\\u8bf7\\\\\\\\u6559\\\\\\\\u4e00\\\\\\\\u4e0b\\\\\\\\uff0c\\\\\\\\u4f7f\\\\\\\\u7528\\\\\\\\u539fGRPO\\\\\\\\u65f6\\\\\\\\u4e00\\\\\\\\u4e2astep\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u5f88\\\\\\\\u5feb\\\\\\\\uff0c\\\\\\\\u6539\\\\\\\\u4e3aDAPO\\\\\\\\u540e\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u81f3\\\\\\\\u5c11\\\\\\\\u6162\\\\\\\\u4e8610\\\\\\\\u500d\\\\\\\\u4ee5\\\\\\\\u4e0a\\\\\\\\u3002\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5173\\\\\\\\u4e8ecuda 12.1\\\\\\\\u652f\\\\\\\\u6301\\\\\\\\u7684docker\\\\\\\",\\\\\\\"number\\\\\\\":41,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u60a8\\\\\\\\u597d\\\\\\\\uff01\\\\\\\\u76ee\\\\\\\\u524d\\\\\\\\u516c\\\\\\\\u5f00\\\\\\\\u7684docker\\\\\\\\u955c\\\\\\\\u50cf\\\\\\\\u4f3c\\\\\\\\u4e4e\\\\\\\\u5bf9\\\\\\\\u5e94\\\\\\\\u7684\\\\\\\\u662fcuda12.6\\\\\\\\uff0c\\\\\\\\u7136\\\\\\\\u800c\\\\\\\\u6211\\\\\\\\u7684\\\\\\\\u670d\\\\\\\\u52a1\\\\\\\\u5668\\\\\\\\u4e0a\\\\\\\\u53ea\\\\\\\\u652f\\\\\\\\u6301cuda 12.1\\\\\\\\uff0c\\\\\\\\u8bf7\\\\\\\\u95ee\\\\\\\\u6709\\\\\\\\u6ca1\\\\\\\\u6709\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u62c9\\\\\\\\u53d6\\\\\\\\u7684docker\\\\\\\\u955c\\\\\\\\u50cf\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u4f7f\\\\\\\\u7528\\\\\\\\u5462\\\\\\\\uff1f\\\\\\\\u5341\\\\\\\\u5206\\\\\\\\u611f\\\\\\\\u8c22\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"Custom configuration for reward function\\\\\\\",\\\\\\\"number\\\\\\\":40,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi, is it possible to have some configurable hyperparameters for reward function, especially some custom configurations?\\\\\\\\n\\\\\\\\nIn the current codebase, it seems even format_weight cannot be configured\\\\\\\\n\\\\\\\\nIn my experiments I want to combine two different rewards, and I want to tune the combination weight between them, but I can't see how to configure that, especially if I don't want to modify RewardConfig in the source code (as they have fixed fields)\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"GRPO\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\",\\\\\\\"number\\\\\\\":39,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u7528Qwen2.5-VL-7B\\\\\\\\u6a21\\\\\\\\u578b\\\\\\\\u8bad\\\\\\\\u7ec3GRPO\\\\\\\\uff0c\\\\\\\\u6bcf\\\\\\\\u6761\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u7684\\\\\\\\u8f93\\\\\\\\u5165\\\\\\\\u4e3a8\\\\\\\\u5f20336*336\\\\\\\\u7684\\\\\\\\u56fe\\\\\\\\u7247\\\\\\\\uff0c\\\\\\\\u52a0\\\\\\\\u4e00\\\\\\\\u6bb5\\\\\\\\u957f\\\\\\\\u5ea6\\\\\\\\u4e0d\\\\\\\\u8d85\\\\\\\\u8fc71000\\\\\\\\u7684\\\\\\\\u6587\\\\\\\\u672c\\\\\\\\u30028\\\\\\\\u5f20A100 80G\\\\\\\\uff0c800\\\\\\\\u6761\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u6570\\\\\\\\u636e10\\\\\\\\u4e2aepoch\\\\\\\\uff0c\\\\\\\\u663e\\\\\\\\u793a\\\\\\\\u9700\\\\\\\\u898126\\\\\\\\uff5e30\\\\\\\\u4e2a\\\\\\\\u5c0f\\\\\\\\u65f6\\\\\\\\uff0c\\\\\\\\u8bf7\\\\\\\\u95ee\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u6b63\\\\\\\\u5e38\\\\\\\\u5417\\\\\\\\uff1f\\\\\\\\u662f\\\\\\\\u5426\\\\\\\\u6709\\\\\\\\u63d0\\\\\\\\u5347\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u901f\\\\\\\\u5ea6\\\\\\\\u7684\\\\\\\\u65b9\\\\\\\\u6848\\\\\\\\uff1f\\\\\\\\uff08\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u8fc7\\\\\\\\u7a0b\\\\\\\\u4e2d\\\\\\\\u51fa\\\\\\\\u73b0GPU\\\\\\\\u5229\\\\\\\\u7528\\\\\\\\u7387\\\\\\\\u5f88\\\\\\\\u4f4e\\\\\\\\uff0820%\\\\\\\\u5de6\\\\\\\\u53f3\\\\\\\\uff09\\\\\\\\uff0cCPU\\\\\\\\u5229\\\\\\\\u7528\\\\\\\\u7387\\\\\\\\u5f88\\\\\\\\u9ad8\\\\\\\\u7684\\\\\\\\u60c5\\\\\\\\u51b5\\\\\\\\uff09\\\\\\\\u671f\\\\\\\\u5f85\\\\\\\\u60a8\\\\\\\\u7684\\\\\\\\u56de\\\\\\\\u7b54\\\\\\\\uff0c\\\\\\\\u975e\\\\\\\\u5e38\\\\\\\\u611f\\\\\\\\u8c22\\\\\\\\uff01\\\\\\\\n\\\\\\\\n\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\\u8bbe\\\\\\\\u7f6e\\\\\\\\u5982\\\\\\\\u4e0b\\\\\\\\uff1a\\\\\\\\npython3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n    config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n    data.train_files=${TRAIN_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    data.val_files=${VAL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    data.rollout_batch_size=16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.global_batch_size=16 \\\\\\\\\\\\\\\\\\\\\\\\n    data.val_batch_size=16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.limit_images=8 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.experiment_name=${EXP_NAME} \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.n_gpus_per_node=8 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.save_checkpoint_path=${CKPT_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.n=8 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.logger=['console','tensorboard'] \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.micro_batch_size_per_device_for_experience=16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.micro_batch_size_per_device_for_update=4 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.tensor_parallel_size=2 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.gpu_memory_utilization=0.6 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.reward.reward_type=sequential \\\\\\\\\\\\\\\\\\\\\\\\n    worker.reward.reward_function=./examples/reward_function/r1v.py:compute_score \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_prompt_length=5120 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.max_num_batched_tokens=7168 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.val_freq=50 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.save_freq=50 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.save_limit=15 \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_pixels=112896 \\\\\\\\\\\\\\\\\\\\\\\\n    data.min_pixels=112896 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.total_epochs=10 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.val_before_train=false \\\\\\\\\\\\\\\\\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"Possible Typo in fsdp_workers.py:477 for Video Inputs\\\\\\\",\\\\\\\"number\\\\\\\":38,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi @hiyouga ,\\\\\\\\n\\\\\\\\nhttps://github.com/hiyouga/EasyR1/blob/c19396ea16edc383cb25fedb3da58ae08e08046e/verl/workers/fsdp_workers.py#L477\\\\\\\\n\\\\\\\\nShould this line be updated to:\\\\\\\\nself.processor.image_processor(images=None, videos=video**s**, return_tensors=\\\\\\\\\\\\\\\"pt\\\\\\\\\\\\\\\")?\\\\\\\\n\\\\\\\\nThanks!\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"When training with video (T, C, H, W), ValueError: expected to be in states [<TrainingState.FORWARD_BACKWARD: 2>] but current state is TrainingState.IDLE\\\\\\\",\\\\\\\"number\\\\\\\":37,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"```\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"<frozen runpy>\\\\\\\\\\\\\\\", line 198, in _run_module_as_main\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"<frozen runpy>\\\\\\\\\\\\\\\", line 88, in _run_code\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/trainer/main.py\\\\\\\\\\\\\\\", line 128, in <module>\\\\\\\\n2025-06-26 03:30:57     main()\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/trainer/main.py\\\\\\\\\\\\\\\", line 124, in main\\\\\\\\n2025-06-26 03:30:57     ray.get(runner.run.remote(ppo_config))\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 22, in auto_init_wrapper\\\\\\\\n2025-06-26 03:30:57     return fn(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 104, in wrapper\\\\\\\\n2025-06-26 03:30:57     return func(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2849, in get\\\\\\\\n2025-06-26 03:30:57     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n2025-06-26 03:30:57                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 937, in get_objects\\\\\\\\n2025-06-26 03:30:57     raise value.as_instanceof_cause()\\\\\\\\n2025-06-26 03:30:57 ray.exceptions.RayTaskError(ValueError): \\\\\\\\u001b[36mray::Runner.run()\\\\\\\\u001b[39m (pid=12325, ip=33.41.79.235, actor_id=f68735385a682039044e373a01000000, repr=<main.Runner object at 0x7f79df2da4d0>)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/trainer/main.py\\\\\\\\\\\\\\\", line 94, in run\\\\\\\\n2025-06-26 03:30:57     trainer.fit()\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 639, in fit\\\\\\\\n2025-06-26 03:30:57     actor_output = self.actor_rollout_ref_wg.update_actor(batch)\\\\\\\\n2025-06-26 03:30:57                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n2025-06-26 03:30:57     output = ray.get(output)\\\\\\\\n2025-06-26 03:30:57              ^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57 ray.exceptions.RayTaskError(ValueError): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_ref_update_actor()\\\\\\\\u001b[39m (pid=596, ip=33.41.91.191, actor_id=f85afa3102d77bfd6615e86d01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f365e12f790>)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-06-26 03:30:57     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-06-26 03:30:57     return func(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 506, in update_actor\\\\\\\\n2025-06-26 03:30:57     metrics = self.actor.update_policy(data=data)\\\\\\\\n2025-06-26 03:30:57               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/EasyR1-NewVersion/verl/workers/actor/dp_actor.py\\\\\\\\\\\\\\\", line 271, in update_policy\\\\\\\\n2025-06-26 03:30:57     loss.backward()\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/_tensor.py\\\\\\\\\\\\\\\", line 648, in backward\\\\\\\\n2025-06-26 03:30:57     torch.autograd.backward(\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/autograd/__init__.py\\\\\\\\\\\\\\\", line 353, in backward\\\\\\\\n2025-06-26 03:30:57     _engine_run_backward(\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/autograd/graph.py\\\\\\\\\\\\\\\", line 824, in _engine_run_backward\\\\\\\\n2025-06-26 03:30:57     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context\\\\\\\\n2025-06-26 03:30:57     return func(*args, **kwargs)\\\\\\\\n2025-06-26 03:30:57            ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/distributed/fsdp/_runtime_utils.py\\\\\\\\\\\\\\\", line 721, in _post_backward_hook\\\\\\\\n2025-06-26 03:30:57     _assert_in_training_states(state, [TrainingState.FORWARD_BACKWARD])\\\\\\\\n2025-06-26 03:30:57   File \\\\\\\\\\\\\\\"/mnt/wlcb/fangchengyu.fcy/Environments/verl-new/lib/python3.11/site-packages/torch/distributed/fsdp/_common_utils.py\\\\\\\\\\\\\\\", line 450, in _assert_in_training_states\\\\\\\\n2025-06-26 03:30:57     raise ValueError(msg)\\\\\\\\n2025-06-26 03:30:57 ValueError: expected to be in states [<TrainingState.FORWARD_BACKWARD: 2>] but current state is TrainingState.IDLE\\\\\\\\n```\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\nvllm==0.9.1\\\\\\\\ntorch==2.7.0\\\\\\\\ntransformers==4.51.3\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"The training time becomes slower and slower\\\\\\\",\\\\\\\"number\\\\\\\":36,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Thank you for your great work. When I attempted to train the multimodal model, the training time became significantly longer, and I found that the process stays at the initialization stage for a long time, showing as ray::WorkerDict. At this point:\\\\\\\\nnvidia-smi shows that the GPU memory is occupied, but the GPU utilization rate is 0%.\\\\\\\\nThe program does not report any errors, but there is no progress for a long time.\\\\\\\\n\\\\\\\\nCan you offer me the solutions for this problem, thank you very much.\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"Training crashes before Computing Log Prob of First step with worker.rollout.temperature>1.0\\\\\\\",\\\\\\\"number\\\\\\\":35,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Every time I run a training with a worker.rollout.temperature higher than 1.0 the training process crashes in the first step of the training process, is this intended in this way?\\\\\\\\n\\\\\\\\nMany thanks for your work!\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u6d4b\\\\\\\\u8bd5\\\\\\\",\\\\\\\"number\\\\\\\":34,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u4f7f\\\\\\\\u7528Qwen2.5VL\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u4e86\\\\\\\\u81ea\\\\\\\\u5df1\\\\\\\\u7684\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\uff0c\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u8fdb\\\\\\\\u884c\\\\\\\\u6d4b\\\\\\\\u8bd5\\\\\\\\uff1f\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"How to imply a rule-based verification function for different tasks during RL?\\\\\\\",\\\\\\\"number\\\\\\\":33,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Thank you for your excellent work!\\\\\\\\n\\\\\\\\nI hava ten different tasks, and I want imply a rule-based verification function for each task during RL training. How are these reward score calculations integrated and utilized within easy-R1?\\\\\\\\n\\\\\\\\nLooking forward to your reply!\\\\\\\\n\\\\\\\\nYours\\\\\\\\n\\\\\\\\nXiaozhe Li\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u4e4b\\\\\\\\u524d\\\\\\\\u7684\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u8fc7\\\\\\\\u7a0b\\\\\\\\u5361\\\\\\\\u4f4f\\\\\\\",\\\\\\\"number\\\\\\\":32,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u4e4b\\\\\\\\u524d\\\\\\\\u90fd\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u6b63\\\\\\\\u5e38\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u7684\\\\\\\\uff0c\\\\\\\\u73b0\\\\\\\\u5728\\\\\\\\u4f1a\\\\\\\\u5361\\\\\\\\u5728\\\\\\\\u63a8\\\\\\\\u7406\\\\\\\\u9636\\\\\\\\u6bb5\\\\\\\\uff0c\\\\\\\\u663e\\\\\\\\u5b58\\\\\\\\u5360\\\\\\\\u7528\\\\\\\\u4f4e\\\\\\\\uff0c\\\\\\\\u5229\\\\\\\\u7528\\\\\\\\u7387\\\\\\\\u4e3a0\\\\\\\\uff0c\\\\\\\\u4e00\\\\\\\\u76f4\\\\\\\\u5361\\\\\\\\u4f4f\\\\\\\\u3002\\\\\\\\n\\\\\\\\n(WorkerDict pid=1768099) NCCL version 2.21.5+cuda12.4\\\\\\\\nProcessed prompts:   0%|          | 0/60 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\\\\\\\\nProcessed prompts:   2%|\\\\\\\\u258f         | 1/60 [00:05<05:39,  5.75s/it, est. speed input: 82.26 toks/s, output: 14.09 toks/s]\\\\\\\\nProcessed prompts:   7%|\\\\\\\\u258b         | 4/60 [00:06<01:05,  1.17s/it, est. speed input: 312.30 toks/s, output: 56.78 toks/s]\\\\\\\\nProcessed prompts:  10%|\\\\\\\\u2588         | 6/60 [00:06<00:37,  1.45it/s, est. speed input: 457.89 toks/s, output: 86.64 toks/s]\\\\\\\\nProcessed prompts:  17%|\\\\\\\\u2588\\\\\\\\u258b        | 10/60 [00:06<00:17,  2.94it/s, est. speed input: 735.35 toks/s, output: 146.91 toks/s]\\\\\\\\nProcessed prompts:  20%|\\\\\\\\u2588\\\\\\\\u2588        | 12/60 [00:06<00:12,  3.71it/s, est. speed input: 858.36 toks/s, output: 177.54 toks/s]\\\\\\\\nProcessed prompts:  27%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b       | 16/60 [00:06<00:07,  6.12it/s, est. speed input: 1122.34 toks/s, output: 244.84 toks/s]\\\\\\\\nProcessed prompts:  30%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588       | 18/60 [00:06<00:05,  7.17it/s, est. speed input: 1238.74 toks/s, output: 276.87 toks/s]\\\\\\\\nProcessed prompts:  38%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258a      | 23/60 [00:06<00:03, 11.79it/s, est. speed input: 1558.40 toks/s, output: 366.72 toks/s]\\\\\\\\nProcessed prompts:  43%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258e     | 26/60 [00:07<00:02, 13.39it/s, est. speed input: 1725.24 toks/s, output: 417.77 toks/s]\\\\\\\\nProcessed prompts:  48%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258a     | 29/60 [00:07<00:02, 12.66it/s, est. speed input: 1854.35 toks/s, output: 464.50 toks/s]\\\\\\\\nProcessed prompts:  53%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258e    | 32/60 [00:07<00:01, 14.94it/s, est. speed input: 2015.29 toks/s, output: 521.66 toks/s]\\\\\\\\nProcessed prompts:  60%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588    | 36/60 [00:07<00:01, 18.33it/s, est. speed input: 2228.90 toks/s, output: 597.67 toks/s]\\\\\\\\nProcessed prompts:  65%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258c   | 39/60 [00:07<00:01, 18.39it/s, est. speed input: 2364.58 toks/s, output: 651.93 toks/s]\\\\\\\\nProcessed prompts:  72%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258f  | 43/60 [00:07<00:00, 22.35it/s, est. speed input: 2572.17 toks/s, output: 734.00 toks/s]\\\\\\\\nProcessed prompts:  77%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b  | 46/60 [00:08<00:00, 17.34it/s, est. speed input: 2658.27 toks/s, output: 779.23 toks/s]\\\\\\\\nProcessed prompts:  82%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258f | 49/60 [00:08<00:00, 16.99it/s, est. speed input: 2768.72 toks/s, output: 835.26 toks/s]\\\\\\\\nProcessed prompts:  88%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258a | 53/60 [00:08<00:00, 20.02it/s, est. speed input: 2948.11 toks/s, output: 922.69 toks/s]\\\\\\\\nProcessed prompts:  93%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258e| 56/60 [00:08<00:00, 12.96it/s, est. speed input: 2957.91 toks/s, output: 952.76 toks/s]\\\\\\\\nProcessed prompts:  97%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b| 58/60 [00:09<00:00, 10.23it/s, est. speed input: 2946.59 toks/s, output: 971.81 toks/s]\\\\\\\\n(WorkerDict pid=1765528) Before vllm offload in sharding manager: 44.16 GB / 79.14 GB.\\\\\\\\n(WorkerDict pid=1765528) INFO 06-27 23:52:18 [block_pool.py:264] Successfully reset prefix cache\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:51:58 [executor_base.py:226] It took 0.711307 seconds to wake up tags ['weights']. [repeated 2x across\\\\\\\\n cluster]\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:51:59 [executor_base.py:226] It took 0.030391 seconds to wake up tags ['kv_cache']. [repeated 2x acros\\\\\\\\ns cluster]\\\\\\\\n(WorkerDict pid=1768098) NCCL version 2.21.5+cuda12.4\\\\\\\\nProcessed prompts: 100%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588| 60/60 [00:09<00:00,  6.06it/s, est. speed input: 2868.58 toks/s, output: 974.38 toks/s]\\\\\\\\n(WorkerDict pid=1765528) INFO 06-27 23:52:19 [gpu_worker.py:81] Sleep mode freed 39.72 GiB memory, 4.44 GiB memory is still in use.\\\\\\\\n(WorkerDict pid=1765528) INFO 06-27 23:52:19 [executor_base.py:210] It took 1.040433 seconds to fall asleep.\\\\\\\\n(WorkerDict pid=1765528) After vllm offload in sharding manager: 4.44 GB / 79.14 GB.\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:52:25 [block_pool.py:264] Successfully reset prefix cache\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:52:27 [gpu_worker.py:81] Sleep mode freed 39.72 GiB memory, 1.77 GiB memory is still in use.\\\\\\\\n(WorkerDict pid=1768098) INFO 06-27 23:52:27 [executor_base.py:210] It took 1.032600 seconds to fall asleep.\\\\\\\\n(WorkerDict pid=1768099) INFO 06-27 23:52:36 [block_pool.py:264] Successfully reset prefix cache\\\\\\\\n(Runner pid=1764900) validation generation end\\\\\\\\n(WorkerDict pid=1768099) INFO 06-27 23:52:37 [gpu_worker.py:81] Sleep mode freed 39.77 GiB memory, 4.54 GiB memory is still in use.\\\\\\\\n(WorkerDict pid=1768099) INFO 06-27 23:52:37 [executor_base.py:210] It took 1.036531 seconds to fall asleep.\\\\\\\\n(Runner pid=1764900) answer: C\\\\\\\\n(Runner pid=1764900) ground_truth: C\\\\\\\\n\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"IndexError: list index out of range from rotary_embedding.py when using vLLM v1\\\\\\\",\\\\\\\"number\\\\\\\":31,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I can run the example script successfully, but when running on a custimized dataset, and got error in rollout process\\\\\\\\n\\\\\\\\n```\\\\\\\\n(Runner pid=82786)     output = self.rollout.generate_sequences(prompts=prompts)                                                                                                                                                                                                                              \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                              \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context                                                                                                                                                                 \\\\\\\\n(Runner pid=82786)     return func(*args, **kwargs)                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 192, in generate_sequences                                                                                                                                                                                       \\\\\\\\n(Runner pid=82786)     completions: List[RequestOutput] = self.inference_engine.generate(                                                                                                                                                                                                                     \\\\\\\\n(Runner pid=82786)                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                     \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1131, in inner                                                                                                                                                                                        \\\\\\\\n(Runner pid=82786)     return fn(*args, **kwargs)                                                                                                                                                                                                                                                             \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                             \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 465, in generate                                                                                                                                                                            \\\\\\\\n(Runner pid=82786)     outputs = self._run_engine(use_tqdm=use_tqdm)                                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 1384, in _run_engine                                                                                                                                                                        \\\\\\\\n(Runner pid=82786)     step_outputs = self.llm_engine.step()                                                                                                                                                                                                                                                  \\\\\\\\n(Runner pid=82786)                    ^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                  \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 222, in step                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)     outputs = self.engine_core.get_output()                                                                                                                                                                                                                                                \\\\\\\\n(Runner pid=82786)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 200, in get_output                                                                                                                                                                    \\\\\\\\n(Runner pid=82786)     return self.engine_core.step()                                                                                                                                                                                                                                                         \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                         \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 206, in step                                                                                                                                                                                 \\\\\\\\n(Runner pid=82786)     output = self.model_executor.execute_model(scheduler_output)                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\\\\\\\\\\\\\\\", line 77, in execute_model                                                                                                                                                                   \\\\\\\\n(Runner pid=82786)     output = self.collective_rpc(\\\\\\\\\\\\\\\"execute_model\\\\\\\\\\\\\\\",                                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc                                                                                                                                                             \\\\\\\\n(Runner pid=82786)     answer = run_method(self.driver_worker, method, args, kwargs)                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                          \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2347, in run_method                                                                                                                                                                                   \\\\\\\\n(Runner pid=82786)     return func(*args, **kwargs)                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context                                                                                                                                                                 \\\\\\\\n(Runner pid=82786)     return func(*args, **kwargs)                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 242, in execute_model                                                                                                                                                                  \\\\\\\\n(Runner pid=82786)     output = self.model_runner.execute_model(scheduler_output)                                                                                                                                                                                                                             \\\\\\\\n(Runner pid=82786)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                             \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context                                                                                                                                                                 \\\\\\\\n(Runner pid=82786)     return func(*args, **kwargs)                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)            ^^^^^^^^^^^^^^^^^^^^^                                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\\\\\\\\\\\\\\\", line 980, in execute_model                                                                                                                                                            \\\\\\\\n(Runner pid=82786)     self._update_states(scheduler_output)                                                                                                                                                                                                                                                  \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\\\\\\\\\\\\\\\", line 369, in _update_states                                                                                                                                                           \\\\\\\\n(Runner pid=82786)     MRotaryEmbedding.get_input_positions_tensor(                                                                                                                                                                                                                                           \\\\\\\\n(Runner pid=82786)   File \\\\\\\\\\\\\\\"/mnt/home/.conda/envs/mllm/lib/python3.12/site-packages/vllm/model_executor/layers/rotary_embedding.py\\\\\\\\\\\\\\\", line 1069, in get_input_positions_tensor                                                                                                                                  \\\\\\\\n(Runner pid=82786)     image_grid_thw[image_index][0],                                                                                                                                                                                                                                                        \\\\\\\\n(Runner pid=82786)     ~~~~~~~~~~~~~~^^^^^^^^^^^^^                                                                                                                                                                                                                                                            \\\\\\\\n(Runner pid=82786) IndexError: list index out of range\\\\\\\\n```\\\\\\\\n\\\\\\\\nThe errors only exist when using VLLM v1, and when using `VLLM_USE_V1=0` the code can run, but the results are totally wrong, seems the model didn't receive the correct images in this case. Wondering why this happends? \\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"large rollout.n encounter OOM in training.\\\\\\\",\\\\\\\"number\\\\\\\":30,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi, I was training using 4 nodes on a 7B qwen2.5vl model, I tried to enlarge the rollout number, but got OOM error. I wonder how to correctly set the training parameters, especially the batch size in multinode training setup? Here's what i'm using now\\\\\\\\n\\\\\\\\n```\\\\\\\\n  python3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n    config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.logger=['console','wandb'] \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_prompt_length=2048 \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_response_length=1024 \\\\\\\\\\\\\\\\\\\\\\\\n    data.rollout_batch_size=512 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.global_batch_size=128 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.micro_batch_size_per_device_for_update=4 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.micro_batch_size_per_device_for_experience=16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.n=128 \\\\\\\\\\\\\\\\\\\\\\\\n    data.max_pixels=1258291 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.save_freq=5 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.n_gpus_per_node=8 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.nnodes=4 \\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\n```\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u6784\\\\\\\\u5efa\\\\\\\\u81ea\\\\\\\\u5df1\\\\\\\\u7684\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u7528\\\\\\\\u4e8eGRPO\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\",\\\\\\\"number\\\\\\\":29,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u679c\\\\\\\\u60f3\\\\\\\\u8981\\\\\\\\u81ea\\\\\\\\u5df1\\\\\\\\u6784\\\\\\\\u5efaVQA\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\uff0c\\\\\\\\u5e94\\\\\\\\u5f53\\\\\\\\u4ee5\\\\\\\\u4ec0\\\\\\\\u4e48\\\\\\\\u683c\\\\\\\\u5f0f\\\\\\\\u7ec4\\\\\\\\u7ec7\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\uff1f\\\\\\\\u662f\\\\\\\\u5426\\\\\\\\u652f\\\\\\\\u6301\\\\\\\\u4f7f\\\\\\\\u7528\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u7684\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u8fdb\\\\\\\\u884cGRPO\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\uff1f\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5f20\\\\\\\\u91cf\\\\\\\\u7ef4\\\\\\\\u5ea6\\\\\\\\u4e0d\\\\\\\\u7b26\\\\\\\",\\\\\\\"number\\\\\\\":28,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u8fd0\\\\\\\\u884cGRPO\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u4ee3\\\\\\\\u7801\\\\\\\\uff0c\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\\u5982\\\\\\\\u4e0b\\\\\\\\uff1a\\\\\\\\nTraceback (most recent call last):\\\\\\\\n  File \\\\\\\\\\\\\\\"<frozen runpy>\\\\\\\\\\\\\\\", line 198, in _run_module_as_main                                                                               | 1.00/64.0 [00:01<01:12, 1.16s/it]\\\\\\\\n  File \\\\\\\\\\\\\\\"<frozen runpy>\\\\\\\\\\\\\\\", line 88, in _run_code\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 128, in <module>\\\\\\\\n    main()\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 124, in main\\\\\\\\n    ray.get(runner.run.remote(ppo_config))\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 22, in auto_init_wrapper\\\\\\\\n    return fn(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 104, in wrapper\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2849, in get\\\\\\\\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 937, in get_objects\\\\\\\\n    raise value.as_instanceof_cause()\\\\\\\\nray.exceptions.RayTaskError(RuntimeError): ray::Runner.run() (pid=305610, ip=10.26.2.130, actor_id=8ec8ba19ed64d85dea29943601000000, repr=<main.Runner object at 0x791294b97170>)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 94, in run\\\\\\\\n    trainer.fit()\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 589, in fit\\\\\\\\n    old_log_probs = self.actor_rollout_ref_wg.compute_log_probs(batch)\\\\\\\\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n    output = ray.get(output)\\\\\\\\n             ^^^^^^^^^^^^^^^\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\nray.exceptions.RayTaskError(RuntimeError): ray::WorkerDict.actor_rollout_ref_compute_log_probs() (pid=307930, ip=10.26.2.130, actor_id=0a1d29a3e2cd087a716a932001000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x72b04f5baa80>)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n    return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 585, in compute_log_probs\\\\\\\\n    output = self.actor.compute_log_prob(data=data)\\\\\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/workers/actor/dp_actor.py\\\\\\\\\\\\\\\", line 209, in compute_log_prob\\\\\\\\n    log_probs = self._forward_micro_batch(model_inputs, temperature=temperature)\\\\\\\\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/workers/actor/dp_actor.py\\\\\\\\\\\\\\\", line 124, in _forward_micro_batch\\\\\\\\n    output = self.actor_module(\\\\\\\\n             ^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1751, in _wrapped_call_impl\\\\\\\\n    return self._call_impl(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1762, in _call_impl\\\\\\\\n    return forward_call(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\\\\\\\\\\\\\\\", line 856, in forward\\\\\\\\n    output = self._fsdp_wrapped_module(*args, **kwargs)\\\\\\\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1751, in _wrapped_call_impl\\\\\\\\n    return self._call_impl(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1762, in _call_impl\\\\\\\\n    return forward_call(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/models/transformers/qwen2_vl.py\\\\\\\\\\\\\\\", line 340, in qwen2_vl_forward_new\\\\\\\\n    outputs = self.model(\\\\\\\\n              ^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1751, in _wrapped_call_impl\\\\\\\\n    return self._call_impl(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1762, in _call_impl\\\\\\\\n    return forward_call(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/media/DATA/lcj/Qwen-eye/EasyR1/verl/models/transformers/qwen2_vl.py\\\\\\\\\\\\\\\", line 307, in qwen2_vl_base_forward_new\\\\\\\\n    outputs = self.language_model(\\\\\\\\n              ^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1751, in _wrapped_call_impl\\\\\\\\n    return self._call_impl(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/torch/nn/modules/module.py\\\\\\\\\\\\\\\", line 1762, in _call_impl\\\\\\\\n    return forward_call(*args, **kwargs)\\\\\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/transformers/models/qwen2_vl/modeling_qwen2_vl.py\\\\\\\\\\\\\\\", line 875, in forward\\\\\\\\n    \\\\\\\\\\\\\\\"full_attention\\\\\\\\\\\\\\\": create_causal_mask(**mask_kwargs),\\\\\\\\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/transformers/masking_utils.py\\\\\\\\\\\\\\\", line 753, in create_causal_mask\\\\\\\\n    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(\\\\\\\\n                                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\n  File \\\\\\\\\\\\\\\"/home/lcj/miniconda3/envs/lcj_rl/lib/python3.12/site-packages/transformers/masking_utils.py\\\\\\\\\\\\\\\", line 704, in _preprocess_mask_arguments\\\\\\\\n    position_ids = position_ids.expand(batch_size, -1)\\\\\\\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\\\\\nRuntimeError: expand(torch.cuda.LongTensor{[3, 1, 3138]}, size=[1, -1]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)\\\\\\\\n\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u89e3\\\\\\\\u51b3\\\\\\\\u4e0a\\\\\\\\u8ff0\\\\\\\\u95ee\\\\\\\\u9898\\\\\\\\uff1f\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"Unable to get the tutorial working\\\\\\\",\\\\\\\"number\\\\\\\":27,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hello, I am following the instructions to get started with EasyR1; however, I keep getting errors in the process. These errors change every time a run the docker environment: I have gotten errors ranging from a problem with the number of gpus I am passing in ---\\\\\\\\\\\\\\\"got gpu 0 expected 8\\\\\\\\\\\\\\\" to workers not being synchronized.\\\\\\\\nAny help in understanding what I am doing wrong would be greatly appreciated. I have looked online and in the issues on this repo, but have not found the solution. I am new to docker, so apologies if this is a simple problem to fix or if I did not set up something correctly.\\\\\\\\n\\\\\\\\nThe steps I have done are: \\\\\\\\n1. downloaded the docker enviroment: `docker pull hiyouga/verl:ngc-th2.7.0-cu12.6-vllm0.9.1`\\\\\\\\n2. used `docker run --gpus all --ipc=host --ulimit memlock=-1 -it --rm -v /HOME_DIR/EasyR1:/workspace/EasyR1 -w /workspace db618adc68d5 bash`\\\\\\\\n3. ran `bash examples/qwen2_5_vl_7b_geo3k_grpo.sh`\\\\\\\\n\\\\\\\\nAnd, most recently, I am getting the following traceback/error:\\\\\\\\n```\\\\\\\\n /usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_5_vl.py:930: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)\\\\\\\\n(WorkerDict pid=12946)   sizes = grid_thw.prod(-1) // merge_size // merge_size\\\\\\\\n(WorkerDict pid=12943) Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`. [repeated 7x across cluster]\\\\\\\\n[...]\\\\\\\\nNCCL version 2.21.5+cuda12.4\\\\\\\\n(WorkerDict pid=25283) [rank0]:[W713 04:12:56.043733966 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\\\\\\\\nTraceback (most recent call last):\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/lib/python3.10/runpy.py\\\\\\\\\\\\\\\", line 196, in _run_module_as_main\\\\\\\\n    return _run_code(code, main_globals, None,\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/lib/python3.10/runpy.py\\\\\\\\\\\\\\\", line 86, in _run_code\\\\\\\\n    exec(code, run_globals)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 128, in <module>\\\\\\\\n    main()\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 124, in main\\\\\\\\n    ray.get(runner.run.remote(ppo_config))\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 21, in auto_init_wrapper\\\\\\\\n    return fn(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 103, in wrapper\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2771, in get\\\\\\\\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 919, in get_objects\\\\\\\\n    raise value.as_instanceof_cause()\\\\\\\\nray.exceptions.RayTaskError(DistBackendError): ray::Runner.run() (pid=24423, ip=192.168.0.2, actor_id=971cb4efa74e3acac3b5b28a01000000, repr=<main.Runner object at 0x7f9a701eca90>)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 93, in run\\\\\\\\n    trainer.init_workers()\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 302, in init_workers\\\\\\\\n    self.actor_rollout_ref_wg.init_model()\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n    output = ray.get(output)\\\\\\\\nray.exceptions.RayTaskError(DistBackendError): ray::WorkerDict.actor_rollout_ref_init_model() (pid=29344, ip=192.168.0.2, actor_id=a4a7e38e959ee13c17167d8501000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f5dbc82b310>)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n    return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 366, in init_model\\\\\\\\n    self._build_model_optimizer(\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 242, in _build_model_optimizer\\\\\\\\n    dist.barrier()\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\\\\\\\\\\\\\\\", line 81, in wrapper\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\\\\\\\\\\\\\\\", line 4551, in barrier\\\\\\\\n    work = group.barrier(opts=opts)\\\\\\\\ntorch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.hpp:268, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\\\\\\\\nncclUnhandledCudaError: Call to CUDA function failed.\\\\\\\\nLast error:\\\\\\\\nCuda failure 'invalid argument'\\\\\\\\n(WorkerDict pid=29344) \\\\\\\\n(WorkerDict pid=29344) e4077bb80b9c:29344:30132 [0] transport/p2p.cc:275 NCCL WARN Cuda failure 'invalid argument'\\\\\\\\n\\\\\\\\n```\\\\\\\\n\\\\\\\\nAnother time I ran this I got:\\\\\\\\n`ValueError: Total available GPUs 0 is less than total desired GPUs 8.`\\\\\\\\n\\\\\\\\nAnother time i get:\\\\\\\\n`(Runner pid=7873)   File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/vllm/compilation/backends.py\\\\\\\\\\\\\\\", line 677, in __call__\\\\\\\\n(Runner pid=7873)     with torch.cuda.graph(cudagraph, pool=self.graph_pool):\\\\\\\\n(Runner pid=7873)   File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/cuda/graphs.py\\\\\\\\\\\\\\\", line 186, in __exit__\\\\\\\\n(Runner pid=7873)     self.cuda_graph.capture_end()\\\\\\\\n(Runner pid=7873)   File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/cuda/graphs.py\\\\\\\\\\\\\\\", line 84, in capture_end\\\\\\\\n(Runner pid=7873)     super().capture_end()\\\\\\\\n(Runner pid=7873) RuntimeError: CUDA error: operation not permitted\\\\\\\\n(Runner pid=7873) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\\\\\\\n(Runner pid=7873) For debugging consider passing CUDA_LAUNCH_BLOCKING=1\\\\\\\\n(Runner pid=7873) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\\\\\\\n(Runner pid=7873) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WorkerDict.actor_rollout_ref_init_model() (pid=12947, ip=192.168.0.3, actor_id=92b10155f368aac73ed7f31601000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f2ce366b2e0>)`\\\\\\\\n\\\\\\\\nI am not sure what I am doing wrong. Shouldn't this script run without any issues? At least, that is what the tutorial makes it seem like. \\\\\\\\n\\\\\\\\nThank you!\\\\\\\\n\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"tokenized_path \\\\\\\\u652f\\\\\\\\u6301token\\\\\\\\u4fdd\\\\\\\\u5b58\\\\\\\\u4e48\\\\\\\\uff1f\\\\\\\",\\\\\\\"number\\\\\\\":26,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5927\\\\\\\\u4f6c\\\\\\\\u4f60\\\\\\\\u597d\\\\\\\\uff0c\\\\\\\\u60f3\\\\\\\\u77e5\\\\\\\\u9053EasyR1\\\\\\\\u6709\\\\\\\\u7c7b\\\\\\\\u4f3cllamafactory\\\\\\\\u4e2d\\\\\\\\uff0c\\\\\\\\u652f\\\\\\\\u6301tokenized_path\\\\\\\\u8fdb\\\\\\\\u884c\\\\\\\\u8bad\\\\\\\\u7ec3token\\\\\\\\u4fdd\\\\\\\\u5b58\\\\\\\\u7684\\\\\\\\u8bbe\\\\\\\\u7f6e\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\\u4e48\\\\\\\\uff1f\\\\\\\\n\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"any plan for supporting Flow-GRPO?\\\\\\\",\\\\\\\"number\\\\\\\":25,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"https://github.com/yifan123/flow_grpo\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"Some bug with training\\\\\\\",\\\\\\\"number\\\\\\\":24,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I tried to run the tutorial on my server recently. It is a 8*A100 GPUs with 80GB RAM for each. As the cuda version in the host in below the lowest requirement and it is hard to install another version. I choose to use docker to build the environment, I pull the container and start the container. The script in the examples work proporly in the first and second step. However, when I get to the third step of updating the policy, it just clash.\\\\\\\\n\\\\\\\\n                                                                                                                           \\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 22, in auto_init_wrapper             \\\\\\\\nray.get(runner.run.remote(ppo_config))                                                                                     \\\\\\\\n                                                                                                                           \\\\\\\\n                                                                                                                           \\\\\\\\n                                                                                                                           \\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 104, in wrapper                    \\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2849, in get                                 \\\\\\\\n(Runner pid=16098) wandb:              critic/rewards/mean 0.33543                                                         \\\\\\\\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 937, in get_objects\\\\\\\\n(pid=16098) Running step 0:   5%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b                                                   | 3.00/60.0 [16:06<4:30:39, 285s/it]\\\\\\\\n\\\\\\\\n(pid=33233) Train mini-batches 1: 100%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588| 4.00/4.00 [12:25<00:00, 22.8s/it]\\\\\\\\n    raise value.as_instanceof_cause()\\\\\\\\n\\\\\\\\n(pid=33233) Update policy 2: 100%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u2588| 5.00/5.00 [11:53<00:00, 4.64s/it]ray.exceptions.RayTaskError(IndexError): ray::Runner.run() (pid=16098, ip=172.17.0.2, actor_id=462c3f5e6c3ec7441f02a99e01000000, repr=<main.Runner object at 0x707158542f20>)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 94, in run\\\\\\\\n    trainer.fit()\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 607, in fit\\\\\\\\n    old_log_probs = self.actor_rollout_ref_wg.compute_log_probs(batch)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n    output = ray.get(output)\\\\\\\\nray.exceptions.RayTaskError(IndexError): ray::WorkerDict.actor_rollout_ref_compute_log_probs() (pid=33458, ip=172.17.0.2, actor_id=11af29bb9fed89a5977131d901000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x72fdd7d62290>)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n    return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 585, in compute_log_probs\\\\\\\\n    output = self.actor.compute_log_prob(data=data)\\\\\\\\n  File \\\\\\\\\\\\\\\"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\\\\\\\\\\\\\\\", line 116, in decorate_context\\\\\\\\n    return func(*args, **kwargs)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/workers/actor/dp_actor.py\\\\\\\\\\\\\\\", line 199, in compute_log_prob\\\\\\\\n    micro_batches, batch_idx_list = prepare_dynamic_batch(data, max_token_len=max_token_len)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/utils/seqlen_balancing.py\\\\\\\\\\\\\\\", line 307, in prepare_dynamic_batch\\\\\\\\n    batch, batch_idx_list = rearrange_micro_batches(data.batch, max_token_len=max_token_len)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/utils/seqlen_balancing.py\\\\\\\\\\\\\\\", line 261, in rearrange_micro_batches\\\\\\\\n    micro_bsz_idx = get_seqlen_balanced_partitions(effective_seqlen, num_micro_batches, equal_size=False)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/utils/seqlen_balancing.py\\\\\\\\\\\\\\\", line 184, in get_seqlen_balanced_partitions\\\\\\\\n    partitions = karmarkar_karp(seqlen_list=seqlen_list, k_partitions=k_partitions, equal_size=equal_size)\\\\\\\\n  File \\\\\\\\\\\\\\\"/workspace/EasyR1/verl/utils/seqlen_balancing.py\\\\\\\\\\\\\\\", line 114, in karmarkar_karp\\\\\\\\n    heapq.heappush(states_pq, State(items=[(idx, seqlen)], k=k_partitions))\\\\\\\\n(Runner pid=16098) wandb:               critic/rewards/min 0                                                               \\\\\\\\n(Runner pid=16098) wandb:                 critic/score/max 1                                                               \\\\\\\\n(Runner pid=16098) wandb:                critic/score/mean 0.33543                                                         \\\\\\\\n(Runner pid=16098) wandb:                 critic/score/min 0                                                               \\\\\\\\n(Runner pid=16098) wandb:       global_seqlen/balanced_max 247670                                                          \\\\\\\\n(Runner pid=16098) wandb:       global_seqlen/balanced_min 247090                                                          \\\\\\\\n(Runner pid=16098) wandb:                global_seqlen/max 254919                                                          \\\\\\\\n(Runner pid=16098) wandb:               global_seqlen/mean 247163.25                                                       \\\\\\\\n(Runner pid=16098) wandb:                global_seqlen/min 240316                                                          \\\\\\\\n(Runner pid=16098) wandb:        global_seqlen/minmax_diff 14603                                                           \\\\\\\\n(Runner pid=16098) wandb:          perf/cpu_memory_used_gb 107.23108                                                       \\\\\\\\n(Runner pid=16098) wandb:     perf/max_memory_allocated_gb 42.98667                                                        \\\\\\\\n(Runner pid=16098) wandb:      perf/max_memory_reserved_gb 64.66187                                                        \\\\\\\\n(Runner pid=16098) wandb:                   perf/mfu_actor 0.33784                                                         \\\\\\\\n(Runner pid=16098) wandb:                  perf/throughput 955.20544                                                       \\\\\\\\n(Runner pid=16098) wandb:               perf/time_per_step 258.75402                                                       \\\\\\\\n(Runner pid=16098) wandb:            perf/total_num_tokens 1977306                                                         \\\\\\\\n(Runner pid=16098) wandb:         prompt_length/clip_ratio 0                                                               \\\\\\\\n(Runner pid=16098) wandb:                prompt_length/max 776                                                             \\\\\\\\n(Runner pid=16098) wandb:               prompt_length/mean 425.51953                                                       \\\\\\\\n(Runner pid=16098) wandb:                prompt_length/min 387                                                             \\\\\\\\n(Runner pid=16098) wandb:       response_length/clip_ratio 0.00039                                                         \\\\\\\\n(Runner pid=16098) wandb:              response_length/max 2048                                                            \\\\\\\\n(Runner pid=16098) wandb:             response_length/mean 346.86563                                                       \\\\\\\\n(Runner pid=16098) wandb:              response_length/min 56                                                              \\\\\\\\n(Runner pid=16098) wandb:                  reward/accuracy 0.325                                                           \\\\\\\\n(Runner pid=16098) wandb:                    reward/format 0.4293                                                          \\\\\\\\n(Runner pid=16098) wandb:                   reward/overall 0.33543                                                         \\\\\\\\n(Runner pid=16098) wandb:          timing_per_token_ms/adv 6e-05                                                           \\\\\\\\n(Runner pid=16098) wandb:          timing_per_token_ms/gen 0.08111                                                         \\\\\\\\n(Runner pid=16098) wandb:          timing_per_token_ms/old 0.01803                                                         \\\\\\\\n(Runner pid=16098) wandb:          timing_per_token_ms/ref 0.01467                                                         \\\\\\\\n(Runner pid=16098) wandb:       timing_per_token_ms/reward 0.00238                                                         \\\\\\\\n(Runner pid=16098) wandb: timing_per_token_ms/update_actor 0.0604                                                          \\\\\\\\n(Runner pid=16098) wandb:                     timing_s/adv 0.12637                                                         \\\\\\\\n(Runner pid=16098) wandb:                     timing_s/gen 72.02265                                                        \\\\\\\\n(Runner pid=16098) wandb:                     timing_s/old 35.64964                                                        \\\\\\\\n(Runner pid=16098) wandb:                     timing_s/ref 29.00215                                                        \\\\\\\\n(Runner pid=16098) wandb:                  timing_s/reward 2.11218                                                         \\\\\\\\n(Runner pid=16098) wandb:                    timing_s/step 258.75402                                                       \\\\\\\\n(Runner pid=16098) wandb:            timing_s/update_actor 119.43438                                                       \\\\\\\\n(Runner pid=16098) wandb:              val/accuracy_reward 0.3827                                                          \\\\\\\\n(Runner pid=16098) wandb:                val/format_reward 0.1797                                                          \\\\\\\\n(Runner pid=16098) wandb:               val/overall_reward 0.3624                                                          \\\\\\\\n(Runner pid=16098) wandb:                 val/reward_score 0.3624                                                          \\\\\\\\n(Runner pid=16098) wandb:     val_prompt_length/clip_ratio 0                                                               \\\\\\\\n(Runner pid=16098) wandb:            val_prompt_length/max 784                                                             \\\\\\\\n(Runner pid=16098) wandb:           val_prompt_length/mean 428.5574                                                        \\\\\\\\n(Runner pid=16098) wandb:            val_prompt_length/min 387                                                             \\\\\\\\n(Runner pid=16098) wandb:   val_response_length/clip_ratio 0.00333                                                         \\\\\\\\n(Runner pid=16098) wandb:          val_response_length/max 2048                                                            \\\\\\\\n(Runner pid=16098) wandb:         val_response_length/mean 371.5441                                                        \\\\\\\\n(Runner pid=16098) wandb:          val_response_length/min 60                                                              \\\\\\\\n(Runner pid=16098) wandb:                                                                                                  \\\\\\\\n(Runner pid=16098) wandb: \\\\\\\\ud83d\\\\\\\\ude80 View run qwen2_5_vl_7b_geo_grpo at: https://wandb.ai/GreatResearchTakesTime/easy_r1/runs/zabor1wb                                                                                                                        \\\\\\\\n(Runner pid=16098) wandb: \\\\\\\\u2b50\\\\\\\\ufe0f View project at: https://wandb.ai/GreatResearchTakesTime/easy_r1                              \\\\\\\\n(Runner pid=16098) wandb: Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)                    \\\\\\\\n(Runner pid=16098) wandb: Find logs at: ./wandb/run-20250716_071305-zabor1wb/logs                                          \\\\\\\\n(pid=16098) Running step 0:   5%|\\\\\\\\u2588\\\\\\\\u2588\\\\\\\\u258b                                                   | 3.00/60.0 [16:08<4:30:39, 285s/itException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookException ignored in: Exception ignored in sys.unraisablehookroot@60b7a9c\\\\\\\\nroot@60b7a9cfca50:/workspace/EasyR1# \\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"tensor_parallel_size\\\\\\\\u8bbe\\\\\\\\u7f6e\\\\\\\\u4e3a2\\\\\\\\u65f6\\\\\\\\u5019\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\",\\\\\\\"number\\\\\\\":23,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u9ed8\\\\\\\\u8ba4\\\\\\\\u7684tensor_parallel_size\\\\\\\\u4e3a1\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u60f3\\\\\\\\u4fee\\\\\\\\u6539\\\\\\\\u62102\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\\uff1a\\\\\\\\n```\\\\\\\\n2025-07-17 17:00:39 | INFO | stdout | \\\\\\\\u001b[36m(pid=47761)\\\\\\\\u001b[0m NCCL version 2.21.5+cuda11.0\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | Traceback (most recent call last):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/runpy.py\\\\\\\\\\\\\\\", line 196, in _run_module_as_main\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return _run_code(code, main_globals, None,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/runpy.py\\\\\\\\\\\\\\\", line 86, in _run_code\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     exec(code, run_globals)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 216, in <module>\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     main()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 212, in main\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     ray.get(runner.run.remote(ppo_config))\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\\\\\\\\\\\\\\\", line 21, in auto_init_wrapper\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\\\\\\\\\\\\\\\", line 103, in wrapper\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 2822, in get\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/ray/_private/worker.py\\\\\\\\\\\\\\\", line 930, in get_objects\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     raise value.as_instanceof_cause()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | ray.exceptions.RayTaskError(KeyError): \\\\\\\\u001b[36mray::Runner.run()\\\\\\\\u001b[39m (pid=41424, ip=10.140.37.138, actor_id=aac3cbdc33303e9fb12a736501000000, repr=<main.Runner object at 0x7fc5f5d2cee0>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/trainer/main.py\\\\\\\\\\\\\\\", line 99, in run\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     trainer.init_workers()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/trainer/ray_trainer.py\\\\\\\\\\\\\\\", line 443, in init_workers\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.actor_rollout_wg.init_model()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 47, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     output = ray.get(output)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | ray.exceptions.RayTaskError(KeyError): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_init_model()\\\\\\\\u001b[39m (pid=47760, ip=10.140.37.138, actor_id=c03b337d86fc2c524621f42701000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f31ad170c10>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 384, in init_model\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self._build_rollout()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 315, in _build_rollout\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.rollout = vLLMRollout(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 76, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.inference_engine = LLM(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1161, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 247, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.llm_engine = LLMEngine.from_engine_args(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/engine/llm_engine.py\\\\\\\\\\\\\\\", line 510, in from_engine_args\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return engine_cls.from_vllm_config(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 112, in from_vllm_config\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return cls(vllm_config=vllm_config,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 92, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCoreClient.make_client(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 75, in make_client\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return InprocClient(vllm_config, executor_class, log_stats)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 198, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCore(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 64, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.model_executor = executor_class(vllm_config)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/executor_base.py\\\\\\\\\\\\\\\", line 52, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self._init_executor()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 121, in _init_executor\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.collective_rpc(\\\\\\\\\\\\\\\"init_device\\\\\\\\\\\\\\\")\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     answer = run_method(self.driver_worker, method, args, kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2456, in run_method\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/worker/worker_base.py\\\\\\\\\\\\\\\", line 604, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.worker.init_device()  # type: ignore\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 135, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     init_worker_distributed_environment(self.vllm_config, self.rank,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 326, in init_worker_distributed_environment\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 1022, in ensure_model_parallel_initialized\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     initialize_model_parallel(tensor_model_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 975, in initialize_model_parallel\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     _TP = init_model_parallel_group(group_ranks,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 788, in init_model_parallel_group\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return GroupCoordinator(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 252, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.device_communicator = device_comm_cls(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py\\\\\\\\\\\\\\\", line 47, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     self.ca_comm = CustomAllreduce(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 145, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     if not current_platform.is_rocm() and not _can_p2p(rank, world_size):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 37, in _can_p2p\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     if not gpu_p2p_access_check(rank, i):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\\\\\\\\\\\\\\\", line 248, in gpu_p2p_access_check\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr |     return _gpu_p2p_access_cache[f\\\\\\\\\\\\\\\"{src}->{tgt}\\\\\\\\\\\\\\\"]\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | KeyError: '1->0'\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 16:59:58 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 16:59:59 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 16:59:59 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 17:00:05 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 17:00:06 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 17:00:07 | ERROR | stderr | \\\\\\\\n\\\\\\\\u001b[36m(pid=46880)\\\\\\\\u001b[0m 2025-07-17 17:00:07 | ERROR | stderr |\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_init_model()\\\\\\\\u001b[39m (pid=47762, ip=10.140.37.138, actor_id=fd8e5e2f1f8107d072c83c9801000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fc572d50c10>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 384, in init_model\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._build_rollout()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 315, in _build_rollout\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.rollout = vLLMRollout(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 76, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.inference_engine = LLM(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1161, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 247, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.llm_engine = LLMEngine.from_engine_args(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/engine/llm_engine.py\\\\\\\\\\\\\\\", line 510, in from_engine_args\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return engine_cls.from_vllm_config(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 112, in from_vllm_config\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return cls(vllm_config=vllm_config,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 92, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCoreClient.make_client(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 75, in make_client\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return InprocClient(vllm_config, executor_class, log_stats)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 198, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCore(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 64, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.model_executor = executor_class(vllm_config)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/executor_base.py\\\\\\\\\\\\\\\", line 52, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._init_executor()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 121, in _init_executor\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.collective_rpc(\\\\\\\\\\\\\\\"init_device\\\\\\\\\\\\\\\")\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     answer = run_method(self.driver_worker, method, args, kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2456, in run_method\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/worker/worker_base.py\\\\\\\\\\\\\\\", line 604, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.worker.init_device()  # type: ignore\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 135, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     init_worker_distributed_environment(self.vllm_config, self.rank,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 326, in init_worker_distributed_environment\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 1022, in ensure_model_parallel_initialized\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     initialize_model_parallel(tensor_model_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 975, in initialize_model_parallel\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     _TP = init_model_parallel_group(group_ranks,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 788, in init_model_parallel_group\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return GroupCoordinator(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 252, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.device_communicator = device_comm_cls(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py\\\\\\\\\\\\\\\", line 47, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.ca_comm = CustomAllreduce(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 145, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not current_platform.is_rocm() and not _can_p2p(rank, world_size):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 37, in _can_p2p\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not gpu_p2p_access_check(rank, i):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\\\\\\\\\\\\\\\", line 248, in gpu_p2p_access_check\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return _gpu_p2p_access_cache[f\\\\\\\\\\\\\\\"{src}->{tgt}\\\\\\\\\\\\\\\"]\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | KeyError: '1->0'\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_init_model()\\\\\\\\u001b[39m (pid=47761, ip=10.140.37.138, actor_id=47ed97652b87fc5762ebbb2401000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7eeb7b44f0a0>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 384, in init_model\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._build_rollout()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 315, in _build_rollout\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.rollout = vLLMRollout(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 76, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.inference_engine = LLM(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1161, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 247, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.llm_engine = LLMEngine.from_engine_args(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/engine/llm_engine.py\\\\\\\\\\\\\\\", line 510, in from_engine_args\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return engine_cls.from_vllm_config(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 112, in from_vllm_config\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return cls(vllm_config=vllm_config,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 92, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCoreClient.make_client(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 75, in make_client\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return InprocClient(vllm_config, executor_class, log_stats)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 198, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCore(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 64, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.model_executor = executor_class(vllm_config)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/executor_base.py\\\\\\\\\\\\\\\", line 52, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._init_executor()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 121, in _init_executor\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.collective_rpc(\\\\\\\\\\\\\\\"init_device\\\\\\\\\\\\\\\")\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     answer = run_method(self.driver_worker, method, args, kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2456, in run_method\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/worker/worker_base.py\\\\\\\\\\\\\\\", line 604, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.worker.init_device()  # type: ignore\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 135, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     init_worker_distributed_environment(self.vllm_config, self.rank,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 326, in init_worker_distributed_environment\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 1022, in ensure_model_parallel_initialized\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     initialize_model_parallel(tensor_model_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 975, in initialize_model_parallel\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     _TP = init_model_parallel_group(group_ranks,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 788, in init_model_parallel_group\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return GroupCoordinator(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 252, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.device_communicator = device_comm_cls(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py\\\\\\\\\\\\\\\", line 47, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.ca_comm = CustomAllreduce(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 145, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not current_platform.is_rocm() and not _can_p2p(rank, world_size):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 37, in _can_p2p\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not gpu_p2p_access_check(rank, i):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\\\\\\\\\\\\\\\", line 248, in gpu_p2p_access_check\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return _gpu_p2p_access_cache[f\\\\\\\\\\\\\\\"{src}->{tgt}\\\\\\\\\\\\\\\"]\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | KeyError: '0->1'\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \\\\\\\\u001b[36mray::WorkerDict.actor_rollout_init_model()\\\\\\\\u001b[39m (pid=46880, ip=10.140.37.138, actor_id=f0ee4094614e96a389af02d101000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fca14d141f0>)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/ray/base.py\\\\\\\\\\\\\\\", line 432, in func\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return getattr(self.worker_dict[key], name)(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/single_controller/base/decorator.py\\\\\\\\\\\\\\\", line 207, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 384, in init_model\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._build_rollout()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/fsdp_workers.py\\\\\\\\\\\\\\\", line 315, in _build_rollout\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.rollout = vLLMRollout(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/visual-code/EasyR1/verl/workers/rollout/vllm_rollout_spmd.py\\\\\\\\\\\\\\\", line 76, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.inference_engine = LLM(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 1161, in inner\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return fn(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/entrypoints/llm.py\\\\\\\\\\\\\\\", line 247, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.llm_engine = LLMEngine.from_engine_args(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/engine/llm_engine.py\\\\\\\\\\\\\\\", line 510, in from_engine_args\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return engine_cls.from_vllm_config(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 112, in from_vllm_config\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return cls(vllm_config=vllm_config,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py\\\\\\\\\\\\\\\", line 92, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCoreClient.make_client(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 75, in make_client\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return InprocClient(vllm_config, executor_class, log_stats)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core_client.py\\\\\\\\\\\\\\\", line 198, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.engine_core = EngineCore(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/engine/core.py\\\\\\\\\\\\\\\", line 64, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.model_executor = executor_class(vllm_config)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/executor_base.py\\\\\\\\\\\\\\\", line 52, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self._init_executor()\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 121, in _init_executor\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.collective_rpc(\\\\\\\\\\\\\\\"init_device\\\\\\\\\\\\\\\")\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\\\\\\\\\\\\\\\", line 56, in collective_rpc\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     answer = run_method(self.driver_worker, method, args, kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/utils.py\\\\\\\\\\\\\\\", line 2456, in run_method\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return func(*args, **kwargs)\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/worker/worker_base.py\\\\\\\\\\\\\\\", line 604, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.worker.init_device()  # type: ignore\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 135, in init_device\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     init_worker_distributed_environment(self.vllm_config, self.rank,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\\\\\\\\\\\\\\\", line 326, in init_worker_distributed_environment\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 1022, in ensure_model_parallel_initialized\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     initialize_model_parallel(tensor_model_parallel_size,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 975, in initialize_model_parallel\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     _TP = init_model_parallel_group(group_ranks,\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 788, in init_model_parallel_group\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return GroupCoordinator(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/parallel_state.py\\\\\\\\\\\\\\\", line 252, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.device_communicator = device_comm_cls(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/cuda_communicator.py\\\\\\\\\\\\\\\", line 47, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     self.ca_comm = CustomAllreduce(\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 145, in __init__\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not current_platform.is_rocm() and not _can_p2p(rank, world_size):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py\\\\\\\\\\\\\\\", line 37, in _can_p2p\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     if not gpu_p2p_access_check(rank, i):\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |   File \\\\\\\\\\\\\\\"/mnt/petrelfs/sunhaoyu/miniconda3/envs/visual/lib/python3.10/site-packages/vllm/distributed/device_communicators/custom_all_reduce_utils.py\\\\\\\\\\\\\\\", line 248, in gpu_p2p_access_check\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr |     return _gpu_p2p_access_cache[f\\\\\\\\\\\\\\\"{src}->{tgt}\\\\\\\\\\\\\\\"]\\\\\\\\n2025-07-17 17:00:39 | ERROR | stderr | \\\\\\\\u001b[36m(pid=41424)\\\\\\\\u001b[0m 2025-07-17 17:00:39 | ERROR | stderr | KeyError: '0->1'```\\\\\\\\n\\\\\\\\n\\\\\\\\u6211\\\\\\\\u5c1d\\\\\\\\u8bd5\\\\\\\\u4e86\\\\\\\\u4e00\\\\\\\\u4e9b\\\\\\\\u65b9\\\\\\\\u6cd5\\\\\\\\uff0c\\\\\\\\u6bd4\\\\\\\\u5982\\\\\\\\u8bbe\\\\\\\\u7f6eexport VLLM_RPC_BASE_PATH=/dev/shm\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u6ca1\\\\\\\\u6709\\\\\\\\u7528\\\\\\\\uff0c\\\\\\\\u4ee3\\\\\\\\u7801\\\\\\\\u672c\\\\\\\\u8eab\\\\\\\\u662f\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u8fd0\\\\\\\\u884c\\\\\\\\u7684\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u53ea\\\\\\\\u6539\\\\\\\\u4e86\\\\\\\\u8fd9\\\\\\\\u4e00\\\\\\\\u4e2a\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u52a0\\\\\\\\u8f7d\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\\uff0cv3.0.1\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u52a0\\\\\\\\u8f7d\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\uff1f\\\\\\\",\\\\\\\"number\\\\\\\":22,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"v3.0.1\\\\\\\\u4e2d\\\\\\\\u7684\\\\\\\\uff1a\\\\\\\\nEasyR1/verl/utils/dataset.py line 132\\\\\\\\n...\\\\\\\\n        if os.path.isdir(data_path):\\\\\\\\n            # when we use dataset builder, we should always refer to the train split\\\\\\\\n            **file_type = os.path.splitext(os.listdir(data_path)[0])[-1][1:].replace(\\\\\\\\\\\\\\\"jsonl\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\")**\\\\\\\\n            self.dataset = load_dataset(file_type, data_dir=data_path, split=data_split)\\\\\\\\n        elif os.path.isfile(data_path):\\\\\\\\n            file_type = os.path.splitext(data_path)[-1][1:].replace(\\\\\\\\\\\\\\\"jsonl\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"json\\\\\\\\\\\\\\\")\\\\\\\\n            self.dataset = load_dataset(file_type, data_files=data_path, split=data_split)\\\\\\\\n        else:\\\\\\\\n            # load remote dataset from huggingface hub\\\\\\\\n            self.dataset = load_dataset(data_path, split=data_split)\\\\\\\\n...\\\\\\\\n\\\\\\\\n\\\\\\\\u8fd9\\\\\\\\u4e00\\\\\\\\u884c\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\\uff0c\\\\\\\\u5904\\\\\\\\u7406\\\\\\\\u4e0d\\\\\\\\u4e86\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u7684./geomery3k \\\\\\\\u8def\\\\\\\\u5f84\\\\\\\\u3002\\\\\\\\n\\\\\\\\u6539\\\\\\\\u6210v3.0.0\\\\\\\\u4e2d\\\\\\\\u7684\\\\\\\\nself.dataset = load_dataset(\\\\\\\\\\\\\\\"parquet\\\\\\\\\\\\\\\", data_dir=data_path, split=data_split)\\\\\\\\n\\\\\\\\u5c31\\\\\\\\u6b63\\\\\\\\u5e38\\\\\\\\u4e86\\\\\\\\u3002\\\\\\\\n\\\\\\\\n\\\\\\\\u662f\\\\\\\\u6211\\\\\\\\u5b9a\\\\\\\\u4e49\\\\\\\\u672c\\\\\\\\u5730\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u7684\\\\\\\\u65b9\\\\\\\\u5f0f\\\\\\\\u4e0d\\\\\\\\u5bf9\\\\\\\\u4e48\\\\\\\\uff1f\\\\\\\\n\\\\\\\\n\\\\\\\\n```bash\\\\\\\\nMODEL_PATH=/home/xxx/models/Qwen2.5-VL-3B-Instruct/  # replace it with your local file path\\\\\\\\n\\\\\\\\npython3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n    config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n    data.train_files=./geometry3k@train \\\\\\\\\\\\\\\\\\\\\\\\n    data.val_files=./geometry3k@test \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.tensor_parallel_size=2 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.experiment_name=XiaomiMiMo_7B_RL_geo_grpo \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.fsdp.torch_dtype=bf16 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.optim.strategy=adamw_bf16 \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.logger=['console'] \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.n_gpus_per_node=2\\\\\\\\n```\\\\\\\\n\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"v100\\\\\\\\u4e0d\\\\\\\\u517c\\\\\\\\u5bb9flash atten\\\\\\\\uff0c\\\\\\\\u4f7f\\\\\\\\u7528xformer\\\\\\\\u4ecd\\\\\\\\u7136\\\\\\\\u62a5\\\\\\\\u9519\\\\\\\",\\\\\\\"number\\\\\\\":21,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u4e4b\\\\\\\\u524d\\\\\\\\u7528H100\\\\\\\\u8dd1\\\\\\\\u901a\\\\\\\\u4e86\\\\\\\\u4ee3\\\\\\\\u7801\\\\\\\\uff0c\\\\\\\\u4f46\\\\\\\\u5207\\\\\\\\u5230V100\\\\\\\\u663e\\\\\\\\u793a\\\\\\\\u4e0d\\\\\\\\u652f\\\\\\\\u6301flash atten2\\\\\\\\uff0c\\\\\\\\u6211\\\\\\\\u5728\\\\\\\\u65b0\\\\\\\\u7684\\\\\\\\u73af\\\\\\\\u5883\\\\\\\\u4e0b\\\\\\\\u6ca1\\\\\\\\u6709\\\\\\\\u4e0b\\\\\\\\u8f7dflash atten\\\\\\\\uff0c\\\\\\\\u5e76\\\\\\\\u5728\\\\\\\\u914d\\\\\\\\u7f6e\\\\\\\\u6587\\\\\\\\u4ef6\\\\\\\\u4e2d\\\\\\\\u6307\\\\\\\\u5b9aattn_implementation: xformers\\\\\\\\uff0c\\\\\\\\u7981\\\\\\\\u7528\\\\\\\\u6240\\\\\\\\u6709atten\\\\\\\\u53c2\\\\\\\\u6570\\\\\\\\u540e\\\\\\\\u8fd8\\\\\\\\u662f\\\\\\\\u62a5\\\\\\\\u540c\\\\\\\\u6837\\\\\\\\u7684\\\\\\\\u4e0d\\\\\\\\u652f\\\\\\\\u6301flash atten2\\\\\\\\u7684\\\\\\\\u9519\\\\\\\\u8bef\\\\\\\\uff0c\\\\\\\\u611f\\\\\\\\u8c22\\\\\\\\u56de\\\\\\\\u590d\\\\\\\\n raise ImportError(f\\\\\\\\\\\\\\\"{preface} the package flash_attn seems to be not installed. {install_message}\\\\\\\\\\\\\\\")\\\\\\\\n(Runner pid=320551) ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"nnodes>1 Software caused connection abort\\\\\\\",\\\\\\\"number\\\\\\\":20,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"<img width=\\\\\\\\\\\\\\\"2478\\\\\\\\\\\\\\\" height=\\\\\\\\\\\\\\\"1114\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/14086594-9e11-4b81-b528-f922219dd7c5\\\\\\\\\\\\\\\" />\\\\\\\\nyaml nnodes: 2or3or4 will report above error\\\\\\\\ni have checked ray status, all 4 nodes are settled. i run the bash script in the master node, it seems always try to connect to a node that do not exist in my ray list.\\\\\\\\n\\\\\\\\n<!-- Failed to upload \\\\\\\\\\\\\\\"image.png\\\\\\\\\\\\\\\" -->\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"video-image mixed dataset\\\\\\\",\\\\\\\"number\\\\\\\":19,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"video or image \\\\\\\\u5355\\\\\\\\u72ec\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u662f\\\\\\\\u6b63\\\\\\\\u5e38\\\\\\\\u7684\\\\\\\\n\\\\\\\\u4f46\\\\\\\\u662f\\\\\\\\u6df7\\\\\\\\u5408\\\\\\\\u8bad\\\\\\\\u4f1a\\\\\\\\u5361\\\\\\\\u5728Update policy\\\\\\\\n\\\\\\\\n\\\\\\\\u91c7\\\\\\\\u7528\\\\\\\\u7684\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u5f62\\\\\\\\u5f0f\\\\\\\\u662f\\\\\\\\uff1a\\\\\\\\n ```\\\\\\\\nif example['data_type'] == 'image':\\\\\\\\n                data = {\\\\\\\\n                    \\\\\\\\\\\\\\\"problem_type\\\\\\\\\\\\\\\": example['problem_type'],\\\\\\\\n                    \\\\\\\\\\\\\\\"data_source\\\\\\\\\\\\\\\": data_source,\\\\\\\\n                    \\\\\\\\\\\\\\\"problem\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"<image>\\\\\\\\\\\\\\\" + QUESTION_TEMPLATE.format(Question=question) + TYPE_TEMPLATE[example['problem_type']],\\\\\\\\n                    \\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\": answer,\\\\\\\\n                    \\\\\\\\\\\\\\\"images\\\\\\\\\\\\\\\": [video_path],\\\\\\\\n                    \\\\\\\\\\\\\\\"videos\\\\\\\\\\\\\\\": [],\\\\\\\\n                    \\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\": answer_raw\\\\\\\\n                }\\\\\\\\n            else:\\\\\\\\n                data = {\\\\\\\\n                    \\\\\\\\\\\\\\\"problem_type\\\\\\\\\\\\\\\": example['problem_type'],\\\\\\\\n                    \\\\\\\\\\\\\\\"data_source\\\\\\\\\\\\\\\": data_source,\\\\\\\\n                    \\\\\\\\\\\\\\\"problem\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"<video>\\\\\\\\\\\\\\\" + QUESTION_TEMPLATE.format(Question=question) + TYPE_TEMPLATE[example['problem_type']],\\\\\\\\n                    \\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\": answer,\\\\\\\\n                    \\\\\\\\\\\\\\\"images\\\\\\\\\\\\\\\": [],\\\\\\\\n                    \\\\\\\\\\\\\\\"videos\\\\\\\\\\\\\\\": [image_path],\\\\\\\\n                    \\\\\\\\\\\\\\\"ground_truth\\\\\\\\\\\\\\\": answer_raw\\\\\\\\n                }\\\\\\\\n```\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"During debugging, the program cannot enter the reward function, even though a breakpoint is set.\\\\\\\",\\\\\\\"number\\\\\\\":18,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi, Thanks for your great work. \\\\\\\\n\\\\\\\\nI want to incorporate my reward function into easyr1. However, the program cannot enter the reward function, even though a breakpoint is set.\\\\\\\\n\\\\\\\\nIt seems that the program can only enter the line without further getting into the reward function. \\\\\\\\n\\\\\\\\nreward_tensor, reward_metrics = ray.get(self.val_reward_fn.compute_reward.remote(test_batch))\\\\\\\\nhttps://github.com/hiyouga/EasyR1/blob/1bdeb1296c120ab9875ce3f4f083eb8b59c829c6/verl/trainer/ray_trainer.py#L420\\\\\\\\n\\\\\\\\nI use the Ray Distributed Debugger following verl's docs. Could you help me to take a check?\\\\\\\\n\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5e0c\\\\\\\\u671b\\\\\\\\u80fd\\\\\\\\u591f\\\\\\\\u548cLLama factory\\\\\\\\u4e00\\\\\\\\u6837\\\\\\\\u63a8\\\\\\\\u51faweb ui\\\\\\\",\\\\\\\"number\\\\\\\":17,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5e0c\\\\\\\\u671b\\\\\\\\u80fd\\\\\\\\u591f\\\\\\\\u548cLLama factory\\\\\\\\u4e00\\\\\\\\u6837\\\\\\\\u63a8\\\\\\\\u51faweb ui \\\\\\\\u671f\\\\\\\\u5f85\\\\\\\\u8fd9\\\\\\\\u4e2a\\\\\\\\u80fd\\\\\\\\u529b\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"Is it possible to support the SGlang Rollout?\\\\\\\",\\\\\\\"number\\\\\\\":16,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Sglang rollout is also a very convenient inference engine. I'd like to ask if you've considered officially supporting the Sglang backend. \\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"Support for 3D MLLM (e.g., PointLLM) Integration\\\\\\\",\\\\\\\"number\\\\\\\":15,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi team,\\\\\\\\n\\\\\\\\nI'm wondering if EasyR1 currently supports 3D multimodal large language models like PointLLM?\\\\\\\\n\\\\\\\\nIf this feature isn't available yet or not in the roadmap, could you provide some guidance on how to integrate 3D MLLMs with the GRPO algorithm? Any development tips or architectural considerations would be greatly appreciated.\\\\\\\\n\\\\\\\\nThank you for your time and the excellent work on this project!\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u4f55\\\\\\\\u5728\\\\\\\\u8bad\\\\\\\\u7ec3\\\\\\\\u7684\\\\\\\\u65f6\\\\\\\\u5019\\\\\\\\u51bb\\\\\\\\u7ed3\\\\\\\\u8bed\\\\\\\\u8a00\\\\\\\\u6a21\\\\\\\\u5757\\\\\\\\uff0c\\\\\\\\u53ea\\\\\\\\u8bad\\\\\\\\u7ec3vision tower\\\\\\\\uff1f\\\\\\\",\\\\\\\"number\\\\\\\":14,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u6211\\\\\\\\u5728https://github.com/hiyouga/EasyR1/blob/main/verl/workers/fsdp_workers.py#L239 \\\\\\\\u4e4b\\\\\\\\u540e\\\\\\\\u52a0\\\\\\\\u4e86\\\\\\\\u8fd9\\\\\\\\u6837\\\\\\\\u7684\\\\\\\\u4ee3\\\\\\\\u7801\\\\\\\\uff0c\\\\\\\\u4e0d\\\\\\\\u77e5\\\\\\\\u9053\\\\\\\\u5bf9\\\\\\\\u4e0d\\\\\\\\u5bf9\\\\\\\\uff1f\\\\\\\\n\\\\\\\\n        if model_config.freeze_language_backbone:\\\\\\\\n            model.requires_grad_(False)\\\\\\\\n            \\\\\\\\n            if not model_config.freeze_vision_tower:\\\\\\\\n                 if hasattr(model, \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\") and hasattr(model.model, \\\\\\\\\\\\\\\"visual\\\\\\\\\\\\\\\"):\\\\\\\\n                    model.model.visual.requires_grad_(True)\\\\\\\\n                 elif hasattr(model, \\\\\\\\\\\\\\\"visual\\\\\\\\\\\\\\\"):\\\\\\\\n                    model.visual.requires_grad_(True)\\\\\\\\n\\\\\\\\n            if hasattr(model, \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\") and hasattr(model.model, \\\\\\\\\\\\\\\"mm_projector\\\\\\\\\\\\\\\"):\\\\\\\\n                model.model.mm_projector.requires_grad_(True)\\\\\\\\n            if hasattr(model, \\\\\\\\\\\\\\\"lm_head\\\\\\\\\\\\\\\"):\\\\\\\\n                model.lm_head.requires_grad_(True)\\\\\\\\n            \\\\\\\\n            fsdp_config.use_orig_params = True\\\\\\\\n            self.print_rank0(\\\\\\\\\\\\\\\"Language backbone is frozen. Only specified layers are trainable.\\\\\\\\\\\\\\\")\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"AttributeError: module 'torch.library' has no attribute 'wrap_triton'\\\\\\\",\\\\\\\"number\\\\\\\":13,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"What is the version of torch, torchvision, and torchaudio?\\\\\\\\n\\\\\\\\nMy version:\\\\\\\\ntorch==2.5.1\\\\\\\\ntorchvision==0.20.1\\\\\\\\ntorchaudio==2.5.1\\\\\\\\n\\\\\\\\nAnd the error: AttributeError: module 'torch.library' has no attribute 'wrap_triton'\\\\\\\\n\\\\\\\\nThanks for help!!\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"core_worker.cc:492: Failed to register worker to Raylet: IOError: [RayletClient] Unable to register worker with raylet. Failed to read data from the socket: End of file worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff\\\\\\\",\\\\\\\"number\\\\\\\":12,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"When I run `bash examples/qwen2_5_vl_7b_geo3k_grpo.sh`, it raises error `core_worker.cc:492: Failed to register worker to Raylet: IOError: [RayletClient] Unable to register worker with raylet. Failed to read data from the socket: End of file worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff`. \\\\\\\\n\\\\\\\\nThe whole output is:\\\\\\\\n```\\\\\\\\nINFO 07-25 14:15:49 [__init__.py:256] Automatically detected platform cuda.\\\\\\\\n2025-07-25 14:16:34,565 ERROR services.py:1350 -- Failed to start the dashboard \\\\\\\\n2025-07-25 14:16:34,566 ERROR services.py:1375 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure' to find where the log file is.\\\\\\\\n2025-07-25 14:16:34,567 ERROR services.py:1419 -- \\\\\\\\nThe last 20 lines of /root/tmp/ray/session_2025-07-25_14-16-09_760500_504098/logs/dashboard.log (it contains the error message from the dashboard): \\\\\\\\n2025-07-25 14:16:23,921 INFO utils.py:337 -- Available modules: [<class 'ray.dashboard.modules.usage_stats.usage_stats_head.UsageStatsHead'>]\\\\\\\\n2025-07-25 14:16:23,922 INFO head.py:234 -- DashboardHeadModules to load: None.\\\\\\\\n2025-07-25 14:16:23,922 INFO head.py:237 -- Loading DashboardHeadModule: <class 'ray.dashboard.modules.usage_stats.usage_stats_head.UsageStatsHead'>.\\\\\\\\n2025-07-25 14:16:23,922 INFO head.py:241 -- Loaded 1 dashboard head modules: [<ray.dashboard.modules.usage_stats.usage_stats_head.UsageStatsHead object at 0x7fa85d74a2c0>].\\\\\\\\n2025-07-25 14:16:23,922 INFO utils.py:304 -- Get all modules by type: SubprocessModule\\\\\\\\n2025-07-25 14:16:24,019 INFO utils.py:337 -- Available modules: [<class 'ray.dashboard.modules.metrics.metrics_head.MetricsHead'>, <class 'ray.dashboard.modules.data.data_head.DataHead'>, <class 'ray.dashboard.modules.event.event_head.EventHead'>, <class 'ray.dashboard.modules.job.job_head.JobHead'>, <class 'ray.dashboard.modules.node.node_head.NodeHead'>, <class 'ray.dashboard.modules.reporter.reporter_head.ReportHead'>, <class 'ray.dashboard.modules.serve.serve_head.ServeHead'>, <class 'ray.dashboard.modules.state.state_head.StateHead'>, <class 'ray.dashboard.modules.train.train_head.TrainHead'>]\\\\\\\\n2025-07-25 14:16:24,020 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.metrics.metrics_head.MetricsHead'>.\\\\\\\\n2025-07-25 14:16:24,020 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.data.data_head.DataHead'>.\\\\\\\\n2025-07-25 14:16:24,020 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.event.event_head.EventHead'>.\\\\\\\\n2025-07-25 14:16:24,020 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.job.job_head.JobHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.node.node_head.NodeHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.reporter.reporter_head.ReportHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.serve.serve_head.ServeHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.state.state_head.StateHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:291 -- Loading SubprocessModule: <class 'ray.dashboard.modules.train.train_head.TrainHead'>.\\\\\\\\n2025-07-25 14:16:24,021 INFO head.py:295 -- Loaded 9 subprocess modules: [<ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa84a5aba30>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa8521e0fa0>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa8521e0f40>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848070c10>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848073820>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848073f70>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848073fd0>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848072890>, <ray.dashboard.subprocesses.handle.SubprocessModuleHandle object at 0x7fa848073f40>].\\\\\\\\n\\\\\\\\n2025-07-25 14:16:37,063 INFO worker.py:1927 -- Started a local Ray instance.\\\\\\\\n[2025-07-25 14:17:08,382 E 504098 504098] core_worker.cc:492: Failed to register worker to Raylet: IOError: [RayletClient] Unable to register worker with raylet. Failed to read data from the socket: End of file worker_id=01000000ffffffffffffffffffffffffffffffffffffffffffffffff\\\\\\\\n```\\\\\\\\n\\\\\\\\nThe version of the packages:\\\\\\\\ntorch 2.6.0\\\\\\\\nray 2.48.0\\\\\\\\nvllm 0.8.0\\\\\\\\ntransformers 4.52.4\\\\\\\\n\\\\\\\\n\\\\\\\\nActually, I met this error under all the environments based on `verl`, but I can't solve it by searching online.\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u66f4\\\\\\\\u65b0\\\\\\\\u5fae\\\\\\\\u4fe1\\\\\\\\u7fa4\\\\\\\\u4e8c\\\\\\\\u7ef4\\\\\\\\u7801\\\\\\\",\\\\\\\"number\\\\\\\":11,\\\\\\\"state\\\\\\\":\\\\\\\"closed\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u60a8\\\\\\\\u597d\\\\\\\\uff0c\\\\\\\\u5fae\\\\\\\\u4fe1\\\\\\\\u7fa4\\\\\\\\u4e8c\\\\\\\\u7ef4\\\\\\\\u7801\\\\\\\\u8fc7\\\\\\\\u671f\\\\\\\\u4e86\\\\\\\\uff0c\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u66f4\\\\\\\\u65b0\\\\\\\\u4e00\\\\\\\\u4e0b\\\\\\\\u5417\\\\\\\\uff0c\\\\\\\\u8c22\\\\\\\\u8c22\\\\\\\\uff01\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"PG loss does not converge\\\\\\\",\\\\\\\"number\\\\\\\":10,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I attempted to train the Qwen 2.5 7B math model on the Math12K dataset, but the results show severe divergence. In my experiments, I disabled the KL divergence terms since current research suggests they have negligible impact (all other parameters remained unchanged). Has anyone encountered a similar issue?\\\\\\\\n\\\\\\\\n<img width=\\\\\\\\\\\\\\\"1099\\\\\\\\\\\\\\\" height=\\\\\\\\\\\\\\\"324\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/ff7f55b3-908a-4f1d-8ddc-bf5294ccdea6\\\\\\\\\\\\\\\" />\\\\\\\\n\\\\\\\\n<img width=\\\\\\\\\\\\\\\"1111\\\\\\\\\\\\\\\" height=\\\\\\\\\\\\\\\"329\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/109c2c10-266a-4239-a218-89c02d2d7b7c\\\\\\\\\\\\\\\" />\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"Training Memory Leak at various steps\\\\\\\",\\\\\\\"number\\\\\\\":9,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"While training with a larger batch size, I encounter consistently a OOM issue when the second my train-batch is started, while with lower batch sizes the OOM error happens at various steps between step 3 and 7. What could be the reason for this?\\\\\\\\n\\\\\\\\nI using the config.yaml as basis with the following adjustments on 4 H100s for the lower batch size:\\\\\\\\n\\\\\\\\n#!/bin/bash\\\\\\\\n\\\\\\\\nset -x\\\\\\\\n\\\\\\\\nexport PYTHONUNBUFFERED=1\\\\\\\\n\\\\\\\\nMODEL_PATH=Qwen/Qwen2.5-VL-7B-Instruct\\\\\\\\n\\\\\\\\npython3 -m verl.trainer.main \\\\\\\\\\\\\\\\\\\\\\\\n    config=examples/config.yaml \\\\\\\\\\\\\\\\\\\\\\\\n    worker.actor.model.model_path=${MODEL_PATH} \\\\\\\\\\\\\\\\\\\\\\\\n    trainer.n_gpus_per_node=4 \\\\\\\\\\\\\\\\\\\\\\\\n    data.rollout_batch_size=128 \\\\\\\\\\\\\\\\\\\\\\\\n    worker.rollout.n=20 \\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\n\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"Does GRPO training of VLM support using a Hugging Face model as the rollout-inference model?\\\\\\\",\\\\\\\"number\\\\\\\":8,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Does it support using Hugging Face to load the VLM model for inference during the rollout phase of GRPO training, instead of using the vLLM engine?\\\\\\\\nIs it convenient to make this modification?\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u8bf7\\\\\\\\u95ee\\\\\\\\u4e0b\\\\\\\\u5956\\\\\\\\u52b1\\\\\\\\u51fd\\\\\\\\u6570\\\\\\\\u80fd\\\\\\\\u4f20\\\\\\\\u5165\\\\\\\\u4e00\\\\\\\\u4e9b\\\\\\\\u5916\\\\\\\\u90e8\\\\\\\\u4fe1\\\\\\\\u606f\\\\\\\\u5417\\\\\\\\uff1f\\\\\\\",\\\\\\\"number\\\\\\\":7,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5c31\\\\\\\\u7c7b\\\\\\\\u4f3c\\\\\\\\u4e8everl\\\\\\\\u4e2d\\\\\\\\u7684extra_info\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"memory need for training Qwen-2.5VL-instruct-3B?\\\\\\\",\\\\\\\"number\\\\\\\":6,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"as suggested, i use BF16 and training Qwen-2.5VL-instruct-3B on a 48GB L20. To enable bf16 training, I added `torch_dtype: bf16` and `strategy: adamw_bf16` to the `examples/config.yaml` file. However, when I subsequently ran the training command `bash examples/qwen2_5_vl_3b_geo3k_grpo.sh` for the 3B model, I encountered an out-of-memory (OOM) error. I'm using a single L20 GPU with 48GB VRAM. According to the official GitHub project documentation, training the 3B model (BF) should require only 40GB of memory. I'm unclear why this OOM error occurred.\\\\\\\\n\\\\\\\\n<img width=\\\\\\\\\\\\\\\"909\\\\\\\\\\\\\\\" height=\\\\\\\\\\\\\\\"488\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"Image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/2665b497-e084-44fb-a7ca-c8ed666d8be6\\\\\\\\\\\\\\\" />\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"Why use qwen2-vl for qwen2.5-vl model?\\\\\\\",\\\\\\\"number\\\\\\\":5,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Hi! \\\\\\\\nI noticed that your framework supports the training of QWEN2.5-VL, but in dictionary verl/models/transformers, why do you use qwen2-vl for modeling qwen2.5-vl, wouldn't this lead to incompatibility with the 2.5 model function?\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"How to add reward model to compute reward signal?\\\\\\\",\\\\\\\"number\\\\\\\":4,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"I want to load an additional reward model (such as CLIP or a smaller-scale model) to calculate rewards for the response. How should I proceed?\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"\\\\\\\\u76ee\\\\\\\\u524d\\\\\\\\u53ef\\\\\\\\u4ee5\\\\\\\\u652f\\\\\\\\u6301\\\\\\\\u89c6\\\\\\\\u9891\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u5417\\\\\\\\uff0c\\\\\\\\u6837\\\\\\\\u4f8b\\\\\\\\u6570\\\\\\\\u636e\\\\\\\\u96c6\\\\\\\\u53ea\\\\\\\\u770b\\\\\\\\u5230\\\\\\\\u56fe\\\\\\\\u7247\\\\\\\\u6ca1\\\\\\\\u770b\\\\\\\\u5230\\\\\\\\u89c6\\\\\\\\u9891\\\\\\\",\\\\\\\"number\\\\\\\":3,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"\\\\\\\\u5982\\\\\\\\u9898\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"Verl version\\\\\\\",\\\\\\\"number\\\\\\\":2,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"Which version of verl is used in the EasyR1 project?\\\\\\\"},{\\\\\\\"title\\\\\\\":\\\\\\\"Support GSPO\\\\\\\",\\\\\\\"number\\\\\\\":1,\\\\\\\"state\\\\\\\":\\\\\\\"open\\\\\\\",\\\\\\\"body\\\\\\\":\\\\\\\"GSPO seems to be very effective. \\\\\\\\nWhen will GSPO be supported?\\\\\\\"}],\\\\\\\"status_code\\\\\\\":200}}\\\", \\\"annotations\\\": null, \\\"meta\\\": null}], \\\"structuredContent\\\": null, \\\"isError\\\": false}\", \"annotations\": null, \"meta\": null}",
    "type": "function_call_output"
  }
]